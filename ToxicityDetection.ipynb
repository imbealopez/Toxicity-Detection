{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicityDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SvjVqDbZo2D3",
        "Jj9pPx5cabQR",
        "CL-tipna2aA7"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imbealopez/Toxicity-Detection/blob/master/ToxicityDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2DgZ9z-3RCSC",
        "outputId": "c682f61c-5228-4dc0-f09d-c69ea6d6eb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# essential imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import warnings  # Ignore warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re  # regular expressions\n",
        "import math  # math functions\n",
        "import scipy.stats as stats\n",
        "import random  # random numbers and generator\n",
        "import copy  # copy objects\n",
        "import pickle  # copy objects into binary files\n",
        "import timeit  # timer\n",
        "import time\n",
        "import os  # system functions\n",
        "import sys\n",
        "import multiprocessing\n",
        "import gc\n",
        "import json\n",
        "import unicodedata\n",
        "import datetime\n",
        "import pkg_resources\n",
        "import string\n",
        "\n",
        "#progress bar make sure to restart runtime\n",
        "!pip install tqdm==4.36.1\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "tqdm.pandas()\n",
        "\n",
        "# import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # plotting tool\n",
        "\n",
        "# scikit-learn\n",
        "# evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# model selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "# preprocessing\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# preprocess text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "\n",
        "# tensorflow\n",
        "# import tensorflow as tf\n",
        "# print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.6/dist-packages (4.36.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrMCmVW-XQDp",
        "colab_type": "text"
      },
      "source": [
        "# Load data and set defaults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzs_gPkvRi-9",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "9f60c019-dce6-44ef-9b62-8861df2ffb51"
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page. \n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "%cd /content/\n",
        "\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# List available datasets.\n",
        "#!kaggle datasets list\n",
        "# download dataset from api\n",
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n",
        "\n",
        "#unzip train and test sets into data directory\n",
        "!unzip train.csv.zip -d ./data\n",
        "!unzip test_private_expanded.csv.zip -d ./data\n",
        "!unzip test_public_expanded.csv.zip -d ./data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-785d4581-65ac-4055-bc8a-2c9be2745223\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-785d4581-65ac-4055-bc8a-2c9be2745223\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 66 Nov 25 18:15 kaggle.json\n",
            "/content\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading identity_individual_annotations.csv.zip to /content\n",
            " 39% 5.00M/12.7M [00:00<00:00, 11.0MB/s]\n",
            "100% 12.7M/12.7M [00:00<00:00, 25.8MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/224k [00:00<?, ?B/s]\n",
            "100% 224k/224k [00:00<00:00, 71.3MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 42% 5.00M/12.0M [00:00<00:00, 10.2MB/s]\n",
            "100% 12.0M/12.0M [00:00<00:00, 22.2MB/s]\n",
            "Downloading test_private_expanded.csv.zip to /content\n",
            " 58% 9.00M/15.6M [00:00<00:00, 11.9MB/s]\n",
            "100% 15.6M/15.6M [00:00<00:00, 18.9MB/s]\n",
            "Downloading test_public_expanded.csv.zip to /content\n",
            " 57% 9.00M/15.7M [00:00<00:00, 16.3MB/s]\n",
            "100% 15.7M/15.7M [00:00<00:00, 26.7MB/s]\n",
            "Downloading toxicity_individual_annotations.csv.zip to /content\n",
            " 74% 49.0M/66.6M [00:02<00:00, 20.3MB/s]\n",
            "100% 66.6M/66.6M [00:02<00:00, 29.5MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 91% 249M/273M [00:07<00:01, 23.2MB/s]\n",
            "100% 273M/273M [00:08<00:00, 35.4MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: ./data/train.csv        \n",
            "Archive:  test_private_expanded.csv.zip\n",
            "  inflating: ./data/test_private_expanded.csv  \n",
            "Archive:  test_public_expanded.csv.zip\n",
            "  inflating: ./data/test_public_expanded.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x0w3GppRUOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set defaults\n",
        "#%matplotlib inline\n",
        "plt.ion()\n",
        "#pd options\n",
        "pd.set_option(\"display.max_columns\", 500)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "pd.set_option(\"display.width\", 1000)\n",
        "\n",
        "\n",
        "# default seeding for reproducability\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "\n",
        "# toxicity score column\n",
        "TOXICITY_COLUMN = \"target\"\n",
        "# text comment column\n",
        "TEXT_COLUMN = \"comment_text\"\n",
        "# List all identities\n",
        "# target and subgroup columns\n",
        "\n",
        "identity_columns = [\n",
        "    \"male\",\n",
        "    \"female\",\n",
        "    \"homosexual_gay_or_lesbian\",\n",
        "    \"christian\",\n",
        "    \"jewish\",\n",
        "    \"muslim\",\n",
        "    \"black\",\n",
        "    \"white\",\n",
        "    \"psychiatric_or_mental_illness\",\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLDDHvn9KfuG",
        "colab_type": "code",
        "outputId": "50bd9f22-ed7d-4912-bc64-bedca9f9e63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from multiprocessing import Pool\n",
        "num_partitions = 4 # number of partitions to split dataframe\n",
        "CORE_COUNT = os.cpu_count()\n",
        "print('number of cores:', CORE_COUNT)\n",
        "\n",
        "def df_parallelize_run(df, func):\n",
        "    \n",
        "    df_split = np.array_split(df, num_partitions)\n",
        "    pool = Pool(CORE_COUNT)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    #df = sp.vstack(pool.map(func, df_split), format='csr') faster and mem efficient for\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of cores: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhK4D6gQVWuF",
        "colab_type": "code",
        "outputId": "0c1cdabe-c2ab-4f62-afe0-c90a53d0d21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Load train and test comments\n",
        "train_comments = pd.read_csv('/content/data/train.csv')\n",
        "test_private_comments = pd.read_csv('/content/data/test_private_expanded.csv')\n",
        "#test_public_comments = pd.read_csv('/content/data/test_public_expanded.csv')\n",
        "\n",
        "# Make sure all comment_text values are strings\n",
        "train_comments[\"comment_text\"] = train_comments[\"comment_text\"].astype(str)\n",
        "test_private_comments[\"comment_text\"] = test_private_comments[\"comment_text\"].astype(str)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.31 s, sys: 1.71 s, total: 11 s\n",
            "Wall time: 11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvjVqDbZo2D3",
        "colab_type": "text"
      },
      "source": [
        "## display data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9A0GQK5a2_p",
        "colab_type": "code",
        "outputId": "333d9e6d-a660-4b42-e1fb-9a477f120d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"loaded %d records\" % len(train_comments))\n",
        "#%%\n",
        "# display first comment\n",
        "print(train_comments.iloc[0][\"comment_text\"])\n",
        "# display head\n",
        "train_comments.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded 1804874 records\n",
            "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>bisexual</th>\n",
              "      <th>black</th>\n",
              "      <th>buddhist</th>\n",
              "      <th>christian</th>\n",
              "      <th>female</th>\n",
              "      <th>heterosexual</th>\n",
              "      <th>hindu</th>\n",
              "      <th>homosexual_gay_or_lesbian</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>jewish</th>\n",
              "      <th>latino</th>\n",
              "      <th>male</th>\n",
              "      <th>muslim</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>other_gender</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>other_religion</th>\n",
              "      <th>other_sexual_orientation</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>transgender</th>\n",
              "      <th>white</th>\n",
              "      <th>created_date</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:41.987077+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Thank you!! This would make my life a lot less...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:42.870083+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is such an urgent design problem; kudos t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:45.222647+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59855</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Is this something I'll be able to install on m...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:47.601894+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59856</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>haha you guys are a bunch of losers.</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.872340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2015-09-29 10:50:48.488476+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>59859</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>ur a sh*tty comment.</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.638095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:50.865549+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>59861</td>\n",
              "      <td>0.457627</td>\n",
              "      <td>hahahahahahahahhha suck it.</td>\n",
              "      <td>0.050847</td>\n",
              "      <td>0.305085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:52.451277+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.220339</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>59863</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:54.055221+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>239575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>The ranchers seem motivated by mostly by greed...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 18:01:05.156229+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26662</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>239576</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>It was a great show. Not a combo I'd of expect...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 18:11:21.570460+00</td>\n",
              "      <td>6</td>\n",
              "      <td>239522.0</td>\n",
              "      <td>26650</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>239578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Wow, that sounds great.</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 18:33:56.298783+00</td>\n",
              "      <td>6</td>\n",
              "      <td>239519.0</td>\n",
              "      <td>26650</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>239579</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>This is a great story. Man. I wonder if the pe...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.293333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-01-13 18:43:00.710664+00</td>\n",
              "      <td>6</td>\n",
              "      <td>239524.0</td>\n",
              "      <td>26650</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>10</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>239582</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This seems like a step in the right direction.</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 18:57:24.023698+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>239583</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>It's ridiculous that these guys are being call...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:02:22.655293+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26670</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>239584</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>This story gets more ridiculous by the hour! A...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:04:31.238894+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26670</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>239585</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>I agree; I don't want to grant them the legiti...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:12:25.561335+00</td>\n",
              "      <td>6</td>\n",
              "      <td>239583.0</td>\n",
              "      <td>26670</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>239589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Interesting. I'll be curious to see how this w...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:25:28.280515+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>239590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Awesome! I love Civil Comments!</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:39:16.688592+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>239591</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>I'm glad you're working on this, and I look fo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:42:06.626287+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>239592</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>Angry trolls, misogynists and Racists\", oh my....</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-01-13 19:48:45.619202+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id    target                                       comment_text  severe_toxicity   obscene  identity_attack    insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
              "0    59848  0.000000  This is so cool. It's like, 'would you want yo...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:41.987077+00               2        NaN        2006  rejected      0    0    0      0         0         0.000000                         0                         4\n",
              "1    59849  0.000000  Thank you!! This would make my life a lot less...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:42.870083+00               2        NaN        2006  rejected      0    0    0      0         0         0.000000                         0                         4\n",
              "2    59852  0.000000  This is such an urgent design problem; kudos t...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:45.222647+00               2        NaN        2006  rejected      0    0    0      0         0         0.000000                         0                         4\n",
              "3    59855  0.000000  Is this something I'll be able to install on m...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:47.601894+00               2        NaN        2006  rejected      0    0    0      0         0         0.000000                         0                         4\n",
              "4    59856  0.893617               haha you guys are a bunch of losers.         0.021277  0.000000         0.021277  0.872340     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.25     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2015-09-29 10:50:48.488476+00               2        NaN        2006  rejected      0    0    0      1         0         0.000000                         4                        47\n",
              "5    59859  0.666667                               ur a sh*tty comment.         0.047619  0.638095         0.000000  0.333333     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:50.865549+00               2        NaN        2006  rejected      0    0    0      0         0         0.009524                         0                       105\n",
              "6    59861  0.457627                        hahahahahahahahhha suck it.         0.050847  0.305085         0.000000  0.254237     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:52.451277+00               2        NaN        2006  rejected      0    0    0      0         0         0.220339                         0                        59\n",
              "7    59863  0.000000                                FFFFUUUUUUUUUUUUUUU         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:54.055221+00               2        NaN        2006  rejected      0    0    0      0         0         0.000000                         0                         4\n",
              "8   239575  0.000000  The ranchers seem motivated by mostly by greed...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 18:01:05.156229+00               6        NaN       26662  approved      0    0    0      0         0         0.000000                         0                         4\n",
              "9   239576  0.000000  It was a great show. Not a combo I'd of expect...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 18:11:21.570460+00               6   239522.0       26650  approved      0    0    0      1         0         0.000000                         0                         4\n",
              "10  239578  0.000000                            Wow, that sounds great.         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 18:33:56.298783+00               6   239519.0       26650  approved      0    0    0      0         0         0.000000                         0                         4\n",
              "11  239579  0.440000  This is a great story. Man. I wonder if the pe...         0.000000  0.293333         0.000000  0.320000     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.00     0.0     0.0   0.6     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2016-01-13 18:43:00.710664+00               6   239524.0       26650  approved      0    0    0      1         0         0.040000                        10                        75\n",
              "12  239582  0.000000     This seems like a step in the right direction.         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 18:57:24.023698+00               6        NaN       26795  approved      0    0    0      5         0         0.000000                         0                         4\n",
              "13  239583  0.600000  It's ridiculous that these guys are being call...         0.000000  0.100000         0.000000  0.600000     0.1    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:02:22.655293+00               6        NaN       26670  approved      0    0    0      3         0         0.000000                         0                        10\n",
              "14  239584  0.500000  This story gets more ridiculous by the hour! A...         0.000000  0.000000         0.000000  0.300000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:04:31.238894+00               6        NaN       26670  approved      0    0    0      9         0         0.000000                         0                        10\n",
              "15  239585  0.000000  I agree; I don't want to grant them the legiti...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:12:25.561335+00               6   239583.0       26670  approved      0    0    0      3         0         0.000000                         0                         4\n",
              "16  239589  0.000000  Interesting. I'll be curious to see how this w...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:25:28.280515+00               6        NaN       26795  approved      0    0    0     20         0         0.000000                         0                         4\n",
              "17  239590  0.000000                    Awesome! I love Civil Comments!         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:39:16.688592+00               6        NaN       26795  approved      0    0    0      3         0         0.000000                         0                         4\n",
              "18  239591  0.000000  I'm glad you're working on this, and I look fo...         0.000000  0.000000         0.000000  0.000000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:42:06.626287+00               6        NaN       26795  approved      0    0    0      4         0         0.000000                         0                         4\n",
              "19  239592  0.500000  Angry trolls, misogynists and Racists\", oh my....         0.000000  0.000000         0.100000  0.500000     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.00     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2016-01-13 19:48:45.619202+00               6        NaN       26795  approved      0    0    0      0         0         0.000000                         4                        10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU7hAzXPdxyx",
        "colab_type": "code",
        "outputId": "bbaa61c4-f96c-438c-8841-95572321fee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# display toxic comments above target 0.5\n",
        "train_comments[train_comments[\"target\"] >= 0.5].head()\n",
        "# shuffle\n",
        "# train_comments = train_comments.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>bisexual</th>\n",
              "      <th>black</th>\n",
              "      <th>buddhist</th>\n",
              "      <th>christian</th>\n",
              "      <th>female</th>\n",
              "      <th>heterosexual</th>\n",
              "      <th>hindu</th>\n",
              "      <th>homosexual_gay_or_lesbian</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>jewish</th>\n",
              "      <th>latino</th>\n",
              "      <th>male</th>\n",
              "      <th>muslim</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>other_gender</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>other_religion</th>\n",
              "      <th>other_sexual_orientation</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>transgender</th>\n",
              "      <th>white</th>\n",
              "      <th>created_date</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59856</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>haha you guys are a bunch of losers.</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.872340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2015-09-29 10:50:48.488476+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>59859</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>ur a sh*tty comment.</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.638095</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:50.865549+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>239583</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>It's ridiculous that these guys are being call...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:02:22.655293+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26670</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>239584</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>This story gets more ridiculous by the hour! A...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-13 19:04:31.238894+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26670</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>239592</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>Angry trolls, misogynists and Racists\", oh my....</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-01-13 19:48:45.619202+00</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26795</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id    target                                       comment_text  severe_toxicity   obscene  identity_attack    insult  threat  asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  latino  male  muslim  other_disability  other_gender  other_race_or_ethnicity  other_religion  other_sexual_orientation  physical_disability  psychiatric_or_mental_illness  transgender  white                   created_date  publication_id  parent_id  article_id    rating  funny  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  toxicity_annotator_count\n",
              "4    59856  0.893617               haha you guys are a bunch of losers.         0.021277  0.000000         0.021277  0.872340     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.25     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2015-09-29 10:50:48.488476+00               2        NaN        2006  rejected      0    0    0      1         0         0.000000                         4                        47\n",
              "5    59859  0.666667                               ur a sh*tty comment.         0.047619  0.638095         0.000000  0.333333     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2015-09-29 10:50:50.865549+00               2        NaN        2006  rejected      0    0    0      0         0         0.009524                         0                       105\n",
              "13  239583  0.600000  It's ridiculous that these guys are being call...         0.000000  0.100000         0.000000  0.600000     0.1    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:02:22.655293+00               6        NaN       26670  approved      0    0    0      3         0         0.000000                         0                        10\n",
              "14  239584  0.500000  This story gets more ridiculous by the hour! A...         0.000000  0.000000         0.000000  0.300000     0.0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN    NaN                        NaN                                  NaN     NaN     NaN   NaN     NaN               NaN           NaN                      NaN             NaN                       NaN                  NaN                            NaN          NaN    NaN  2016-01-13 19:04:31.238894+00               6        NaN       26670  approved      0    0    0      9         0         0.000000                         0                        10\n",
              "19  239592  0.500000  Angry trolls, misogynists and Racists\", oh my....         0.000000  0.000000         0.100000  0.500000     0.0    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0    0.0                        0.0                                 0.00     0.0     0.0   0.0     0.0               0.0           0.0                      0.0             0.0                       0.0                  0.0                            0.0          0.0    0.0  2016-01-13 19:48:45.619202+00               6        NaN       26795  approved      0    0    0      0         0         0.000000                         4                        10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2-5MW6pXF94",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiIkze1rFby8",
        "colab_type": "text"
      },
      "source": [
        "##download and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buuBKjJi9v5u",
        "colab_type": "code",
        "outputId": "31bb54be-89f9-4943-f5cc-a15ed4172020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        }
      },
      "source": [
        "#imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#emoji codes\n",
        "!pip install emoji --upgrade\n",
        "import emoji\n",
        "\n",
        "# tokenizer\n",
        "from nltk import TweetTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "# stem/lemmatizers\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "# word embeddings\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#better text preprocessor\n",
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from ekphrasis.classes.spellcorrect import SpellCorrector"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=d982ca983c8b9b55c139584b0b34ce415744f2846af9f07bd7e0f783b8b6c247\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 28.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.36.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.17.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (41.6.0)\n",
            "Building wheels for collected packages: ekphrasis, ujson, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82844 sha256=37d6521835a7d13641b835149ae511c93a4c000757fcd77e228cdefc05887f8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68034 sha256=c2d49ff2ed782a51ee12aee58d28fbf24b4e9501b48e19e0d14a511c45ffff70\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=ee36342954cb483c83c0c7e01b85ff1f6d2ff7fc611d4145c4f4793296eb61d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "Successfully built ekphrasis ujson ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.1 ekphrasis-0.5.1 ftfy-5.6 ujson-1.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu3N4SAqdLeo",
        "colab_type": "code",
        "outputId": "50bfd7db-624c-4eb9-fac4-d2a0a42d1e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#download word embeddings (takes around 5min)\n",
        "#https://www.kaggle.com/iezepov/gensim-embeddings-dataset\n",
        "!kaggle datasets download -d iezepov/gensim-embeddings-dataset\n",
        "!unzip gensim-embeddings-dataset.zip -d gensim-embeddings-dataset\n",
        "\n",
        "#download word embeddings direct link\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading gensim-embeddings-dataset.zip to /content\n",
            "100% 8.47G/8.47G [03:00<00:00, 31.3MB/s]\n",
            "100% 8.47G/8.47G [03:00<00:00, 50.4MB/s]\n",
            "Archive:  gensim-embeddings-dataset.zip\n",
            "  inflating: gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim  \n",
            "  inflating: gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim.vectors.npy  \n",
            "  inflating: gensim-embeddings-dataset/crawl-300d-2M.gensim  \n",
            "  inflating: gensim-embeddings-dataset/crawl-300d-2M.gensim.vectors.npy  \n",
            "  inflating: gensim-embeddings-dataset/glove.840B.300d.gensim  \n",
            "  inflating: gensim-embeddings-dataset/glove.840B.300d.gensim.vectors.npy  \n",
            "  inflating: gensim-embeddings-dataset/glove.twitter.27B.200d.gensim  \n",
            "  inflating: gensim-embeddings-dataset/glove.twitter.27B.200d.gensim.vectors.npy  \n",
            "  inflating: gensim-embeddings-dataset/numberbatch-en.gensim  \n",
            "  inflating: gensim-embeddings-dataset/numberbatch-en.gensim.vectors.npy  \n",
            "  inflating: gensim-embeddings-dataset/paragram_300_sl999.gensim  \n",
            "  inflating: gensim-embeddings-dataset/paragram_300_sl999.gensim.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj9pPx5cabQR",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKKs9vjbexZd",
        "colab_type": "code",
        "outputId": "b4200283-2750-44f3-c78e-2c0481f38d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# display 10 toxic comments \n",
        "train_comments[train_comments[\"target\"] >= 0.5]['comment_text'].head(10).values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['haha you guys are a bunch of losers.', 'ur a sh*tty comment.',\n",
              "       'It\\'s ridiculous that these guys are being called \"protesters\". Being armed is a threat of violence, which makes them terrorists.',\n",
              "       \"This story gets more ridiculous by the hour! And, I love that people are sending these guys dildos in the mail now. But… if they really think there's a happy ending in this for any of them, I think they're even more deluded than all of the jokes about them assume.\",\n",
              "       'Angry trolls, misogynists and Racists\", oh my. It doesn\\'t take all of my 150 IQ to see the slant here.  it\\'s the \"Diversity diode\" at work yet again. \"We can say anything that we want because we are Diversity. You on the other hand must only  say what we allow you to say. From now on, winning arguments against any member of diversity will be considered offensive language.  facts, cogent, linear posts and Math are now verboten.',\n",
              "       \"Yet call out all Muslims for the acts of a few will get you pilloried.   So why is it okay to smear an entire religion over these few idiots?  Or is this because it's okay to bash Christian sects?\",\n",
              "       'This bitch is nuts. Who would read a book by a woman.',\n",
              "       \"Tuesday Jan. 13, members of \\nthis socalled 'militia' traveled to\\nJohn Day,Or to try to enlist the sherif \\nthere to come to Malheur and support them. He refused to go\\nwithout the approval of the local \\nsheriff in Burns, Dave Ward.\\nSheriff Ward said he only wanted \\nThe John Day sheriff to come to\\nBurns if he would be willing to 'shame and humiliate' the trespassers into leaving.\\nThe Bundy Bunch are armed terrorist idiots who are attempting to poison my State with their toxic and divisive brand of 'politics'. They need to be arrested and prosecuted.\\nRallies to support public lands and\\nthe Malheur Refuge are being held in \\nPDX, Eugene, and other cities across \\nOr. next Tuesday Jan. 19: check out\\n#SupportMalheur.\\nAnd yes, I have lived in both rural \\nOr (25 years) and urban Or.\",\n",
              "       \"Love that they also participate in the Portland Farmer's Market at PSU... and trying to gluten-shame Tastebud and propping Papa Murphy's in the same sentence is laughable.\",\n",
              "       \"Took this as an opportunity to check back in on The Yard and the floorpans are finally up and they are ATROCIOUS.  Yeah, because I want to spend 2500+ to share a thin wall with my roommate.  Oh, and I want to pick between a dining table OR living room set, because in this modern world you can't have both.  And if you do want both, one is going to be in a tiny spot by your door and the other is going to be down a long hallway of useless square ft next to your kitchenette.  So dumb.\\nhttp://yardpdx.com/leasing/\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwGLsuORfZEz",
        "colab_type": "code",
        "outputId": "12b082ff-9400-455f-ab02-2e3010d30da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# display 10 selected identity comments\n",
        "train_comments[train_comments[\"homosexual_gay_or_lesbian\"] >= 0.5]['comment_text'].head(10).values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Is there any such thing as a 'gay trans woman '?  Technically?  Theoretically, for example, a female, once you become your born sex, anatomically, you will be/continue to be attracted to women and not necessarily gay women.\",\n",
              "       'Well, this wasn\\'t meant to be a dissertation. It\\'s an entertainment piece, so I was mostly trying to be funny—whether I was successful in that regard is up to the reader. And of all these guys, I have a certain fondness for Hitchens, as I make clear, but I had to make fun of him for that whole \"women aren\\'t funny\" thing. And while I can\\'t say I have any \"favorite\" atheist artists or writers—I don\\'t really seek them out, to be honest—I certainly don\\'t think all atheists are \"awful\" (some of my best friends are atheists; some of them may even be gay and/or black!). I mostly just get annoyed when someone thinks identifying as an atheist inherently makes them smarter than a rational theist. I\\'m cool with whatever philosophical choice you\\'ve decided to live your life under, as long as you\\'re not a dick about it.',\n",
              "       \"As far as i can tell, there is one reason to vote for Hillary, which you mention, but so very many reasons not to.  I fear she lost her soul to the establishment, and find it hard to believe she makes any decisions without polling it first. NAFTA, the war in Iraq, gay marriage, legalizing marijuana, the TPP, etc., etc. etc.  She votes for what's politically popular every time.  I wish Elizabeth Warren was running!\",\n",
              "       \"Other than the inherent hypocricy, it really doesn't matter.  Personally, as I've stated before, I don't judge candidates by gender.  Male, Female, Lesbian, Gay, Bi-sexual or Transgender is pretty irrelevant for me.  What are your polices and beliefs are what I'm interested in.   For Gloria Steinem and Madeleine Albright, sex defines their politics and that's kind of sad.  Any woman is better than any man seems to be their mantra.  I'm glad to see that the younger generation seems to have a more liberal attitude than these old warriors who can't forget the past.\",\n",
              "       \"Let's get another thing clear. As an (I presume) American, you are welcome to believe whatever you prefer regarding homosexuality, environmental protection, the number of children you wish to have, and hunting, and are free to express those views. But when you presume to judge anyone else and call them selfish, irresponsible, and a terrible person because they disagree with your personal preferences, and most particularly when they are a recently deceased and highly esteemed Supreme Court Justice for three decades, then you have revealed yourself to be against the ideals of our Constitution and civil society, and have lost all credibility with me and probably others who believe in American ideals. A retraction and apology to Judge Scalia's family and the readers of this comment section are in order.\",\n",
              "       \"I cannot wait to see if pimping underage girls will result in a larger fine than declining to bake a cake for a gay wedding.  It will provide insight into Avakian's priorities.\",\n",
              "       'Dave - if the 13 year old were gay or bisexual, Avakian would have imposed a big fine like the cake or the karaoke bar back when they filed the criminal case. I think avakians bias is already clear.',\n",
              "       \"Interesting.  But since no one has ever  been fined for refusing to bake a cake for a gay wedding.  I can't see how it wouldn't be.\",\n",
              "       \"Ms. Slovic:\\n\\nWow.  Thanks for this.  I love the disconnect between Bailey attending a somewhat scary church (female submission, anti-gay) and being a progressive.  It shows something hard to define, but not very likable, about his character.  He's kinda wishy-washy.   \\n\\nI, for one, don't want a candidate who acts on woo-woo beliefs\\nand goes to a church that disdains the lifestyles of many Portlanders.\\n\\nGay people, co-habitors, feminists should consider whether they want Bailey to lead the city, given his almost schizoid spirituality (my church is great even if the doctrines and clergy are backwards and bigoted).\\n\\nWhat does it say about Bailey's intellect that he can hold such self-contradictory ideas?\",\n",
              "       'I know many practicing Catholics who attend Mass regularly that reject the Catholic Church’s teachings on birth control and homosexuality…just to name two.\\n\\nEach individual has the capacity to embrace spiritual beliefs without feeling beholden to what a human contrived Church tells them they should or should not do in their daily lives.\\n\\nI say good for Jules.  I have never met him but I am impressed with his reported intellectual curiosity and rigor.  I think he is demonstrating the capacity a Mayor needs to be strong in his convictions while being secure enough in his being to nuance the black and white choices a Mayor confronts on a daily basis.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTIGfeyu_fb",
        "colab_type": "text"
      },
      "source": [
        "check for sensored toxic words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYm1rsp3nKvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_comments['stared words'] = train_comments['comment_text'].apply(lambda comment: set(w for w in comment.split() if (w.count('*') > 0)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CATA32t9ugpk",
        "colab_type": "code",
        "outputId": "88a65799-1a84-4281-9e85-a119dbfceee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(set().union(*train_comments[train_comments[\"stared words\"] != set()]['stared words'].values\n",
        "))[4000:5000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*/snark*',\n",
              " 'Re*tarded*.',\n",
              " '*directly*',\n",
              " 'towers.**',\n",
              " '*now*',\n",
              " '*just*',\n",
              " '*sses',\n",
              " 'f***the',\n",
              " 'BULL(*&^.',\n",
              " '*retirement',\n",
              " 'stress**.',\n",
              " 'bullcr*p.',\n",
              " '****But',\n",
              " '*priority,*',\n",
              " '**universally**',\n",
              " 'Post*/',\n",
              " '\"*Assault',\n",
              " 'religious*',\n",
              " '*Now',\n",
              " '*federal*',\n",
              " '\"p*ssy',\n",
              " '*Globe\"',\n",
              " 'mind-f**king',\n",
              " 'points**.',\n",
              " '*miles*',\n",
              " '**sigh**',\n",
              " 'place.*',\n",
              " 'bl*wing',\n",
              " 'ni**er.\"',\n",
              " '*physiologically*',\n",
              " '*abortion*',\n",
              " '*criticize*:',\n",
              " 'Dispatch*',\n",
              " 'p*nis.',\n",
              " '*personal*',\n",
              " '*claim\"',\n",
              " 'F**ked',\n",
              " '*permanently*,',\n",
              " \"'Ni**er\",\n",
              " 'I***t',\n",
              " 'su**ing',\n",
              " 'Jihadism.*”',\n",
              " 'Tr*mpkin.',\n",
              " '*Stuck',\n",
              " 'conference*.',\n",
              " 'than,,SH**\"',\n",
              " '*little*',\n",
              " '*Parents*',\n",
              " '\"pu**y',\n",
              " 'H***!',\n",
              " 'f**',\n",
              " \"*'MuckyMuck'*\",\n",
              " 'ballot***',\n",
              " '*absolute*',\n",
              " '**11-year-old',\n",
              " 'p***y,\"',\n",
              " 'freedom*',\n",
              " '*OUR*',\n",
              " 'off*',\n",
              " 'Hollande*',\n",
              " 's***less',\n",
              " 'gallery*',\n",
              " '**thoroughly',\n",
              " 'too*',\n",
              " \"s*x',\",\n",
              " 'liberalism.*',\n",
              " '*justified*',\n",
              " 'same*',\n",
              " '*fine*',\n",
              " '“bullsh*t.”',\n",
              " 'bulls**t.',\n",
              " '*intelligibility*',\n",
              " 'Portland.”***',\n",
              " '*completely',\n",
              " 'F%&*',\n",
              " '**E**xpect',\n",
              " 'h*eritical',\n",
              " 'Home.*',\n",
              " '**Look',\n",
              " '*refugee*',\n",
              " '1*C',\n",
              " 'regime*',\n",
              " '**still**',\n",
              " '*pewresearch.org',\n",
              " 'horses*#t.',\n",
              " 'OJT*.',\n",
              " '*during*',\n",
              " '*kudos*',\n",
              " 'horsesh*t',\n",
              " '*blindness*',\n",
              " '*commentors',\n",
              " '*confusion*.',\n",
              " '*Deliberately*',\n",
              " 'a**es',\n",
              " \"*can't*\",\n",
              " 'on*)',\n",
              " '*myth*',\n",
              " 'place**',\n",
              " '*conversion*',\n",
              " '*USE',\n",
              " '\"*everyone*\"',\n",
              " '*ssholes..',\n",
              " '*catechism*',\n",
              " '*(convincing',\n",
              " '*acupuncture',\n",
              " \"*someone's*\",\n",
              " ';(*',\n",
              " '*think*)',\n",
              " '*James',\n",
              " 'one*',\n",
              " 's*fu',\n",
              " 'surveillance*.',\n",
              " '*everything*',\n",
              " '*they',\n",
              " 'a****',\n",
              " 'ComeyCottagecheeseCurdles**!',\n",
              " 'a****le',\n",
              " '*arrest*',\n",
              " '\"p*',\n",
              " '*negative*',\n",
              " 'F*ckface',\n",
              " 'who**.',\n",
              " 'Po...*',\n",
              " '*Citation',\n",
              " 'a**clown,',\n",
              " '*time*',\n",
              " 'news*.',\n",
              " '*have',\n",
              " '*attempt*',\n",
              " '*emphasizing*',\n",
              " 'City.****',\n",
              " 's*&t',\n",
              " 'property*',\n",
              " \"*'Born\",\n",
              " '\"A**hole\"....so',\n",
              " '*there',\n",
              " '*encryption*',\n",
              " 'Bullsh*t!',\n",
              " '*God,',\n",
              " '****______________________________________________________________________________________________________________________________**',\n",
              " 'habits*.',\n",
              " '*inspiring*',\n",
              " '*established',\n",
              " 'usually*',\n",
              " '*timing*',\n",
              " '*tragedy*',\n",
              " 'bullsh*t!',\n",
              " '*event,*',\n",
              " '*Precisely',\n",
              " '*Mexican',\n",
              " '\"b***-hurt\".',\n",
              " '*either/or*',\n",
              " 'wh*ring',\n",
              " 'bag**',\n",
              " '*editors',\n",
              " '*essential*',\n",
              " '*doing*',\n",
              " 'person*;',\n",
              " 'herds*******',\n",
              " '*democracy*',\n",
              " 'business*',\n",
              " '*2014-the',\n",
              " '50%*',\n",
              " 'nit*wit',\n",
              " 'fide.\"*',\n",
              " 'houses*',\n",
              " 'yesterday.*',\n",
              " 'ni**er',\n",
              " '*serious*',\n",
              " '*I\\'m\"',\n",
              " 'calling*',\n",
              " '*Hacking',\n",
              " 'Hawaii.****',\n",
              " 'RIGHT!*',\n",
              " '*Ak',\n",
              " '*Dr.',\n",
              " '*Tourism',\n",
              " 'MRI.*',\n",
              " '*has*',\n",
              " 'SG/2*',\n",
              " 'a*%hole',\n",
              " '*LIBERAL*,',\n",
              " 'society*',\n",
              " '*libel*',\n",
              " '*discussion*',\n",
              " '**Follow',\n",
              " 'ups*.',\n",
              " '\"****',\n",
              " 'id*iot!',\n",
              " '*URD',\n",
              " 'a*holes',\n",
              " 's*#tless',\n",
              " '*fear',\n",
              " 'Papers\"**',\n",
              " 'reprobate!!!*...',\n",
              " 'property*,',\n",
              " '*sound',\n",
              " 'housing.**',\n",
              " '*There',\n",
              " '*piece',\n",
              " '*$155.2B',\n",
              " 'f*(%',\n",
              " '*error*',\n",
              " 'evil*',\n",
              " 'redevelopment.***',\n",
              " '*headesk**headesk**headesk**headesk**headesk**headesk**headesk**headesk*...',\n",
              " 'share...**but',\n",
              " '*law',\n",
              " '*University',\n",
              " '*Spirit',\n",
              " 'Lenin*',\n",
              " '****,',\n",
              " '*Aside',\n",
              " '*ENTIRELY*',\n",
              " '**Made',\n",
              " '*us*',\n",
              " '***hole!\"',\n",
              " '**a',\n",
              " '*Fun',\n",
              " '*look*',\n",
              " '*buying*',\n",
              " 'F*N',\n",
              " '*itself*',\n",
              " '*incremental*',\n",
              " 'square”**',\n",
              " 'anti-*immigration*',\n",
              " 'c**ts',\n",
              " '*Alaska',\n",
              " '(fa**ot),',\n",
              " 'm****f*****,',\n",
              " 'doctor*',\n",
              " '*independent*',\n",
              " '***NEVER***',\n",
              " 'control.*',\n",
              " \"they're*\",\n",
              " 'Users**3,424,971,237',\n",
              " '*Nearly',\n",
              " 'branch*cough*',\n",
              " 'scre***g',\n",
              " '*extra*',\n",
              " '***Yes-',\n",
              " 'fuc***',\n",
              " 'f***\";',\n",
              " \"**There's\",\n",
              " '*start*',\n",
              " \"*we've*\",\n",
              " \"***'s\",\n",
              " 'a**h***,',\n",
              " '*connect',\n",
              " '*SURE*',\n",
              " '*#',\n",
              " '*tradition*',\n",
              " 'c**t',\n",
              " 'S**t\"',\n",
              " '*(@',\n",
              " '*Tourists',\n",
              " 'people.***',\n",
              " 'being.*',\n",
              " '\"c**k\"',\n",
              " 'a**e',\n",
              " 'house*,',\n",
              " '5*,',\n",
              " '*events*,',\n",
              " '(*Dr.',\n",
              " 'base*I',\n",
              " 'system*,',\n",
              " '*Remedial',\n",
              " '*outside*',\n",
              " 'C*A',\n",
              " 'miles*',\n",
              " 'fr**d',\n",
              " '*yes,',\n",
              " 'effect.*snark*',\n",
              " '*was*.',\n",
              " '\"*...and',\n",
              " '*Just',\n",
              " 'cr*ppy',\n",
              " 's#*%load',\n",
              " 'good*',\n",
              " '$h^*',\n",
              " '*guerilla',\n",
              " '***affordable',\n",
              " '*These',\n",
              " '*smiles*',\n",
              " '*aberrant*,',\n",
              " 's*$t',\n",
              " '*it',\n",
              " '*700+*',\n",
              " '*already-existing*',\n",
              " '**openly**',\n",
              " 'gay*',\n",
              " '*sometimes*',\n",
              " \"'Bull$***\",\n",
              " 'Commission,*',\n",
              " '*Economic',\n",
              " '*Sigh*.',\n",
              " '&%&%^$$*',\n",
              " '*says*',\n",
              " \"f**ked'\",\n",
              " 'mike*',\n",
              " 'war*.',\n",
              " 'p***y\"??',\n",
              " 'Bu**',\n",
              " '**only**',\n",
              " 'bulls***',\n",
              " '*fantastic*.',\n",
              " 'p*ssed.',\n",
              " '\"Sh*t',\n",
              " '*inaction*,',\n",
              " '*(Although',\n",
              " 'bull****,',\n",
              " 'MothaF**_%$',\n",
              " '*WE*',\n",
              " '*Anyone',\n",
              " 'Sl*t',\n",
              " '*initially',\n",
              " '*irregular*',\n",
              " '*denial*',\n",
              " '*FAMILY*)',\n",
              " \"'s**thole'?\",\n",
              " '*continues',\n",
              " \"****'.\",\n",
              " '*SHOPO',\n",
              " '**compromise**',\n",
              " 'Bat-sh*t',\n",
              " 'Arseh*&l.',\n",
              " '*try*',\n",
              " 'complementarity,*',\n",
              " '*shouldest',\n",
              " 'f****d.',\n",
              " '*consultants*,',\n",
              " '*welcome',\n",
              " \"*won't*\",\n",
              " 'murder*.',\n",
              " '*AND*',\n",
              " '*hit.',\n",
              " 'V*A*G',\n",
              " 'p*sspot',\n",
              " 'mess*',\n",
              " '**Conduct',\n",
              " 'p*ssy-whipped',\n",
              " '*Zero*',\n",
              " 'f*c*ing',\n",
              " '(*)',\n",
              " '*knew*--as',\n",
              " 'sh*tt',\n",
              " 'marriage.\"*',\n",
              " 'their$%^&*(*^%$$',\n",
              " '*premium*',\n",
              " '****That',\n",
              " 'd**ned,',\n",
              " 'sex*',\n",
              " 'sh*t....pardon',\n",
              " \"f**k'n\",\n",
              " '8*',\n",
              " 'bats**t',\n",
              " '*three*',\n",
              " '*Again,',\n",
              " 'G*D?',\n",
              " 'base*;',\n",
              " 'jacka**!!',\n",
              " '****Relax',\n",
              " '*uncivil*',\n",
              " 'Sh*t;',\n",
              " 'development*',\n",
              " '*relatively*',\n",
              " 'p****s',\n",
              " 'damaged**',\n",
              " '*knocks',\n",
              " 'B*%#!',\n",
              " 'targets*.',\n",
              " 'pigs**t',\n",
              " '*care*',\n",
              " '*ruben,',\n",
              " '30*100,000',\n",
              " 'mine.*',\n",
              " '*^&%',\n",
              " 'n*tt*rs',\n",
              " 'course.***',\n",
              " 'O****a',\n",
              " '*ucker',\n",
              " '*cogent',\n",
              " 'p**',\n",
              " 'https://www.google.ca/#q=schneier+security+theater&*',\n",
              " '*comprehend*',\n",
              " '*𝘊𝘩𝘦𝘤𝘬',\n",
              " 'agents*',\n",
              " '*exclude*',\n",
              " '\"sl*twalk\"...',\n",
              " '*NASTY*',\n",
              " '*Snort!*......',\n",
              " '*Keep',\n",
              " 'cable.*',\n",
              " 'big*t',\n",
              " '****-up',\n",
              " 'healthcare*.',\n",
              " '*feet*.',\n",
              " '*desperate',\n",
              " '*science*.',\n",
              " '%*&.',\n",
              " '***now',\n",
              " 'behavior.*',\n",
              " '****24',\n",
              " 'DUMB*ASS',\n",
              " '3,043*',\n",
              " '*ICM',\n",
              " 'Id***\"',\n",
              " 'f****ed-up,',\n",
              " '***Fair',\n",
              " '**WASTED**',\n",
              " '*cooperation',\n",
              " \"*there's\",\n",
              " '*investigations',\n",
              " 'P*$$^grabbing',\n",
              " '\"w****',\n",
              " '****and',\n",
              " 'p*ssy-grabbing',\n",
              " '*SOP:',\n",
              " '***use',\n",
              " '3%*15yrs=45%.',\n",
              " '***Does',\n",
              " '*Being',\n",
              " 'good,\"*and',\n",
              " 'W*ore',\n",
              " '*mostly',\n",
              " 'pr*sident.',\n",
              " 'Miller***',\n",
              " '*without*',\n",
              " '*understand*',\n",
              " 'law...4...3...2...1...*BUZZ*',\n",
              " '“irony”**',\n",
              " '*head',\n",
              " '*get*',\n",
              " '(r****d)',\n",
              " '**NEWSFLASH**',\n",
              " 'F*ck?',\n",
              " '\"c*nt\"',\n",
              " '*sniff*.',\n",
              " 's*x,',\n",
              " '*extended*',\n",
              " '50*',\n",
              " 'H*LL',\n",
              " '*lie*',\n",
              " 'politics*',\n",
              " 'C***s',\n",
              " '*Harboring',\n",
              " '*Milton',\n",
              " '*?@#',\n",
              " '*balanced*',\n",
              " 's&*%.',\n",
              " '*bologna',\n",
              " 'reasons.*',\n",
              " '**That',\n",
              " '2*pi*r,',\n",
              " \"***y'all\",\n",
              " '*single*',\n",
              " ':*)',\n",
              " 'Bonds*.',\n",
              " 's**t.',\n",
              " 'cr*w*ll.',\n",
              " '*style*',\n",
              " '*clown',\n",
              " '*%#@!head!\".',\n",
              " '**91%**',\n",
              " 'president*--the',\n",
              " '***n',\n",
              " '*GDP=Consumption+Investments+(Exports-Imports)',\n",
              " '*skills*,',\n",
              " 'b****ed,',\n",
              " 'ni**er,',\n",
              " 'more*',\n",
              " '*hide*',\n",
              " '*cause',\n",
              " 'not*',\n",
              " 'a**!',\n",
              " '*before*they',\n",
              " '**like',\n",
              " '*everywhere*!',\n",
              " '*Massachusetts*',\n",
              " 'sh*tcan',\n",
              " 'Head-Up-Your-A**',\n",
              " '[**yawn**],',\n",
              " '**ssy\".',\n",
              " 'SH*T,or',\n",
              " '2%-8%(*)',\n",
              " '*COATS:',\n",
              " '*asked*',\n",
              " 'Absolutely.**',\n",
              " '*ss\"',\n",
              " '**veiled',\n",
              " '*pure',\n",
              " \"*'Black_Dude'*\",\n",
              " '**Okay,',\n",
              " '*unlawful',\n",
              " 'h*ll,',\n",
              " \"*Isn't\",\n",
              " 'history*.',\n",
              " '*completed*',\n",
              " 'misogynist.*',\n",
              " 's*x',\n",
              " '*truism*',\n",
              " 'b*tches\"-',\n",
              " '*&^%,',\n",
              " '*indigenous*',\n",
              " '\"H***\"',\n",
              " '*tiny',\n",
              " 'S***+',\n",
              " 'b*lls',\n",
              " 'corru*T',\n",
              " 'p*sssy,',\n",
              " 'things,\"*',\n",
              " 'Incompetent*',\n",
              " 'fact**),',\n",
              " '*Is',\n",
              " 'G*d.',\n",
              " '...****anyway',\n",
              " 'A***,',\n",
              " 'hand*',\n",
              " '*poof*,',\n",
              " '\"*Please',\n",
              " \"'b***ches'.\",\n",
              " '*name*',\n",
              " 'W/m*m',\n",
              " '(*or*',\n",
              " 'time*.',\n",
              " 'examples***',\n",
              " 'model*',\n",
              " '*spent*',\n",
              " 'a**es,',\n",
              " '*witness',\n",
              " 'hours.****',\n",
              " 'Sh**!',\n",
              " 'thing*',\n",
              " 'revenue*',\n",
              " '*visited',\n",
              " 'c*ck-a-hoop',\n",
              " '\"***',\n",
              " 'form*',\n",
              " 'rats***',\n",
              " '***the',\n",
              " \"*doesn't*\",\n",
              " '*catholic*',\n",
              " 'a**-kisser',\n",
              " 'contractor\"**',\n",
              " '*\"was',\n",
              " 'dumb*#$$es.”',\n",
              " 'cough**',\n",
              " 'snap*',\n",
              " 'a**holes\"',\n",
              " '**,”',\n",
              " 'B***S',\n",
              " 'destiny...*****',\n",
              " 'TeeVee***',\n",
              " 'whatsoever*',\n",
              " 'articles*',\n",
              " '**Put',\n",
              " '\"*The',\n",
              " 'in***,',\n",
              " '*weird*',\n",
              " '*inserted*',\n",
              " '**A**dapt',\n",
              " 'faith*!',\n",
              " 'in...*please*',\n",
              " 'g*n.',\n",
              " 'b***tard',\n",
              " '**people**',\n",
              " '*Brag',\n",
              " 'Group*',\n",
              " 'Criteria*',\n",
              " 'Down*',\n",
              " '**being',\n",
              " 'eyes..*',\n",
              " 'p**sy.',\n",
              " '**\"Ice',\n",
              " '*\"Only,\"*',\n",
              " '\"(%*@!%!!*).',\n",
              " '*GOP-held*',\n",
              " 'careful*',\n",
              " '**I',\n",
              " 'spirited*',\n",
              " '*drops',\n",
              " 'H*ll',\n",
              " 'cl*sterf**k.',\n",
              " '****-life',\n",
              " 'administrations*,,',\n",
              " '***ideas***',\n",
              " 'p*****',\n",
              " 's#*^,',\n",
              " 'dictionary*...',\n",
              " 'Fu**ing',\n",
              " 'B^&&*^#*.',\n",
              " '\"r**ards,\"',\n",
              " '**inherit**',\n",
              " '********************************************************',\n",
              " \"*You're\",\n",
              " '*Around',\n",
              " '**A**',\n",
              " 'https://www.google.ca/#q=trump+fact+check&*',\n",
              " 'C*nt,',\n",
              " 'ashtrays.*',\n",
              " '*Literally*',\n",
              " '*solely*',\n",
              " 'truth*.',\n",
              " '*TEMPORARY*',\n",
              " 'sh*tshow',\n",
              " '*Jesus*-minded',\n",
              " '*showing',\n",
              " '*Greed',\n",
              " 'H*yas',\n",
              " 'Mother****er!',\n",
              " '*One',\n",
              " '**Yawn.**',\n",
              " 'suffering*',\n",
              " '*Not',\n",
              " 'S***',\n",
              " '**within',\n",
              " '****plus',\n",
              " '*due',\n",
              " 'f*ggot',\n",
              " 'f*cked-tier',\n",
              " 'were..**sob**',\n",
              " '*Everyone',\n",
              " 'Cr*p?',\n",
              " 'https://www.google.ca/#q=tea+party+koch+funding&*',\n",
              " 'bullshi**ers',\n",
              " 'b******t',\n",
              " 'stu**d.',\n",
              " 'F***er',\n",
              " 'LIGHT*',\n",
              " 'F**king',\n",
              " '*patriarchal*',\n",
              " '*rezoning',\n",
              " '*********************',\n",
              " 'Trudeau(*)',\n",
              " '*Spying',\n",
              " '*few*',\n",
              " 'civil.****if',\n",
              " '*context*',\n",
              " '***GROSS',\n",
              " '*Doot',\n",
              " 'V*******',\n",
              " 'Un*******',\n",
              " '*‘there’',\n",
              " \"'**sniffle**\",\n",
              " 'p****-grabbing,',\n",
              " 'engagement*.',\n",
              " '4*8',\n",
              " '*percentage*',\n",
              " '*did*.',\n",
              " 'behavior*',\n",
              " 'female*,',\n",
              " '*ss,',\n",
              " '*defined*',\n",
              " '*@#k',\n",
              " '*zero*',\n",
              " '*sweet*',\n",
              " 'f*ck*d',\n",
              " 'store.\"*',\n",
              " '*enabler',\n",
              " '*PRECISELY*',\n",
              " '*zero*.',\n",
              " 'sh*tty.',\n",
              " '*staff',\n",
              " '*not',\n",
              " 'dark*!',\n",
              " '*cough*',\n",
              " '*personally*',\n",
              " 'W*',\n",
              " '*Anchorage*',\n",
              " '*Comparing',\n",
              " 'p*s*y-grabs.',\n",
              " '*making',\n",
              " '*as*',\n",
              " '*or*',\n",
              " '(*&^.',\n",
              " '***David',\n",
              " '*rehabilitating*',\n",
              " '*crickets*',\n",
              " 'su****s',\n",
              " 's**tholes',\n",
              " '****stomped',\n",
              " '*each*',\n",
              " '*them*,',\n",
              " '$h*t',\n",
              " 'Alert****',\n",
              " 'BullSh**',\n",
              " '*&^%..',\n",
              " 'B*F*D.',\n",
              " 'r*t*rd',\n",
              " 'da*n',\n",
              " 'said.*',\n",
              " '*still*',\n",
              " 'jacka**.',\n",
              " 'companies.”****',\n",
              " 'weapons?*',\n",
              " '*Congress*',\n",
              " '*Increasing',\n",
              " 'F^#*',\n",
              " 'B****!',\n",
              " 'Right*',\n",
              " \"of*******'\",\n",
              " '2042.*',\n",
              " 'D*^#',\n",
              " '*specific*',\n",
              " 'Eastman.**',\n",
              " '*Specifically*-',\n",
              " 'b**tards',\n",
              " 'bu**',\n",
              " '($10*32',\n",
              " '*September',\n",
              " '*us*)',\n",
              " '*google',\n",
              " 'Desk*...Gebus...',\n",
              " 'A**hole',\n",
              " '*THEY*',\n",
              " '**Spend',\n",
              " '*Frog',\n",
              " 'F&*%!',\n",
              " 'wh*res.',\n",
              " '*neo-Nazis,',\n",
              " 'on**.',\n",
              " '*can*',\n",
              " '100*velocitybike',\n",
              " 'cr**!!!!',\n",
              " 'bast**ds.',\n",
              " 'f***(ng',\n",
              " 'sh*t-faced,',\n",
              " '%^^&*&',\n",
              " '*http://idccentral.com/beliefs/position-papers/',\n",
              " 'batsh*t',\n",
              " 'D*ck',\n",
              " 'P*ssy?',\n",
              " 'Neo-N*zis',\n",
              " 'f***ups,',\n",
              " '*Skeptics',\n",
              " '*Could*',\n",
              " '*green',\n",
              " 'q*eer',\n",
              " 'JOOB!!*\"',\n",
              " 'N******\".',\n",
              " 'L*M*F*A*O!!!!!!!',\n",
              " 'p*ssies...sad',\n",
              " 'memory*...',\n",
              " '**click...hmmmmm**',\n",
              " '\"bullsh*tter\",',\n",
              " '*ARE*',\n",
              " '**Dear',\n",
              " '(100-5.1)*0.396+5.1',\n",
              " '*real',\n",
              " 'f****ing',\n",
              " 'NEWS*',\n",
              " 'siren*',\n",
              " '*declined*',\n",
              " 'around*',\n",
              " '*Experts',\n",
              " '*witness*,',\n",
              " '*Elizabeth',\n",
              " 'prej*dice,',\n",
              " 'shi*',\n",
              " 'kick-a**',\n",
              " 'walking.***I',\n",
              " 'H***,',\n",
              " '1990****),',\n",
              " '*Direct',\n",
              " '(*From',\n",
              " 'Vv=t*g/2=6433',\n",
              " 'g*d',\n",
              " '*Leviticus',\n",
              " '****TODAY*I*VOTE*FOR*MARGARET*STOCK****',\n",
              " 'because*******',\n",
              " '*Thumbs',\n",
              " \"f***in'\",\n",
              " 'f**cked',\n",
              " 'sl*t.',\n",
              " 'smarta**',\n",
              " 'all*',\n",
              " '*four*',\n",
              " 'now*,',\n",
              " '*&^%#@',\n",
              " '*^@$',\n",
              " 'p***y),',\n",
              " 'lawyers.*',\n",
              " 'F*ggot',\n",
              " '**sustainable**',\n",
              " '3000*',\n",
              " '*willful*',\n",
              " '*whole*',\n",
              " 'bi*ch',\n",
              " '**Crickets**',\n",
              " '2*',\n",
              " '*ever*',\n",
              " '*EXACTLY*',\n",
              " 'costs*',\n",
              " 'close*?',\n",
              " '*different*!',\n",
              " '*Saddler,',\n",
              " '*sucker*',\n",
              " '**runs',\n",
              " '*conditioned*',\n",
              " '(N*)',\n",
              " 'DA?*N',\n",
              " 'chirping*',\n",
              " 'b*stards.....',\n",
              " 'claim*',\n",
              " '\"H***',\n",
              " '*anything',\n",
              " '*help*',\n",
              " '0.4*',\n",
              " '**WILL',\n",
              " 'pu**y).',\n",
              " 'b*****ds',\n",
              " 'decisions**',\n",
              " 'PLANET!*',\n",
              " '*actual*',\n",
              " '*fishy',\n",
              " 'bull****.',\n",
              " 'up*.',\n",
              " '*others*',\n",
              " \"*you're*\",\n",
              " 'a**hole,',\n",
              " 'equality......*groan*.',\n",
              " \"h*ll's\",\n",
              " '*gained*',\n",
              " 'Batsh*t',\n",
              " '*vast*.',\n",
              " '*YOUR*',\n",
              " '*Months',\n",
              " 'voting*',\n",
              " 'selection...*sigh',\n",
              " '*At',\n",
              " 'truck*,',\n",
              " '*near',\n",
              " 'li*rs.',\n",
              " '$#&*\"',\n",
              " 'blossomed\"*',\n",
              " '*See',\n",
              " 'SEA*,',\n",
              " 'sands*',\n",
              " '*black',\n",
              " '$*&#',\n",
              " '*Guest',\n",
              " '*earned*',\n",
              " '****d',\n",
              " '*informed*....flawed???',\n",
              " '#$%^&*',\n",
              " 'here*',\n",
              " 'clusterf^*k',\n",
              " '*of',\n",
              " 'pu&*y',\n",
              " '*saint*.',\n",
              " 'healthcare*',\n",
              " 'b*****s).',\n",
              " '*SoJuvenileAndTiresome,Too',\n",
              " 'P******',\n",
              " 'saying.*\"',\n",
              " '*Julie',\n",
              " '*59',\n",
              " '************************************',\n",
              " 'As*-town',\n",
              " 'n*****.\"',\n",
              " '*stupid*',\n",
              " 'anti-trump.***',\n",
              " 'year*,',\n",
              " '*totally*',\n",
              " 'f****d',\n",
              " '*yes*,',\n",
              " 'sh*t....',\n",
              " '*is',\n",
              " '*Claiming',\n",
              " 'constructed*',\n",
              " 'shops*',\n",
              " 'President*',\n",
              " 'ON*',\n",
              " '**readers**',\n",
              " '*lie*.',\n",
              " '**Don',\n",
              " '*that',\n",
              " 'Auuuugh!*',\n",
              " 'b#^%*.',\n",
              " 'RFLW*?',\n",
              " 'b*#@$&',\n",
              " 'sh**,',\n",
              " '*expressed',\n",
              " \"'grab-them-by-the-p***y'\",\n",
              " 'Hate-f**king',\n",
              " 'gravel*',\n",
              " '*too',\n",
              " 'g*****s.',\n",
              " 'P***',\n",
              " '**prayers',\n",
              " 'd*ck',\n",
              " 'internationally**-',\n",
              " 'c***h******.',\n",
              " '*hilarious*',\n",
              " '**Voter',\n",
              " '*pro-life*',\n",
              " 'K*K',\n",
              " 'Forest*',\n",
              " 'localp**k!',\n",
              " '*themselves*',\n",
              " 'Rivers*',\n",
              " '*the*',\n",
              " '*hat',\n",
              " '***,',\n",
              " '*ourselves*',\n",
              " '*BREAKING',\n",
              " 'P*SSIE\"',\n",
              " 'dum*mies',\n",
              " '*really,',\n",
              " 'professor.\"*',\n",
              " '*contribute*',\n",
              " '*brilliant*',\n",
              " 'so-called*',\n",
              " 'sh*tposting',\n",
              " 'error**,',\n",
              " 'p****grabber',\n",
              " 's**t!',\n",
              " '*during',\n",
              " '*blame*',\n",
              " '*Midtown*',\n",
              " '*blaming*',\n",
              " 'place?*',\n",
              " '*reads',\n",
              " '*Vazquez,',\n",
              " '*current',\n",
              " '*Would',\n",
              " '*shafted',\n",
              " '*nothing*',\n",
              " '*actions*.',\n",
              " '*doomed*',\n",
              " '**Not',\n",
              " '7*24,',\n",
              " '28*',\n",
              " 'N******s',\n",
              " '*C',\n",
              " '*employees*',\n",
              " '*reasonably*',\n",
              " '*Bell',\n",
              " '*which',\n",
              " 'men*.\"',\n",
              " 's***heads',\n",
              " '*person*',\n",
              " '*shunning*',\n",
              " 'horse-sh*t,',\n",
              " 'Market\"*',\n",
              " '*-ist,',\n",
              " '*investigation*',\n",
              " 'proven-to-be-highly-successful*',\n",
              " 'banksters!**',\n",
              " '7*24',\n",
              " '*teachers*,',\n",
              " 'B**tards.',\n",
              " 'grass*',\n",
              " '*?',\n",
              " 'guilty*',\n",
              " 'chirp*',\n",
              " '*CNN',\n",
              " 'L***',\n",
              " 'bi*ch.’',\n",
              " 'du*mb',\n",
              " 's*&^%k',\n",
              " 's*cking',\n",
              " \"s***'\",\n",
              " '*Arresting',\n",
              " '**.',\n",
              " 'f***ing',\n",
              " '*AT',\n",
              " '*IMO',\n",
              " '**larsenfinancial.us/2015/08/fascism-is-not-a-right-wing-ideology**',\n",
              " '**Pander',\n",
              " 'as*holes,',\n",
              " 's#$*#',\n",
              " '*would*',\n",
              " '*Please',\n",
              " '*http://blogs.chicagotribune.com/news_columnists_ezorn/2009/08/never-mind-the-anecdotes-do-canadians-like-their-health-care-system.html',\n",
              " '***How',\n",
              " '*researching',\n",
              " '*Colver,',\n",
              " 'Arctic....*',\n",
              " '*dedication*',\n",
              " 'A**miral.',\n",
              " '*Theresa',\n",
              " '*Insubordinate',\n",
              " 'McCarthyish*',\n",
              " 'p***ies',\n",
              " '*Joe',\n",
              " '\"*First',\n",
              " 'days*',\n",
              " '*Support',\n",
              " \"*Trump's\",\n",
              " '\"Da*ning',\n",
              " '*women*;',\n",
              " 'b%@&*',\n",
              " 'yourself*',\n",
              " '*want*',\n",
              " '*resulting',\n",
              " 'km*',\n",
              " '*eyes',\n",
              " '*meow*',\n",
              " '*$12']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv1xe2IfmW0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_comments['stared words'] = train_comments['comment_text'].apply(lambda comment: set(w for w in comment.split() if (w.count('*') > 0) and (w[0] != '*') and (w[-1] != '*')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUt_Du-9uBZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(set().union(*train_comments[train_comments[\"stared words\"] != set()]['stared words'].values\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWqFdlDGE45a",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess words and word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfUAopB1aYXv",
        "colab_type": "text"
      },
      "source": [
        "### helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnvkPsWaaXE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions\n",
        "\n",
        "# check blank string function\n",
        "def isBlank(myString):\n",
        "    myString = str(myString)\n",
        "    return not (myString and myString.strip())\n",
        "\n",
        "# tokenizers (a lot of feature vectors, raw)\n",
        "\n",
        "class tokenizeBasic():\n",
        "    def tokenize(self, s):\n",
        "        return s.split()\n",
        "class no_tokenize():\n",
        "    def tokenize(self, s):\n",
        "        return s\n",
        "\n",
        "# lemmatizer (doesn't remove punctuation)\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "\n",
        "# stemmer (no difference from lemmatizer?)\n",
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "#TODO\n",
        "\n",
        "def cleanUp(text):\n",
        "    # Initilaise Lemmatizer\n",
        "    lemm = WordNetLemmatizer()\n",
        "\n",
        "    # use alternative stemmer\n",
        "    #snowball = SnowballStemmer(language = 'english')\n",
        "    #ps = PorterStemmer()\n",
        "\n",
        "    # load stopwords\n",
        "    #my_stopwords = stopwords.words('english')\n",
        "    my_stopwords = []\n",
        "    clean_text = \"\"\n",
        "    # tokenize words (convert text from byte to string)\n",
        "    words = word_tokenize(str(text, errors=\"ignore\"))\n",
        "    # print(words[:8])\n",
        "\n",
        "    for word in words:\n",
        "\n",
        "        w = lemm.lemmatize(word.lower())\n",
        "        #w = re.sub('<.*?>', '', w) # remove HTML tags\n",
        "        #w = re.sub(r'[^\\w\\s</>]', '', w) # remove punc.\n",
        "        w = re.sub(r'\\d+','',w)# remove numbers\n",
        "        # lemmatize the word(normalized to lower case)\n",
        "        \n",
        "        # stem the word\n",
        "        #w = snowball.stem(w.lower())\n",
        "\n",
        "        # print(w)\n",
        "\n",
        "        # filter out stopwords\n",
        "        if w not in my_stopwords and len(w) > 0:\n",
        "          clean_text += w + \" \"\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def cleanUpPP(text):\n",
        "    # Initilaise Lemmatizer\n",
        "    lemm = WordNetLemmatizer()\n",
        "\n",
        "    # use alternative stemmer\n",
        "    #snowball = SnowballStemmer(language = 'english')\n",
        "    #ps = PorterStemmer()\n",
        "\n",
        "    # load stopwords\n",
        "    my_stopwords = stopwords.words('english')\n",
        "    my_stopwords = []\n",
        "    clean_text = \"\"\n",
        "    # tokenize words (convert text from byte to string)\n",
        "    words = word_tokenize(str(text, errors=\"ignore\"))\n",
        "    # print(words[:8])\n",
        "\n",
        "    for word in words:\n",
        "\n",
        "        w = lemm.lemmatize(word.lower())\n",
        "        #w = re.sub('<.*?>', '', w) # remove HTML tags\n",
        "        w = re.sub(r'[^\\w\\s</>]', '', w) # remove punc.\n",
        "        w = re.sub(r'\\d+','',w)# remove numbers\n",
        "        # lemmatize the word(normalized to lower case)\n",
        "        \n",
        "        # stem the word\n",
        "        #w = snowball.stem(w.lower())\n",
        "\n",
        "        # print(w)\n",
        "\n",
        "        # filter out stopwords\n",
        "        if w not in my_stopwords and len(w) > 0:\n",
        "          clean_text += w + \" \"\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    clean_data = []\n",
        "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
        "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
        "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
        "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
        "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
        "        if new_text != '':\n",
        "            clean_data.append(new_text)\n",
        "    return clean_data\n",
        "\n",
        "def tokenization_w(words):\n",
        "    w_new = []\n",
        "    for w in (words[:][0]):  # for NumPy = words[:]\n",
        "        w_token = word_tokenize(w)\n",
        "        if w_token != '':\n",
        "            w_new.append(w_token)\n",
        "    return w_new\n",
        "\n",
        "snowball = SnowballStemmer(language = 'english')\n",
        "def stemming(words):\n",
        "    new = []\n",
        "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
        "    new.append(stem_words)\n",
        "    return new\n",
        "    \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(words):\n",
        "    new = []\n",
        "    lem_words = [lemmatizer.lemmatize(x) for x in (words[:][0])]\n",
        "    new.append(lem_words)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyC4vlYRiLr5",
        "colab_type": "text"
      },
      "source": [
        "### custom tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbkDkFZ9-C7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create custom dictionary mappings for single symbols\n",
        "CUSTOM_TABLE = str.maketrans(\n",
        "    {\n",
        "        \"\\xad\": ' ',\n",
        "        \"\\x7f\": ' ',\n",
        "        \"\\ufeff\": ' ',\n",
        "        \"\\u200b\": ' ',\n",
        "        \"\\u200e\": ' ',\n",
        "        \"\\u202a\": ' ',\n",
        "        \"\\u202c\": ' ',\n",
        "        \"‘\": \"'\",\n",
        "        \"’\": \"'\",\n",
        "        \"`\": \"'\",\n",
        "        \"“\": '\"',\n",
        "        \"”\": '\"',\n",
        "        \"«\": '\"',\n",
        "        \"»\": '\"',\n",
        "        \"ɢ\": \"G\",\n",
        "        \"ɪ\": \"I\",\n",
        "        \"ɴ\": \"N\",\n",
        "        \"ʀ\": \"R\",\n",
        "        \"ʏ\": \"Y\",\n",
        "        \"ʙ\": \"B\",\n",
        "        \"ʜ\": \"H\",\n",
        "        \"ʟ\": \"L\",\n",
        "        \"ғ\": \"F\",\n",
        "        \"ᴀ\": \"A\",\n",
        "        \"ᴄ\": \"C\",\n",
        "        \"ᴅ\": \"D\",\n",
        "        \"ᴇ\": \"E\",\n",
        "        \"ᴊ\": \"J\",\n",
        "        \"ᴋ\": \"K\",\n",
        "        \"ᴍ\": \"M\",\n",
        "        \"Μ\": \"M\",\n",
        "        \"ᴏ\": \"O\",\n",
        "        \"ᴘ\": \"P\",\n",
        "        \"ᴛ\": \"T\",\n",
        "        \"ᴜ\": \"U\",\n",
        "        \"ᴡ\": \"W\",\n",
        "        \"ᴠ\": \"V\",\n",
        "        \"ĸ\": \"K\",\n",
        "        \"в\": \"B\",\n",
        "        \"м\": \"M\",\n",
        "        \"н\": \"H\",\n",
        "        \"т\": \"T\",\n",
        "        \"ѕ\": \"S\",\n",
        "        \"—\": \"-\",\n",
        "        \"–\": \"-\",\n",
        "        \"₹\": \" rupee \",\n",
        "        \"´\": \"'\",\n",
        "        \"°\": \" degree \",\n",
        "        \"€\": \" euro \",\n",
        "        \"™\": \" trade mark \",\n",
        "        \"√\": \" sqrt \",\n",
        "        \"×\": \"x\",\n",
        "        \"²\": \" squared \",\n",
        "        \"_\": \"-\",\n",
        "        \"£\": \" pounds \",\n",
        "        '∞': ' infinity ',\n",
        "        'θ': ' theta ',\n",
        "        '÷': ' divide ',\n",
        "        'α': ' alpha ',\n",
        "        '•': '.',\n",
        "        'à': 'a',\n",
        "        '−': '-',\n",
        "        'β': ' beta ',\n",
        "        '∅': ' empty set ',\n",
        "        '³': ' cubed ',\n",
        "        'π': ' pi ',\n",
        "        \"\\uf0d8\" : ' ',\n",
        "        '\\u2061' : ' ',\n",
        "        '\\x10' : ' ',\n",
        "        '\\x9d' : ' ',\n",
        "        '\\xa0' : ' ',\n",
        "        '„' : ' ',\n",
        "        '،':' ',\n",
        "        'ø': 'o',\n",
        "        'Ø': 'O',\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "#add custom words with *stars* here\n",
        "WORDS_REPLACER = [\n",
        "    (\"sh*t\", \"shit\"),\n",
        "    (\"s*it\", \"shit\"),\n",
        "    (\"s*t\", \"shit\"),\n",
        "    (\"sh**\", \"shit\"),\n",
        "    (\"shi*\", \"shit\"),\n",
        "    (\"s**t\", \"shit\"),\n",
        "    (\"s***\", \"shit\"),\n",
        "    (\"sh***ng\", \"shit\"),\n",
        "    (\"sh**ty\", \"shit\"),\n",
        "    (\"buls*t\", \"bullshit\"),\n",
        "    (\"bu***it\", \"bullshit\"),\n",
        "    (\"bull***t\", \"bullshit\"),\n",
        "    (\"bull**it\", \"bullshit\"),\n",
        "    (\"bull*hit\", \"bullshit\"),\n",
        "    (\"b*l*s**t\", \"bullshit\"),\n",
        "    (\"b*llsh*t\", \"bullshit\"),\n",
        "    (\"bull****\", \"bullshit\"),\n",
        "    (\"bat*hit\", \"batshit\"),\n",
        "    (\"bat**it\", \"batshit\"),\n",
        "    (\"bat***t\", \"batshit\"),\n",
        "    (\"bat****\", \"batshit\"),\n",
        "    (\"horse*hit\", \"horseshit\"),\n",
        "    (\"horse**it\", \"horseshit\"),\n",
        "    (\"horse***t\", \"horseshit\"),\n",
        "    (\"horse****\", \"horseshit\"),\n",
        "    (\"f*ck\", \"fuck\"),\n",
        "    (\"fu*k\", \"fuck\"),\n",
        "    (\"f*k\", \"fuck\"),\n",
        "    (\"f*word\", \"fuck\"),\n",
        "    (\"f**k\", \"fuck\"),\n",
        "    (\"f*k\", \"fuck\"),\n",
        "    (\"f*c*\", \"fuck\"),\n",
        "    (\"f***\", \"fuck\"),\n",
        "    (\"f*****g\", \"fucking\"),\n",
        "    (\"*ucking\",\"fucking\"),\n",
        "    (\"f**k*ng\", \"fucking\"),\n",
        "    (\"fu**ing\", \"fucking\"),\n",
        "    (\"p*ssy\", \"pussy\"),\n",
        "    (\"p*ss*\", \"pussy\"),\n",
        "    (\"p*ssies\", \"pussy\"),\n",
        "    (\"pus*ies\", \"pussy\"),\n",
        "    (\"pu**ies\", \"pussy\"),\n",
        "    (\"p***ies\", \"pussy\"),\n",
        "    (\"p***y\", \"pussy\"),\n",
        "    (\"p****y\", \"pussy\"),\n",
        "    (\"p****\", \"pussy\"),\n",
        "    (\"p*s*y\", \"pussy\"),\n",
        "    (\"p**sy\", \"pussy\"),\n",
        "    (\"pu**y\", \"pussy\"),\n",
        "    (\"pu***sy\", \"pussy\"),\n",
        "    (\"b*st*rd\", \"bastard\"),\n",
        "    (\"b*stard\", \"bastard\"),\n",
        "    (\"b**tard\", \"bastard\"),\n",
        "    (\"b***tard\", \"bastard\"),\n",
        "    (\"bas****\", \"bastard\"),\n",
        "    (\"bas**rd\", \"bastard\"),\n",
        "    (\"b*tch\", \"bitch\"),\n",
        "    (\"b**ch\", \"bitch\"),\n",
        "    (\"bi*ch\", \"bitch\"),\n",
        "    (\"**tch\", \"bitch\"),\n",
        "    (\"b***h\", \"bitch\"),\n",
        "    (\"bit*h\", \"bitch\"),\n",
        "    (\"pri*ck\", \"prick\"),\n",
        "    (\"pr*ck\", \"prick\"),\n",
        "    (\"h*ll\", \"hell\"),\n",
        "    (\"h**l\", \"hell\"),\n",
        "    (\"h***\", \"hell\"),\n",
        "    (\"cr*p\", \"crap\"),\n",
        "    (\"cra*\", \"crap\"),\n",
        "    (\"d*m\", \"dam\"),\n",
        "    (\"stu*pid\", \"stupid\"),\n",
        "    (\"st*pid\", \"stupid\"),\n",
        "    (\"n*gger\", \"nigger\"),\n",
        "    (\"n***ga\", \"nigger\"),\n",
        "    (\"ni*ger\", \"nigger\"),\n",
        "    (\"n**ger\", \"nigger\"),\n",
        "    (\"n***er\", \"nigger\"),\n",
        "    (\"ni**er\", \"nigger\"),\n",
        "    (\"ni***r\", \"nigger\"),\n",
        "    (\"ni****\", \"nigger\"),\n",
        "    (\"n****r\", \"nigger\"),\n",
        "    (\"ni**r\", \"nigger\"),\n",
        "    (\"pr*ck\", \"prick\"),\n",
        "    (\"p*nis\", \"penis\"),\n",
        "    (\"f*g\", \"fag\"),\n",
        "    (\"fagg*t\", \"faggot\"),\n",
        "    (\"fa**ot\", \"faggot\"),\n",
        "    (\"f*a*g*g*o*t\", \"faggot\"),\n",
        "    (\"ret**d\", \"retard\"),\n",
        "    (\"r**ard\", \"retard\"),\n",
        "    (\"scr*w\", \"screw\"),\n",
        "    (\"g*d\", \"god\"),\n",
        "    (\"s*x\", \"sex\"),\n",
        "    (\"a*s\", \"ass\"),\n",
        "    (\"a**h***\", \"asshole\"),\n",
        "    (\"assh*le\", \"asshole\"),\n",
        "    (\"a***ole\", \"asshole\"),\n",
        "    (\"ass*ole\", \"asshole\"),\n",
        "    (\"**shole\", \"asshole\"),\n",
        "    (\"***hole\", \"asshole\"),\n",
        "    (\"a**\", \"ass\"),\n",
        "    (\"***es\", \"asses\"),\n",
        "    (\"j**k**s\", \"jackass\"),\n",
        "    (\"jack***\", \"jackass\"),\n",
        "    (\"jack**s\", \"jackass\"),\n",
        "    (\"dumb*ss\", \"dumbass\"),\n",
        "    (\"dumb***\", \"dumbass\"),\n",
        "    (\"an**ly\", \"anally\"),\n",
        "    (\"an*l\", \"anal\"),\n",
        "    (\"p*ss*ng\", \"pissing\"),\n",
        "    (\"p*ss\", \"piss\"),\n",
        "    (\"pi**\", \"piss\"),\n",
        "    (\"pi*s\", \"piss\"),\n",
        "    (\"stu*pid\", \"stupid\"),\n",
        "    (\"st*pid\", \"stupid\"),\n",
        "    (\"stu**d\", \"stupid\"),\n",
        "    (\"stu***\", \"stupid\"),\n",
        "    (\"stup*d\", \"stupid\"),\n",
        "    (\"st*p*d\", \"stupid\"),\n",
        "    (\"scr*w\", \"screw\"),\n",
        "    (\"scr**\", \"screw\"),\n",
        "    (\"dam*\", \"damn\"),\n",
        "    (\"d*amn\", \"damn\"),\n",
        "    (\"t***\", \"twat\"),\n",
        "    (\"tw*t\", \"twat\"),\n",
        "    (\"tw**\", \"twat\"),\n",
        "    (\"d**k\", \"dick\"),\n",
        "    (\"di*k\", \"dick\"),\n",
        "    (\"di**\", \"dick\"),\n",
        "    (\"d***\", \"dick\"),\n",
        "    (\"d*ck\", \"dick\"),\n",
        "    (\"id*ot\", \"idiot\"),\n",
        "    (\"idi*t\", \"idiot\"),\n",
        "    (\"id**t\", \"idiot\"),\n",
        "    (\"ret*ard\", \"retard\"),\n",
        "    (\"ret*rd\", \"retard\"),\n",
        "    (\"c**k\", \"cock\"),\n",
        "    (\"co*k\", \"cock\"),\n",
        "    (\"c*ck\", \"cock\"),\n",
        "    (\"b*tt\", \"butt\"),\n",
        "    (\"b**t\", \"butt\"),\n",
        "    (\"s***ing\", \"sucking\"),\n",
        "    (\"c*rap\", \"crap\"),\n",
        "    (\"c**p\", \"crap\"),\n",
        "    (\"cr*p\", \"crap\"),\n",
        "    (\"fr*ud\", \"fraud\"),\n",
        "    (\"fr**d\", \"fraud\"),\n",
        "    (\"wh*r\",\"whor\"),\n",
        "    (\"w**re\",\"whore\"),\n",
        "    (\"w*ore\",\"whore\"),\n",
        "    (\"wh**e\",\"whore\"),\n",
        "    (\"sl*t\",\"slut\"),\n",
        "    (\"c**t\",\"cunt\"),\n",
        "    (\"c*nt\",\"cunt\"),\n",
        "    (\"du*mb\",\"dumb\"),\n",
        "    (\"d*mb\",\"dumb\"),\n",
        "    (\"b*lls\",\"balls\"),\n",
        "    (\"p*rn\",\"porn\"),\n",
        "    (\"n*zi\",\"nazi\"),\n",
        "    (\"t*rd\",\"turd\"),\n",
        "    (\"q*eer\",\"queer\"),\n",
        "    (\"li*r\",\"liar\"),\n",
        "]\n",
        " \n",
        "REGEX_REPLACER = [\n",
        "    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n",
        "    for pat, repl in WORDS_REPLACER\n",
        "]\n",
        "\n",
        "RE_SPACE = re.compile(r\"\\s\")\n",
        "RE_MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "\n",
        "#remove mark nonspace characters\n",
        "NMS_TABLE = dict.fromkeys(\n",
        "    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n",
        ")\n",
        "\n",
        "#set language specfic characters to same letter\n",
        "HEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\n",
        "ARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\n",
        "CHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\n",
        "KANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\n",
        "HIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\n",
        "KATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n",
        "\n",
        "#setup dictionary table\n",
        "TABLE = dict()\n",
        "TABLE.update(CUSTOM_TABLE)\n",
        "TABLE.update(NMS_TABLE)\n",
        "# Non-english languages\n",
        "TABLE.update(CHINESE_TABLE)\n",
        "TABLE.update(HEBREW_TABLE)\n",
        "TABLE.update(ARABIC_TABLE)\n",
        "TABLE.update(HIRAGANA_TABLE)\n",
        "TABLE.update(KATAKANA_TABLE)\n",
        "TABLE.update(KANJI_TABLE)\n",
        "\n",
        "#normalize emojis\n",
        "EMOJI_REGEXP = emoji.get_emoji_regexp()\n",
        "\n",
        "UNICODE_EMOJI_MY = {\n",
        "    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n",
        "    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n",
        "}\n",
        "\n",
        "def my_demojize(string: str) -> str:\n",
        "    def replace(match):\n",
        "        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n",
        "\n",
        "    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2f35tICni3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#punctuation spacing\n",
        "extra_punct = [\n",
        "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
        "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
        "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
        "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
        "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
        "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
        "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
        "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
        "\n",
        "regular_punct = list(string.punctuation)\n",
        "#''.join(all_punct)\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "all_punct = list(set(regular_punct + extra_punct + list(punct)))\n",
        "\n",
        "all_punct_string = ''.join(all_punct)\n",
        "# do not spacing - ,* and .\n",
        "all_punct.remove('-')\n",
        "all_punct.remove('.')\n",
        "all_punct.remove('*')\n",
        "def spacing_punctuation(text):\n",
        "    \"\"\"\n",
        "    add space before and after punctuation and symbols\n",
        "    \"\"\"\n",
        "    for punc in all_punct:\n",
        "        if punc in text:\n",
        "           text = text.replace(punc, f' {punc} ')\n",
        "    return text\n",
        "\n",
        "def spacing_hash(text):\n",
        "    if '#' in text:\n",
        "        text = text.replace('#', ' #')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cexlMVE2svCG",
        "colab_type": "code",
        "outputId": "52c00c7c-4db5-44ad-f833-46d210c17b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#space connected words\n",
        "mis_connect_list = ['\\b(W|w)hat\\b', '\\b(W|w)hy\\b', '\\b(H|h)ow\\b', '\\b(W|w)hich\\b', '\\b(W|w)here\\b', '\\b(W|w)ill\\b']\n",
        "mis_connect_re  = re.compile('(%s)' % '|'.join(mis_connect_list))\n",
        "\n",
        "mis_spell_mapping = {'whattsup': 'what is up', 'whatasapp':'WhatsApp', 'whatsupp':'what is up', \n",
        "                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n",
        "                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n",
        "                      'Whybis':'Why is', 'laowhy86':'Foreigners who do not respect China',\n",
        "                      'Whyco-education':'Why co-education',\n",
        "                      \"Howddo\":\"How do\", 'Howeber':'However', 'Showh':'Show',\n",
        "                      \"Willowmagic\":'Willow magic', 'WillsEye':'Will Eye', 'Williby':'will by',\n",
        "                     'pretextt':'pre text','amette':'annette','Tridentinus':'mushroom',\n",
        "                    'dailycaller':'daily caller'}\n",
        "\n",
        "def spacing_some_connect_words(text):\n",
        "    \"\"\"\n",
        "    'Whyare' -> 'Why are'\n",
        "    \"\"\"\n",
        "    for error in mis_spell_mapping:\n",
        "        if error in text:\n",
        "            text = text.replace(error, mis_spell_mapping[error])\n",
        "        elif error.capitalize() in text:\n",
        "            text = text.replace(error.capitalize(), mis_spell_mapping[error])\n",
        "        elif error.lower() in text:\n",
        "            text = text.replace(error.lower(), mis_spell_mapping[error])\n",
        "        elif error.upper() in text:\n",
        "            text = text.replace(error.upper(), mis_spell_mapping[error])\n",
        "\n",
        "    text = re.sub(r\"((W|w)hat+(s)*[A|a]*(p)+)\", \" WhatsApp \", text)\n",
        "    text = re.sub(r\"((W|w)hat)\", \" What \", text)\n",
        "    #text = re.sub(r\"(W|w)hat\", \" What \", text)\n",
        "    text = re.sub(r\"((W|w)hy)\", \" Why \", text)\n",
        "    #text = re.sub(r\"(W|w)hy\", \" Why \", text)\n",
        "    #text = re.sub(r\"(H|h)ow\", \" How \", text)\n",
        "    text = re.sub(r\"((H|h)ow)\", \" How \", text)\n",
        "    text = re.sub(r\"((W|w)hich)\", \" Which \", text)\n",
        "    #text = re.sub(r\"\\S(W|w)hich\", \" Which \", text)\n",
        "    #text = re.sub(r\"(W|w)here\\S\", \" Where \", text)\n",
        "    text = re.sub(r\"((W|w)here)\", \" Where \", text)\n",
        "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
        "    text = text.replace(\"What sApp\", ' WhatsApp ')\n",
        "    \n",
        "    return text\n",
        "print(spacing_some_connect_words('iwhichknowwherehowhowahowhowhowawhatappwhatapphecanhowis hehow therehow'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i Which know Where  How  How a How  How  How a   WhatsApp     WhatsApp  hecan How is he How  there How \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB0mKGJg-5DN",
        "colab_type": "text"
      },
      "source": [
        "twitter text preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfmqJrH8P2xy",
        "colab_type": "code",
        "outputId": "b68a1474-e0db-4e93-8707-1fafa0a5c719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#text preprocessor for normalizing\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #    'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
        "    #tokenizer=tokenizeBasic().tokenize,\n",
        "    #tokenizer=no_tokenize().tokenize,\n",
        "    #tokenizer = TweetTokenizer(strip_handles=False, reduce_len=False).tokenize,\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")\n",
        "#twitter preprocessor\n",
        "def cleanUpT(s):\n",
        "  return \" \".join(text_processor.pre_process_doc(s))\n",
        "\n",
        "# doesn't work well\n",
        "def spell_correct(s):\n",
        "    new_s = []\n",
        "    sp = SpellCorrector(corpus=\"english\")\n",
        "    # spell correct\n",
        "    t = text_processor.pre_process_doc(s)\n",
        "    t = [sp.correct(word) for word in t]\n",
        "    return \" \".join(t)\n",
        "\n",
        "print(text_processor.pre_process_doc('WhyIs isn\\'t can\\'t @$%%# <> this whyamihere #lovewordpp is where********FUCK on www.google.com... :) >.<'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n",
            "['WhyIs', 'is', 'not', 'can', 'not', '@', '$%', '%', '#', '<', '>', 'this', 'whyamihere', 'love', 'word', 'pp', 'is', 'where********FUCK', 'on', '<url>', '<happy>', '<annoyed>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL-tipna2aA7",
        "colab_type": "text"
      },
      "source": [
        "#### extra mapping/functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnO-dhrSMmdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#contraction preprocessing\n",
        "contraction_mapping = {\n",
        "    \"trump's\" : 'Trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
        "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n",
        "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
        "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
        "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
        "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
        "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
        "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
        "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
        "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
        "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
        "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
        "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
        "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
        "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
        "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
        "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
        "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
        "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
        "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
        "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
        "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
        "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
        "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
        "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
        "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
        "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
        "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
        "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
        "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
        "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
        "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
        "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
        "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
        "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
        "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
        "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
        "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
        "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
        "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
        "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
        "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
        "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
        "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
        "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
        "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
        "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
        "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
        "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
        "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
        "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
        "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
        "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
        "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
        "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
        "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
        "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
        "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
        "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
        "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
        "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
        "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
        "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
        "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
        "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
        "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
        "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
        "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
        "    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
        "    'trumpCare':'Trump care','trumpcare':'Trump care', 'obamacare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
        "    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','sᴛᴀʀᴛ':'start','gubmit':'submit','co₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
        "    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
        "    'gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
        "    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','mᴀᴋᴇ':'make',\\\n",
        "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
        "    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'sᴛaʀᴛ':' start', 'shopo':'shop', 'let’s':'let us',\n",
        "    }\n",
        "\n",
        "def correct_contraction(x, dic):\n",
        "    for word in dic.keys():\n",
        "        if word in x:\n",
        "            x = x.replace(word, dic[word])\n",
        "        elif word.capitalize() in x:\n",
        "            x = x.replace(word.capitalize(), dic[word])\n",
        "        elif word.upper() in x:\n",
        "            x = x.replace(word.upper(), dic[word])\n",
        "        elif word.lower() in x:\n",
        "            x = x.replace(word.lower(), dic[word])\n",
        "            \n",
        "    return x\n",
        "\n",
        "\n",
        "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
        "                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
        "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
        "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
        "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
        "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
        "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
        "                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
        "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
        "                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n",
        "                      'i.e.': 'in other words', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
        "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
        "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
        "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
        "                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
        "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
        "                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull', 'sh*+': 'shit', 'sh*+s': 'shits', 's**+s': 'shits', 's**+': 'shit', 'W*mbs': 'Wombs', 'f*$K': 'fuck',\n",
        "                      }\n",
        "\n",
        "def pre_clean_rare_words(text):\n",
        "    for rare_word in rare_words_mapping:\n",
        "        if rare_word in text:\n",
        "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
        "        elif rare_word.capitalize() in text:\n",
        "            text = text.replace(rare_word.capitalize(), rare_words_mapping[rare_word])\n",
        "        elif rare_word.upper() in text:\n",
        "            text = text.replace(rare_word.upper(), rare_words_mapping[rare_word])\n",
        "        elif rare_word.lower() in text:\n",
        "            text = text.replace(rare_word.lower(), rare_words_mapping[rare_word])\n",
        "    return text\n",
        "\n",
        "\n",
        "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom',\n",
        "                'washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver',\n",
        "                'SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league',\n",
        "                'Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities',\n",
        "                'ᴀ':'a', '😉':' EMJ wink ','😂':' EMJ joy ','😀':' EMJ stuck out tongue ', 'theguardian':'the guardian','deplorables':'deplorable',\n",
        "                'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl',\n",
        "                'designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations',\n",
        "                'dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
        "\n",
        "def clean_misspell(text):\n",
        "    for bad_word in mispell_dict:\n",
        "        if bad_word in text:\n",
        "            text = text.replace(bad_word, mispell_dict[bad_word])\n",
        "        elif bad_word.capitalize() in text:\n",
        "            text = text.replace(bad_word.capitalize(), mispell_dict[bad_word])\n",
        "        elif bad_word.upper() in text:\n",
        "            text = text.replace(bad_word.upper(), mispell_dict[bad_word])\n",
        "        elif bad_word.lower() in text:\n",
        "            text = text.replace(bad_word.lower(), mispell_dict[bad_word])\n",
        "            \n",
        "    return text\n",
        "\n",
        "bad_case_words = {'nationalpost':'national post','businessinsider':'business insider','jewprofits': 'jew profits', 'QMAS': 'Quality Migrant Admission Scheme', 'casterating': 'castrating',\n",
        "                  'Kashmiristan': 'Kashmir', 'CareOnGo': 'India first and largest Online distributor of medicines',\n",
        "                  'Setya Novanto': 'a former Indonesian politician', 'TestoUltra': 'male sexual enhancement supplement',\n",
        "                  'rammayana': 'ramayana', 'Badaganadu': 'Brahmin community that mainly reside in Karnataka',\n",
        "                  'bitcjes': 'bitches', 'mastubrate': 'masturbate', 'Français': 'France',\n",
        "                  'Adsresses': 'address', 'flemmings': 'flemming', 'intermate': 'inter mating', 'feminisam': 'feminism',\n",
        "                  'cuckholdry': 'cuckold', 'Niggor': 'black hip-hop and electronic artist', 'narcsissist': 'narcissist',\n",
        "                  'Genderfluid': 'Gender fluid', ' Im ': ' I am ', ' dont ': ' do not ', 'Qoura': 'Quora',\n",
        "                  'ethethnicitesnicites': 'ethnicity', 'Namit Bathla': 'Content Writer', 'What sApp': 'WhatsApp',\n",
        "                  'Führer': 'Fuhrer', 'covfefe': 'coverage', 'accedentitly': 'accidentally', 'Cuckerberg': 'Zuckerberg',\n",
        "                  'transtrenders': 'incredibly disrespectful to real transgender people',\n",
        "                  'frozen tamod': 'Pornographic website', 'hindians': 'North Indian', 'hindian': 'North Indian',\n",
        "                  'celibatess': 'celibates', 'Trimp': 'Trump', 'wanket': 'wanker', 'wouldd': 'would',\n",
        "                  'arragent': 'arrogant', 'Ra - apist': 'rapist', 'idoot': 'idiot', 'gangstalkers': 'gangs talkers',\n",
        "                  'toastsexual': 'toast sexual', 'inapropriately': 'inappropriately', 'dumbassess': 'dumbass',\n",
        "                  'germanized': 'become german', 'helisexual': 'sexual', 'regilious': 'religious',\n",
        "                  'timetraveller': 'time traveller', 'darkwebcrawler': 'dark webcrawler', 'routez': 'route',\n",
        "                  'trumpians': 'Trump supporters','Trumpster':'trumpeters', 'irreputable': 'reputation', 'serieusly': 'seriously',\n",
        "                  'anti cipation': 'anticipation', 'microaggression': 'micro aggression', 'Afircans': 'Africans',\n",
        "                  'microapologize': 'micro apologize', 'Vishnus': 'Vishnu', 'excritment': 'excitement',\n",
        "                  'disagreemen': 'disagreement', 'gujratis': 'gujarati', 'gujaratis': 'gujarati',\n",
        "                  'ugggggggllly': 'ugly',\n",
        "                  'Germanity': 'German', 'SoyBoys': 'cuck men lacking masculine characteristics',\n",
        "                  'н': 'h', 'м': 'm', 'ѕ': 's', 'т': 't', 'в': 'b', 'υ': 'u', 'ι': 'i',\n",
        "                  'genetilia': 'genitalia', 'r - apist': 'rapist', 'Borokabama': 'Barack Obama',\n",
        "                  'arectifier': 'rectifier', 'pettypotus': 'petty potus', 'magibabble': 'magi babble',\n",
        "                  'nothinking': 'thinking', 'centimiters': 'centimeters', 'saffronized': 'India, politics, derogatory',\n",
        "                  'saffronize': 'India, politics, derogatory', ' incect ': ' insect ', 'weenus': 'elbow skin',\n",
        "                  'Pakistainies': 'Pakistanis', 'goodspeaks': 'good speaks', 'inpregnated': 'in pregnant',\n",
        "                  'rapefilms': 'rape films', 'rapiest': 'rapist', 'hatrednesss': 'hatred',\n",
        "                  'heightism': 'height discrimination', 'getmy': 'get my', 'onsocial': 'on social',\n",
        "                  'worstplatform': 'worst platform', 'platfrom': 'platform', 'instagate': 'instigate',\n",
        "                  'Loy Machedeo': 'person', ' dsire ': ' desire ', 'iservant': 'servant', 'intelliegent': 'intelligent',\n",
        "                  'WW 1': ' WW1 ', 'WW 2': ' WW2 ', 'ww 1': ' WW1 ', 'ww 2': ' WW2 ',\n",
        "                  'keralapeoples': 'kerala peoples', 'trumpervotes': 'trumper votes', 'fucktrumpet': 'fuck trumpet',\n",
        "                  'likebJaish': 'like bJaish', 'likemy': 'like my', 'Howlikely': 'How likely',\n",
        "                  'disagreementts': 'disagreements', 'disagreementt': 'disagreement',\n",
        "                  'meninist': \"male chauvinism\", 'feminists': 'feminism supporters', 'Ghumendra': 'Bhupendra',\n",
        "                  'emellishments': 'embellishments',\n",
        "                  'settelemen': 'settlement',\n",
        "                  'Richmencupid': 'rich men dating website', 'richmencupid': 'rich men dating website',\n",
        "                  'Gaudry - Schost': '', 'ladymen': 'ladyboy', 'hasserment': 'Harassment',\n",
        "                  'instrumentalizing': 'instrument', 'darskin': 'dark skin', 'balckwemen': 'balck women',\n",
        "                  'recommendor': 'recommender', 'wowmen': 'women', 'expertthink': 'expert think',\n",
        "                  'whitesplaining': 'white splaining', 'Inquoraing': 'inquiring', 'whilemany': 'while many',\n",
        "                  'manyother': 'many other', 'involvedinthe': 'involved in the', 'slavetrade': 'slave trade',\n",
        "                  'aswell': 'as well', 'fewshowanyRemorse': 'few show any Remorse', 'trageting': 'targeting',\n",
        "                  'getile': 'gentile', 'Gujjus': 'derogatory Gujarati', 'judisciously': 'judiciously',\n",
        "                  'Hue Mungus': 'feminist bait', 'Hugh Mungus': 'feminist bait', 'Hindustanis': '',\n",
        "                  'Virushka': 'Great Relationships Couple', 'exclusinary': 'exclusionary', 'himdus': 'hindus',\n",
        "                  'Milo Yianopolous': 'a British polemicist', 'hidusim': 'hinduism',\n",
        "                  'holocaustable': 'holocaust', 'evangilitacal': 'evangelical', 'Busscas': 'Buscas',\n",
        "                  'holocaustal': 'holocaust', 'incestious': 'incestuous', 'Tennesseus': 'Tennessee',\n",
        "                  'GusDur': 'Gus Dur',\n",
        "                  'RPatah - Tan Eng Hwan': 'Silsilah', 'Reinfectus': 'reinfect', 'pharisaistic': 'pharisaism',\n",
        "                  'nuslims': 'Muslims', 'taskus': '', 'musims': 'Muslims',\n",
        "                  'Musevi': 'the independence of Mexico', ' racious ': 'discrimination expression of racism',\n",
        "                  'Muslimophobia': 'Muslim phobia', 'justyfied': 'justified', 'holocause': 'holocaust',\n",
        "                  'musilim': 'Muslim', 'misandrous': 'misandry', 'glrous': 'glorious', 'desemated': 'decimated',\n",
        "                  'votebanks': 'vote banks', 'Parkistan': 'Pakistan', 'Eurooe': 'Europe', 'animlaistic': 'animalistic',\n",
        "                  'Asiasoid': 'Asian', 'Congoid': 'Congolese', 'inheritantly': 'inherently',\n",
        "                  'Asianisation': 'Becoming Asia',\n",
        "                  'Russosphere': 'russia sphere of influence', 'exMuslims': 'Ex-Muslims',\n",
        "                  'discriminatein': 'discrimination', ' hinus ': ' hindus ', 'Nibirus': 'Nibiru',\n",
        "                  'habius - corpus': 'habeas corpus', 'prentious': 'pretentious', 'Sussia': 'ancient Jewish village',\n",
        "                  'moustachess': 'moustaches', 'Russions': 'Russians', 'Yuguslavia': 'Yugoslavia',\n",
        "                  'atrocitties': 'atrocities', 'Muslimophobe': 'Muslim phobic', 'fallicious': 'fallacious',\n",
        "                  'recussed': 'recursed', '@ usafmonitor': '', 'lustfly': 'lustful', 'canMuslims': 'can Muslims',\n",
        "                  'journalust': 'journalist', 'digustingly': 'disgustingly', 'harasing': 'harassing',\n",
        "                  'greatuncle': 'great uncle', 'Drumpf': 'Trump', 'rejectes': 'rejected', 'polyagamous': 'polygamous',\n",
        "                  'Mushlims': 'Muslims', 'accusition': 'accusation', 'geniusses': 'geniuses',\n",
        "                  'moustachesomething': 'moustache something', 'heineous': 'heinous',\n",
        "                  'Sapiosexuals': 'sapiosexual', 'sapiosexuals': 'sapiosexual', 'Sapiosexual': 'sapiosexual',\n",
        "                  'sapiosexual': 'Sexually attracted to intelligence', 'pansexuals': 'pansexual',\n",
        "                  'autosexual': 'auto sexual', 'sexualSlutty': 'sexual Slutty', 'hetorosexuality': 'hetoro sexuality',\n",
        "                  'chinesese': 'chinese', 'pizza gate': 'debunked conspiracy theory',\n",
        "                  'countryless': 'Having no country',\n",
        "                  'muslimare': 'Muslim are', 'iPhoneX': 'iPhone', 'lionese': 'lioness', 'marionettist': 'Marionettes',\n",
        "                  'demonetize': 'demonetized', 'eneyone': 'anyone', 'Karonese': 'Karo people Indonesia',\n",
        "                  'minderheid': 'minder worse', 'mainstreamly': 'mainstream', 'contraproductive': 'contra productive',\n",
        "                  'diffenky': 'differently', 'abandined': 'abandoned', 'p0 rnstars': 'pornstars',\n",
        "                  'overproud': 'over proud',\n",
        "                  'cheekboned': 'cheek boned', 'heriones': 'heroines', 'eventhogh': 'even though',\n",
        "                  'americanmedicalassoc': 'american medical assoc', 'feelwhen': 'feel when', 'Hhhow': 'how',\n",
        "                  'reallySemites': 'really Semites', 'gamergaye': 'gamersgate', 'manspreading': 'man spreading',\n",
        "                  'thammana': 'Tamannaah Bhatia', 'dogmans': 'dogmas', 'managementskills': 'management skills',\n",
        "                  'mangoliod': 'mongoloid', 'geerymandered': 'gerrymandered', 'mandateing': 'man dateing',\n",
        "                  'Romanium': 'Romanum',\n",
        "                  'mailwoman': 'mail woman', 'humancoalition': 'human coalition',\n",
        "                  'manipullate': 'manipulate', 'everyo0 ne': 'everyone', 'takeove': 'takeover',\n",
        "                  'Nonchristians': 'Non Christians', 'goverenments': 'governments', 'govrment': 'government',\n",
        "                  'polygomists': 'polygamists', 'Demogorgan': 'Demogorgon', 'maralago': 'Mar-a-Lago',\n",
        "                  'antibigots': 'anti bigots', 'gouing': 'going', 'muzaffarbad': 'muzaffarabad',\n",
        "                  'suchvstupid': 'such stupid', 'apartheidisrael': 'apartheid israel', \n",
        "                  'personaltiles': 'personal titles', 'lawyergirlfriend': 'lawyer girl friend',\n",
        "                  'northestern': 'northwestern', 'yeardold': 'years old', 'masskiller': 'mass killer',\n",
        "                  'southeners': 'southerners', 'Unitedstatesian': 'United states',\n",
        "\n",
        "                  'peoplekind': 'people kind', 'peoplelike': 'people like', 'countrypeople': 'country people',\n",
        "                  'shitpeople': 'shit people', 'trumpology': 'trump ology', 'trumpites': 'Trump supporters',\n",
        "                  'trumplies': 'trump lies', 'donaldtrumping': 'donald trumping', 'trumpdating': 'trump dating',\n",
        "                  'trumpsters': 'trumpeters','Trumpers':'president trump', 'ciswomen': 'cis women', 'womenizer': 'womanizer',\n",
        "                  'pregnantwomen': 'pregnant women', 'autoliker': 'auto liker', 'smelllike': 'smell like',\n",
        "                  'autolikers': 'auto likers', 'religiouslike': 'religious like', 'likemail': 'like mail',\n",
        "                  'fislike': 'dislike', 'sneakerlike': 'sneaker like', 'like⬇': 'like',\n",
        "                  'likelovequotes': 'like lovequotes', 'likelogo': 'like logo', 'sexlike': 'sex like',\n",
        "                  'Whatwould': 'What would', 'Howwould': 'How would', 'manwould': 'man would',\n",
        "                  'exservicemen': 'ex servicemen', 'femenism': 'feminism', 'devopment': 'development',\n",
        "                  'doccuments': 'documents', 'supplementplatform': 'supplement platform', 'mendatory': 'mandatory',\n",
        "                  'moviments': 'movements', 'Kremenchuh': 'Kremenchug', 'docuements': 'documents',\n",
        "                  'determenism': 'determinism', 'envisionment': 'envision ment',\n",
        "                  'tricompartmental': 'tri compartmental', 'AddMovement': 'Add Movement',\n",
        "                  'mentionong': 'mentioning', 'Whichtreatment': 'Which treatment', 'repyament': 'repayment',\n",
        "                  'insemenated': 'inseminated', 'inverstment': 'investment',\n",
        "                  'managemental': 'manage mental', 'Inviromental': 'Environmental', 'menstrution': 'menstruation',\n",
        "                  'indtrument': 'instrument', 'mentenance': 'maintenance', 'fermentqtion': 'fermentation',\n",
        "                  'achivenment': 'achievement', 'mismanagements': 'mis managements', 'requriment': 'requirement',\n",
        "                  'denomenator': 'denominator', 'drparment': 'department', 'acumens': 'acumen s',\n",
        "                  'celemente': 'Clemente', 'manajement': 'management', 'govermenent': 'government',\n",
        "                  'accomplishmments': 'accomplishments', 'rendementry': 'rendement ry',\n",
        "                  'repariments': 'departments', 'menstrute': 'menstruate', 'determenistic': 'deterministic',\n",
        "                  'resigment': 'resignment', 'selfpayment': 'self payment', 'imrpovement': 'improvement',\n",
        "                  'enivironment': 'environment', 'compartmentley': 'compartment',\n",
        "                  'augumented': 'augmented', 'parmenent': 'permanent', 'dealignment': 'de alignment',\n",
        "                  'develepoments': 'developments', 'menstrated': 'menstruated', 'phnomenon': 'phenomenon',\n",
        "                  'Employmment': 'Employment', 'dimensionalise': 'dimensional ise', 'menigioma': 'meningioma',\n",
        "                  'recrument': 'recrement', 'Promenient': 'Provenient', 'gonverment': 'government',\n",
        "                  'statemment': 'statement', 'recuirement': 'requirement', 'invetsment': 'investment',\n",
        "                  'parilment': 'parchment', 'parmently': 'patiently', 'agreementindia': 'agreement india',\n",
        "                  'menifesto': 'manifesto', 'accomplsihments': 'accomplishments', 'disangagement': 'disengagement',\n",
        "                  'aevelopment': 'development', 'procument': 'procumbent', 'harashment': 'harassment',\n",
        "                  'Tiannanmen': 'Tiananmen', 'commensalisms': 'commensal isms', 'devlelpment': 'development',\n",
        "                  'dimensons': 'dimensions', 'recruitment2017': 'recruitment 2017', 'polishment': 'pol ishment',\n",
        "                  'CommentSafe': 'Comment Safe', 'meausrements': 'measurements', 'geomentrical': 'geometrical',\n",
        "                  'undervelopment': 'undevelopment', 'mensurational': 'mensuration al', 'fanmenow': 'fan menow',\n",
        "                  'permenganate': 'permanganate', 'bussinessmen': 'businessmen',\n",
        "                  'supertournaments': 'super tournaments', 'permanmently': 'permanently',\n",
        "                  'lamenectomy': 'lamnectomy', 'assignmentcanyon': 'assignment canyon', 'adgestment': 'adjustment',\n",
        "                  'mentalized': 'metalized', 'docyments': 'documents', 'requairment': 'requirement',\n",
        "                  'batsmencould': 'batsmen could', 'argumentetc': 'argument etc', 'enjoiment': 'enjoyment',\n",
        "                  'invement': 'movement', 'accompliushments': 'accomplishments', 'regements': 'regiments',\n",
        "                  'departmentHow': 'department How', 'Aremenian': 'Armenian', 'amenclinics': 'amen clinics',\n",
        "                  'nonfermented': 'non fermented', 'Instumentation': 'Instrumentation', 'mentalitiy': 'mentality',\n",
        "                  ' govermen ': 'goverment', 'underdevelopement': 'under developement', 'parlimentry': 'parliamentary',\n",
        "                  'indemenity': 'indemnity', 'Inatrumentation': 'Instrumentation', 'menedatory': 'mandatory',\n",
        "                  'mentiri': 'entire', 'accomploshments': 'accomplishments', 'instrumention': 'instrument ion',\n",
        "                  'afvertisements': 'advertisements', 'parlementarian': 'parlement arian',\n",
        "                  'entitlments': 'entitlements', 'endrosment': 'endorsement', 'improment': 'impriment',\n",
        "                  'archaemenid': 'Achaemenid', 'replecement': 'replacement', 'placdment': 'placement',\n",
        "                  'femenise': 'feminise', 'envinment': 'environment', 'AmenityCompany': 'Amenity Company',\n",
        "                  'increaments': 'increments', 'accomplihsments': 'accomplishments',\n",
        "                  'manygovernment': 'many government', 'panishments': 'punishments', 'elinment': 'eloinment',\n",
        "                  'mendalin': 'mend alin', 'farmention': 'farm ention', 'preincrement': 'pre increment',\n",
        "                  'postincrement': 'post increment', 'achviements': 'achievements', 'menditory': 'mandatory',\n",
        "                  'Emouluments': 'Emoluments', 'Stonemen': 'Stone men', 'menmium': 'medium',\n",
        "                  'entaglement': 'entanglement', 'integumen': 'integument', 'harassument': 'harassment',\n",
        "                  'retairment': 'retainment', 'enviorement': 'environment', 'tormentous': 'torment ous',\n",
        "                  'confiment': 'confident', 'Enchroachment': 'Encroachment', 'prelimenary': 'preliminary',\n",
        "                  'fudamental': 'fundamental', 'instrumenot': 'instrument', 'icrement': 'increment',\n",
        "                  'prodimently': 'prominently', 'meniss': 'menise', 'Whoimplemented': 'Who implemented',\n",
        "                  'Representment': 'Rep resentment', 'StartFragment': 'Start Fragment',\n",
        "                  'EndFragment': 'End Fragment', ' documentarie ': ' documentaries ', 'requriments': 'requirements',\n",
        "                  'constitutionaldevelopment': 'constitutional development', 'parlamentarians': 'parliamentarians',\n",
        "                  'Rumenova': 'Rumen ova', 'argruments': 'arguments', 'findamental': 'fundamental',\n",
        "                  'totalinvestment': 'total investment', 'gevernment': 'government', 'recmommend': 'recommend',\n",
        "                  'appsmoment': 'apps moment', 'menstruual': 'menstrual', 'immplemented': 'implemented',\n",
        "                  'engangement': 'engagement', 'invovement': 'involvement', 'returement': 'retirement',\n",
        "                  'simentaneously': 'simultaneously', 'accompishments': 'accomplishments',\n",
        "                  'menstraution': 'menstruation', 'experimently': 'experiment', 'abdimen': 'abdomen',\n",
        "                  'cemenet': 'cement', 'propelment': 'propel ment', 'unamendable': 'un amendable',\n",
        "                  'employmentnews': 'employment news', 'lawforcement': 'law forcement',\n",
        "                  'menstuating': 'menstruating', 'fevelopment': 'development', 'reglamented': 'reg lamented',\n",
        "                  'imrovment': 'improvement', 'recommening': 'recommending', 'sppliment': 'supplement',\n",
        "                  'measument': 'measurement', 'reimbrusement': 'reimbursement', 'Nutrament': 'Nutriment',\n",
        "                  'puniahment': 'punishment', 'subligamentous': 'sub ligamentous', 'comlementry': 'complementary',\n",
        "                  'reteirement': 'retirement', 'envioronments': 'environments', 'haraasment': 'harassment',\n",
        "                  'USAgovernment': 'USA government', 'Apartmentfinder': 'Apartment finder',\n",
        "                  'encironment': 'environment', 'metacompartment': 'meta compartment',\n",
        "                  'augumentation': 'argumentation', 'dsymenorrhoea': 'dysmenorrhoea',\n",
        "                  'nonabandonment': 'non abandonment', 'annoincement': 'announcement',\n",
        "                  'menberships': 'memberships', 'Gamenights': 'Game nights', 'enliightenment': 'enlightenment',\n",
        "                  'supplymentry': 'supplementary', 'parlamentary': 'parliamentary', 'duramen': 'dura men',\n",
        "                  'hotelmanagement': 'hotel management', 'deartment': 'department',\n",
        "                  'treatmentshelp': 'treatments help', 'attirements': 'attire ments',\n",
        "                  'amendmending': 'amend mending', 'pseudomeningocele': 'pseudo meningocele',\n",
        "                  'intrasegmental': 'intra segmental', 'treatmenent': 'treatment', 'infridgement': 'infringement',\n",
        "                  'infringiment': 'infringement', 'recrecommend': 'rec recommend', 'entartaiment': 'entertainment',\n",
        "                  'inplementing': 'implementing', 'indemendent': 'independent', 'tremendeous': 'tremendous',\n",
        "                  'commencial': 'commercial', 'scomplishments': 'accomplishments', 'Emplement': 'Implement',\n",
        "                  'dimensiondimensions': 'dimension dimensions', 'depolyment': 'deployment',\n",
        "                  'conpartment': 'compartment', 'govnments': 'movements', 'menstrat': 'menstruate',\n",
        "                  'accompplishments': 'accomplishments', 'Enchacement': 'Enchancement',\n",
        "                  'developmenent': 'development', 'emmenagogues': 'emmenagogue', 'aggeement': 'agreement',\n",
        "                  'elementsbond': 'elements bond', 'remenant': 'remnant', 'Manamement': 'Management',\n",
        "                  'Augumented': 'Augmented', 'dimensonless': 'dimensionless',\n",
        "                  'ointmentsointments': 'ointments ointments', 'achiements': 'achievements',\n",
        "                  'recurtment': 'recurrent', 'gouverments': 'governments', 'docoment': 'document',\n",
        "                  'programmingassignments': 'programming assignments', 'menifest': 'manifest',\n",
        "                  'investmentguru': 'investment guru', 'deployements': 'deployments', 'Invetsment': 'Investment',\n",
        "                  'plaement': 'placement', 'Perliament': 'Parliament', 'femenists': 'feminists',\n",
        "                  'ecumencial': 'ecumenical', 'advamcements': 'advancements', 'refundment': 'refund ment',\n",
        "                  'settlementtake': 'settlement take', 'mensrooms': 'mens rooms',\n",
        "                  'productManagement': 'product Management', 'armenains': 'armenians',\n",
        "                  'betweenmanagement': 'between management', 'difigurement': 'disfigurement',\n",
        "                  'Armenized': 'Armenize', 'hurrasement': 'hurra sement', 'mamgement': 'management',\n",
        "                  'momuments': 'monuments', 'eauipments': 'equipments', 'managemenet': 'management',\n",
        "                  'treetment': 'treatment', 'webdevelopement': 'web developement', 'supplemenary': 'supplementary',\n",
        "                  'Encironmental': 'Environmental', 'Understandment': 'Understand ment',\n",
        "                  'enrollnment': 'enrollment', 'thinkstrategic': 'think strategic', 'thinkinh': 'thinking',\n",
        "                  'Softthinks': 'Soft thinks', 'underthinking': 'under thinking', 'thinksurvey': 'think survey',\n",
        "                  'whitelash': 'white lash', 'whiteheds': 'whiteheads', 'whitetning': 'whitening',\n",
        "                  'whitegirls': 'white girls', 'whitewalkers': 'white walkers', 'manycountries': 'many countries',\n",
        "                  'accomany': 'accompany', 'fromGermany': 'from Germany', 'manychat': 'many chat',\n",
        "                  'Germanyl': 'Germany l', 'manyness': 'many ness', 'many4': 'many', 'exmuslims': 'ex muslims',\n",
        "                  'digitizeindia': 'digitize india', 'indiarush': 'india rush', 'indiareads': 'india reads',\n",
        "                  'telegraphindia': 'telegraph india', 'Southindia': 'South india', 'Airindia': 'Air india',\n",
        "                  'siliconindia': 'silicon india', 'airindia': 'air india', 'indianleaders': 'indian leaders',\n",
        "                  'fundsindia': 'funds india', 'indianarmy': 'indian army', 'Technoindia': 'Techno india',\n",
        "                  'Betterindia': 'Better india', 'capesindia': 'capes india', 'Rigetti': 'Ligetti',\n",
        "                  'vegetablr': 'vegetable', 'get90': 'get', 'Magetta': 'Maretta', 'nagetive': 'native',\n",
        "                  'isUnforgettable': 'is Unforgettable', 'get630': 'get 630', 'GadgetPack': 'Gadget Pack',\n",
        "                  'Languagetool': 'Language tool', 'bugdget': 'budget', 'africaget': 'africa get',\n",
        "                  'ABnegetive': 'Abnegative', 'orangetheory': 'orange theory', 'getsmuggled': 'get smuggled',\n",
        "                  'avegeta': 'ave geta', 'gettubg': 'getting', 'gadgetsnow': 'gadgets now',\n",
        "                  'surgetank': 'surge tank', 'gadagets': 'gadgets', 'getallparts': 'get allparts',\n",
        "                  'messenget': 'messenger', 'vegetarean': 'vegetarian', 'get1000': 'get 1000',\n",
        "                  'getfinancing': 'get financing', 'getdrip': 'get drip', 'AdsTargets': 'Ads Targets',\n",
        "                  'tgethr': 'together', 'vegetaries': 'vegetables', 'forgetfulnes': 'forgetfulness',\n",
        "                  'fisgeting': 'fidgeting', 'BudgetAir': 'Budget Air',\n",
        "                  'getDepersonalization': 'get Depersonalization', 'negetively': 'negatively',\n",
        "                  'gettibg': 'getting', 'nauget': 'naught', 'Bugetti': 'Bugatti', 'plagetum': 'plage tum',\n",
        "                  'vegetabale': 'vegetable', 'changetip': 'change tip', 'blackwashing': 'black washing',\n",
        "                  'blackpink': 'black pink', 'blackmoney': 'black money',\n",
        "                  'blackmarks': 'black marks', 'blackbeauty': 'black beauty', 'unblacklisted': 'un blacklisted',\n",
        "                  'blackdotes': 'black dotes', 'blackboxing': 'black boxing', 'blackpaper': 'black paper',\n",
        "                  'blackpower': 'black power', 'Latinamericans': 'Latin americans', 'musigma': 'mu sigma',\n",
        "                  'Indominus': 'In dominus', 'usict': 'USSCt', 'indominus': 'in dominus', 'Musigma': 'Mu sigma',\n",
        "                  'plus5': 'plus', 'Russiagate': 'Russia gate', 'russophobic': 'Russophobiac',\n",
        "                  'Marcusean': 'Marcuse an', 'Radijus': 'Radius', 'cobustion': 'combustion',\n",
        "                  'Austrialians': 'Australians', 'mylogenous': 'myogenous', 'Raddus': 'Radius',\n",
        "                  'hetrogenous': 'heterogenous', 'greenhouseeffect': 'greenhouse effect', 'aquous': 'aqueous',\n",
        "                  'Taharrush': 'Tahar rush', 'Senousa': 'Venous', 'diplococcus': 'diplo coccus',\n",
        "                  'CityAirbus': 'City Airbus', 'sponteneously': 'spontaneously', 'trustless': 't rustless',\n",
        "                  'Pushkaram': 'Pushkara m', 'Fusanosuke': 'Fu sanosuke', 'isthmuses': 'isthmus es',\n",
        "                  'lucideus': 'lucidum', 'overjustification': 'over justification', 'Bindusar': 'Bind usar',\n",
        "                  'cousera': 'couler', 'musturbation': 'masturbation', 'infustry': 'industry',\n",
        "                  'Huswifery': 'Huswife ry', 'rombous': 'bombous', 'disengenuously': 'disingenuously',\n",
        "                  'sllybus': 'syllabus', 'celcious': 'delicious', 'cellsius': 'celsius',\n",
        "                  'lethocerus': 'Lethocerus', 'monogmous': 'monogamous', 'Ballyrumpus': 'Bally rumpus',\n",
        "                  'Koushika': 'Koushik a', 'vivipoarous': 'viviparous', 'ludiculous': 'ridiculous',\n",
        "                  'sychronous': 'synchronous', 'industiry': 'industry', 'scuduse': 'scud use',\n",
        "                  'babymust': 'baby must', 'simultqneously': 'simultaneously', 'exust': 'ex ust',\n",
        "                  'notmusing': 'not musing', 'Zamusu': 'Amuse', 'tusaki': 'tu saki', 'Marrakush': 'Marrakesh',\n",
        "                  'justcheaptickets': 'just cheaptickets', 'Ayahusca': 'Ayahausca', 'samousa': 'samosa',\n",
        "                  'Gusenberg': 'Gutenberg', 'illustratuons': 'illustrations', 'extemporeneous': 'extemporaneous',\n",
        "                  'Mathusla': 'Mathusala', 'Confundus': 'Con fundus', 'tusts': 'trusts', 'poisenious': 'poisonous',\n",
        "                  'Mevius': 'Medius', 'inuslating': 'insulating', 'aroused21000': 'aroused 21000',\n",
        "                  'Wenzeslaus': 'Wenceslaus', 'JustinKase': 'Justin Kase', 'purushottampur': 'purushottam pur',\n",
        "                  'citruspay': 'citrus pay', 'secutus': 'sects', 'austentic': 'austenitic',\n",
        "                  'FacePlusPlus': 'Face PlusPlus', 'aysnchronous': 'asynchronous',\n",
        "                  'teamtreehouse': 'team treehouse', 'uncouncious': 'unconscious', 'Priebuss': 'Prie buss',\n",
        "                  'consciousuness': 'consciousness', 'susubsoil': 'su subsoil', 'trimegistus': 'Trismegistus',\n",
        "                  'protopeterous': 'protopterous', 'trustworhty': 'trustworthy', 'ushually': 'usually',\n",
        "                  'industris': 'industries', 'instantneous': 'instantaneous', 'superplus': 'super plus',\n",
        "                  'shrusti': 'shruti', 'hindhus': 'hindus', 'outonomous': 'autonomous', 'reliegious': 'religious',\n",
        "                  'Kousakis': 'Kou sakis', 'reusult': 'result', 'JanusGraph': 'Janus Graph',\n",
        "                  'palusami': 'palus ami', 'mussraff': 'muss raff', 'hukous': 'humous',\n",
        "                  'photoacoustics': 'photo acoustics', 'kushanas': 'kusha nas', 'justdile': 'justice',\n",
        "                  'Massahusetts': 'Massachusetts', 'uspset': 'upset', 'sustinet': 'sustinent',\n",
        "                  'consicious': 'conscious', 'Sadhgurus': 'Sadh gurus', 'hystericus': 'hysteric us',\n",
        "                  'visahouse': 'visa house', 'supersynchronous': 'super synchronous', 'posinous': 'rosinous',\n",
        "                  'Fernbus': 'Fern bus', 'Tiltbrush': 'Tilt brush', 'glueteus': 'gluteus', 'posionus': 'poisons',\n",
        "                  'Freus': 'Frees', 'Zhuchengtyrannus': 'Zhucheng tyrannus', 'savonious': 'sanious',\n",
        "                  'CusJo': 'Cusco', 'congusion': 'confusion', 'dejavus': 'dejavu s', 'uncosious': 'uncopious',\n",
        "                  'previius': 'previous', 'counciousness': 'conciousness', 'lustorus': 'lustrous',\n",
        "                  'sllyabus': 'syllabus', 'mousquitoes': 'mosquitoes', 'Savvius': 'Savvies', 'arceius': 'Arcesius',\n",
        "                  'prejusticed': 'prejudiced', 'requsitioned': 'requisitioned',\n",
        "                  'deindustralization': 'deindustrialization', 'muscleblaze': 'muscle blaze',\n",
        "                  'ConsciousX5': 'conscious', 'nitrogenious': 'nitrogenous', 'mauritious': 'mauritius',\n",
        "                  'rigrously': 'rigorously', 'Yutyrannus': 'Yu tyrannus', 'muscualr': 'muscular',\n",
        "                  'conscoiusness': 'consciousness', 'Causians': 'Crusians', 'WorkFusion': 'Work Fusion',\n",
        "                  'puspak': 'pu spak', 'Inspirus': 'Inspires', 'illiustrations': 'illustrations',\n",
        "                  'Nobushi': 'No bushi', 'theuseof': 'thereof', 'suspicius': 'suspicious', 'Intuous': 'Virtuous',\n",
        "                  'gaushalas': 'gaus halas', 'campusthrough': 'campus through', 'seriousity': 'seriosity',\n",
        "                  'resustence': 'resistence', 'geminatus': 'geminates', 'disquss': 'discuss',\n",
        "                  'nicholus': 'nicholas', 'Husnai': 'Hussar', 'diiscuss': 'discuss', 'diffussion': 'diffusion',\n",
        "                  'phusicist': 'physicist', 'ernomous': 'enormous', 'Khushali': 'Khushal i', 'heitus': 'Leitus',\n",
        "                  'cracksbecause': 'cracks because', 'Nautlius': 'Nautilus', 'trausted': 'trusted',\n",
        "                  'Dardandus': 'Dardanus', 'Megatapirus': 'Mega tapirus', 'clusture': 'culture',\n",
        "                  'vairamuthus': 'vairamuthu s', 'disclousre': 'disclosure',\n",
        "                  'industrilaization': 'industrialization', 'musilms': 'muslims', 'Australia9': 'Australian',\n",
        "                  'causinng': 'causing', 'ibdustries': 'industries', 'searious': 'serious',\n",
        "                  'Coolmuster': 'Cool muster', 'sissyphus': 'sisyphus', ' justificatio ': 'justification',\n",
        "                  'antihindus': 'anti hindus', 'Moduslink': 'Modus link', 'zymogenous': 'zymogen ous',\n",
        "                  'prospeorus': 'prosperous', 'Retrocausality': 'Retro causality', 'FusionGPS': 'Fusion GPS',\n",
        "                  'Mouseflow': 'Mouse flow', 'bootyplus': 'booty plus', 'Itylus': 'I tylus',\n",
        "                  'Olnhausen': 'Olshausen', 'suspeect': 'suspect', 'entusiasta': 'enthusiast',\n",
        "                  'fecetious': 'facetious', 'bussiest': 'fussiest', 'Draconius': 'Draconis',\n",
        "                  'requsite': 'requisite', 'nauseatic': 'nausea tic', 'Brusssels': 'Brussels',\n",
        "                  'repurcussion': 'repercussion', 'Jeisus': 'Jesus', 'philanderous': 'philander ous',\n",
        "                  'muslisms': 'muslims', 'august2017': 'august 2017', 'calccalculus': 'calc calculus',\n",
        "                  'unanonymously': 'un anonymously', 'Imaprtus': 'Impetus', 'carnivorus': 'carnivorous',\n",
        "                  'Corypheus': 'Coryphees', 'austronauts': 'astronauts', 'neucleus': 'nucleus',\n",
        "                  'housepoor': 'house poor', 'rescouses': 'responses', 'Tagushi': 'Tagus hi',\n",
        "                  'hyperfocusing': 'hyper focusing', 'nutriteous': 'nutritious', 'chylus': 'chylous',\n",
        "                  'preussure': 'pressure', 'outfocus': 'out focus', 'Hanfus': 'Hannus', 'Rustyrose': 'Rusty rose',\n",
        "                  'vibhushant': 'vibhushan t', 'conciousnes': 'conciousness', 'Venus25': 'Venus',\n",
        "                  'Sedataious': 'Seditious', 'promuslim': 'pro muslim', 'statusGuru': 'status Guru',\n",
        "                  'yousician': 'musician', 'transgenus': 'trans genus', 'Pushbullet': 'Push bullet',\n",
        "                  'jeesyllabus': 'jee syllabus', 'complusary': 'compulsory', 'Holocoust': 'Holocaust',\n",
        "                  'careerplus': 'career plus', 'Lllustrate': 'Illustrate', 'Musino': 'Musion',\n",
        "                  'Phinneus': 'Phineus', 'usedtoo': 'used too', 'JustBasic': 'Just Basic', 'webmusic': 'web music',\n",
        "                  'TrustKit': 'Trust Kit', 'industrZgies': 'industries', 'rubustness': 'robustness',\n",
        "                  'Missuses': 'Miss uses', 'Musturbation': 'Masturbation', 'bustees': 'bus tees',\n",
        "                  'justyfy': 'justify', 'pegusus': 'pegasus', 'industrybuying': 'industry buying',\n",
        "                  'advantegeous': 'advantageous', 'kotatsus': 'kotatsu s', 'justcreated': 'just created',\n",
        "                  'simultameously': 'simultaneously', 'husoone': 'huso one', 'twiceusing': 'twice using',\n",
        "                  'cetusplay': 'cetus play', 'sqamous': 'squamous', 'claustophobic': 'claustrophobic',\n",
        "                  'Kaushika': 'Kaushik a', 'dioestrus': 'di oestrus', 'Degenerous': 'De generous',\n",
        "                  'neculeus': 'nucleus', 'cutaneously': 'cu taneously', 'Alamotyrannus': 'Alamo tyrannus',\n",
        "                  'Ivanious': 'Avanious', 'arceous': 'araceous', 'Flixbus': 'Flix bus', 'caausing': 'causing',\n",
        "                  'publious': 'Publius', 'Juilus': 'Julius', 'Australianism': 'Australian ism',\n",
        "                  'vetronus': 'verrons', 'nonspontaneous': 'non spontaneous', 'calcalus': 'calculus',\n",
        "                  'commudus': 'Commodus', 'Rheusus': 'Rhesus', 'syallubus': 'syllabus', 'Yousician': 'Musician',\n",
        "                  'qurush': 'qu rush', 'athiust': 'athirst', 'conclusionless': 'conclusion less',\n",
        "                  'usertesting': 'user testing', 'redius': 'radius', 'Austrolia': 'Australia',\n",
        "                  'sllaybus': 'syllabus', 'toponymous': 'top onymous', 'businiss': 'business',\n",
        "                  'hyperthalamus': 'hyper thalamus', 'clause55': 'clause', 'cosicous': 'conscious',\n",
        "                  'Sushena': 'Saphena', 'Luscinus': 'Luscious', 'Prussophile': 'Russophile', 'jeaslous': 'jealous',\n",
        "                  'Austrelia': 'Australia', 'contiguious': 'contiguous',\n",
        "                  'subconsciousnesses': 'sub consciousnesses', ' jusification ': 'justification',\n",
        "                  'dilusion': 'delusion', 'anticoncussive': 'anti concussive', 'disngush': 'disgust',\n",
        "                  'constiously': 'consciously', 'filabustering': 'filibustering', 'GAPbuster': 'GAP buster',\n",
        "                  'insectivourous': 'insectivorous', 'glocuse': 'louse', 'Antritrust': 'Antitrust',\n",
        "                  'thisAustralian': 'this Australian', 'FusionDrive': 'Fusion Drive', 'nuclus': 'nucleus',\n",
        "                  'abussive': 'abusive', 'mustang1': 'mustangs', 'inradius': 'in radius', 'polonious': 'polonius',\n",
        "                  'ofKulbhushan': 'of Kulbhushan', 'homosporous': 'homos porous', 'circumradius': 'circum radius',\n",
        "                  'atlous': 'atrous', 'insustry': 'industry', 'campuswith': 'campus with', 'beacsuse': 'because',\n",
        "                  'concuous': 'conscious', 'nonHindus': 'non Hindus', 'carnivourous': 'carnivorous',\n",
        "                  'tradeplus': 'trade plus', 'Jeruselam': 'Jerusalem',\n",
        "                  'musuclar': 'muscular', 'deangerous': 'dangerous', 'disscused': 'discussed',\n",
        "                  'industdial': 'industrial', 'sallatious': 'fallacious', 'rohmbus': 'rhombus',\n",
        "                  'golusu': 'gol usu', 'Minangkabaus': 'Minangkabau s', 'Mustansiriyah': 'Mustansiriya h',\n",
        "                  'anomymously': 'anonymously', 'abonymously': 'anonymously', 'indrustry': 'industry',\n",
        "                  'Musharrf': 'Musharraf', 'workouses': 'workhouses', 'sponataneously': 'spontaneously',\n",
        "                  'anmuslim': 'an muslim', 'syallbus': 'syllabus', 'presumptuousnes': 'presumptuousness',\n",
        "                  'Thaedus': 'Thaddus', 'industey': 'industry', 'hkust': 'hust', 'Kousseri': 'Kousser i',\n",
        "                  'mousestats': 'mouses tats', 'russiagate': 'russia gate', 'simantaneously': 'simultaneously',\n",
        "                  'Austertana': 'Auster tana', 'infussions': 'infusions', 'coclusion': 'conclusion',\n",
        "                  'sustainabke': 'sustainable', 'tusami': 'tu sami', 'anonimously': 'anonymously',\n",
        "                  'usebase': 'use base', 'balanoglossus': 'Balanoglossus', 'Unglaus': 'Ung laus',\n",
        "                  'ignoramouses': 'ignoramuses', 'snuus': 'snugs', 'reusibility': 'reusability',\n",
        "                  'Straussianism': 'Straussian ism', 'simoultaneously': 'simultaneously',\n",
        "                  'realbonus': 'real bonus', 'nuchakus': 'nunchakus', 'annonimous': 'anonymous',\n",
        "                  'Incestious': 'Incestuous', 'Manuscriptology': 'Manuscript ology', 'difusse': 'diffuse',\n",
        "                  'Pliosaurus': 'Pliosaur us', 'cushelle': 'cush elle', 'Catallus': 'Catullus',\n",
        "                  'MuscleBlaze': 'Muscle Blaze', 'confousing': 'confusing', 'enthusiasmless': 'enthusiasm less',\n",
        "                  'Tetherusd': 'Tethered', 'Josephius': 'Josephus', 'jusrlt': 'just',\n",
        "                  'simutaneusly': 'simultaneously', 'mountaneous': 'mountainous', 'Badonicus': 'Sardonicus',\n",
        "                  'muccus': 'mucous', 'nicus': 'nidus', 'austinlizards': 'austin lizards',\n",
        "                  'errounously': 'erroneously', 'Australua': 'Australia', 'sylaabus': 'syllabus',\n",
        "                  'dusyant': 'distant', 'javadiscussion': 'java discussion', 'megabuses': 'mega buses',\n",
        "                  'danergous': 'dangerous', 'contestious': 'contentious', 'exause': 'excuse',\n",
        "                  'muscluar': 'muscular', 'avacous': 'vacuous', 'Ingenhousz': 'Ingenious',\n",
        "                  'holocausting': 'holocaust ing', 'Pakustan': 'Pakistan', 'purusharthas': 'purushartha',\n",
        "                  'bapus': 'bapu s', 'useul': 'useful', 'pretenious': 'pretentious', 'homogeneus': 'homogeneous',\n",
        "                  'bhlushes': 'blushes', 'Saggittarius': 'Sagittarius', 'sportsusa': 'sports usa',\n",
        "                  'kerataconus': 'keratoconus', 'infrctuous': 'infectuous', 'Anonoymous': 'Anonymous',\n",
        "                  'triphosphorus': 'tri phosphorus', 'ridicjlously': 'ridiculously',\n",
        "                  'worldbusiness': 'world business', 'hollcaust': 'holocaust', 'Dusra': 'Dura',\n",
        "                  'meritious': 'meritorious', 'Sauskes': 'Causes', 'inudustry': 'industry',\n",
        "                  'frustratd': 'frustrate', 'hypotenous': 'hypogenous', 'Dushasana': 'Dush asana',\n",
        "                  'saadus': 'status', 'keratokonus': 'keratoconus', 'Jarrus': 'Harrus', 'neuseous': 'nauseous',\n",
        "                  'simutanously': 'simultaneously', 'diphosphorus': 'di phosphorus', 'sulprus': 'surplus',\n",
        "                  'Hasidus': 'Hasid us', 'suspenive': 'suspensive', 'illlustrator': 'illustrator',\n",
        "                  'userflows': 'user flows', 'intrusivethoughts': 'intrusive thoughts', 'countinous': 'continuous',\n",
        "                  'gpusli': 'gusli', 'Calculus1': 'Calculus', 'bushiri': 'Bushire',\n",
        "                  'torvosaurus': 'Torosaurus', 'chestbusters': 'chest busters', 'Satannus': 'Sat annus',\n",
        "                  'falaxious': 'fallacious', 'obnxious': 'obnoxious', 'tranfusions': 'transfusions',\n",
        "                  'PlayMagnus': 'Play Magnus', 'Epicodus': 'Episodes', 'Hypercubus': 'Hypercubes',\n",
        "                  'Musickers': 'Musick ers', 'programmebecause': 'programme because', 'indiginious': 'indigenous',\n",
        "                  'housban': 'Housman', 'iusso': 'kusso', 'annilingus': 'anilingus', 'Nennus': 'Genius',\n",
        "                  'pussboy': 'puss boy', 'Photoacoustics': 'Photo acoustics', 'Hindusthanis': 'Hindustanis',\n",
        "                  'lndustrial': 'industrial', 'tyrannously': 'tyrannous', 'Susanoomon': 'Susanoo mon',\n",
        "                  'colmbus': 'columbus', 'sussessful': 'successful', 'ousmania': 'ous mania',\n",
        "                  'ilustrating': 'illustrating', 'famousbirthdays': 'famous birthdays',\n",
        "                  'suspectance': 'suspect ance', 'extroneous': 'extraneous', 'teethbrush': 'teeth brush',\n",
        "                  'abcmouse': 'abc mouse', 'degenerous': 'de generous', 'doesGauss': 'does Gauss',\n",
        "                  'insipudus': 'insipidus', 'movielush': 'movie lush', 'Rustichello': 'Rustic hello',\n",
        "                  'Firdausiya': 'Firdausi ya', 'checkusers': 'check users', 'householdware': 'household ware',\n",
        "                  'prosporously': 'prosperously', 'SteLouse': 'Ste Louse', 'obfuscaton': 'obfuscation',\n",
        "                  'amorphus': 'amorph us', 'trustworhy': 'trustworthy', 'celsious': 'cesious',\n",
        "                  'dangorous': 'dangerous', 'anticancerous': 'anti cancerous', 'cousi ': 'cousin ',\n",
        "                  'austroloid': 'australoid', 'fergussion': 'percussion', 'andKyokushin': 'and Kyokushin',\n",
        "                  'cousan': 'cousin', 'Huskystar': 'Hu skystar', 'retrovisus': 'retrovirus', 'becausr': 'because',\n",
        "                  'Jerusalsem': 'Jerusalem', 'motorious': 'notorious', 'industrilised': 'industrialised',\n",
        "                  'powerballsusa': 'powerballs usa', 'monoceious': 'monoecious', 'batteriesplus': 'batteries plus',\n",
        "                  'nonviscuous': 'nonviscous', 'industion': 'induction', 'bussinss': 'bussings',\n",
        "                  'userbags': 'user bags', 'Jlius': 'Julius', 'thausand': 'thousand', 'plustwo': 'plus two',\n",
        "                  'defpush': 'def push', 'subconcussive': 'sub concussive', 'muslium': 'muslim',\n",
        "                  'industrilization': 'industrialization', 'Maurititus': 'Mauritius', 'uslme': 'some',\n",
        "                  'Susgaon': 'Surgeon', 'Pantherous': 'Panther ous', 'antivirius': 'antivirus',\n",
        "                  'Trustclix': 'Trust clix', 'silumtaneously': 'simultaneously', 'Icompus': 'Corpus',\n",
        "                  'atonomous': 'autonomous', 'Reveuse': 'Reve use', 'legumnous': 'leguminous',\n",
        "                  'syllaybus': 'syllabus', 'louspeaker': 'loudspeaker', 'susbtraction': 'substraction',\n",
        "                  'virituous': 'virtuous', 'disastrius': 'disastrous', 'jerussalem': 'jerusalem',\n",
        "                  'Industrailzed': 'Industrialized', 'recusion': 'recushion',\n",
        "                  'simultenously': 'simultaneously',\n",
        "                  'Pulphus': 'Pulpous', 'harbaceous': 'herbaceous', 'phlegmonous': 'phlegmon ous', 'use38': 'use',\n",
        "                  'jusify': 'justify', 'instatanously': 'instantaneously', 'tetramerous': 'tetramer ous',\n",
        "                  'usedvin': 'used vin', 'sagittarious': 'sagittarius', 'mausturbate': 'masturbate',\n",
        "                  'subcautaneous': 'subcutaneous', 'dangergrous': 'dangerous', 'sylabbus': 'syllabus',\n",
        "                  'hetorozygous': 'heterozygous', 'Ignasius': 'Ignacius', 'businessbor': 'business bor',\n",
        "                  'Bhushi': 'Thushi', 'Moussolini': 'Mussolini', 'usucaption': 'usu caption',\n",
        "                  'Customzation': 'Customization', 'cretinously': 'cretinous', 'genuiuses': 'geniuses',\n",
        "                  'Moushmee': 'Mousmee', 'neigous': 'nervous',\n",
        "                  'infrustructre': 'infrastructure', 'Ilusha': 'Ilesha', 'suconciously': 'unconciously',\n",
        "                  'stusy': 'study', 'mustectomy': 'mastectomy', 'Farmhousebistro': 'Farmhouse bistro',\n",
        "                  'instantanous': 'instantaneous', 'JustForex': 'Just Forex', 'Indusyry': 'Industry',\n",
        "                  'mustabating': 'must abating', 'uninstrusive': 'unintrusive', 'customshoes': 'customs hoes',\n",
        "                  'homageneous': 'homogeneous', 'Empericus': 'Imperious', 'demisexuality': 'demi sexuality',\n",
        "                  'transexualism': 'transsexualism', 'sexualises': 'sexualise', 'demisexuals': 'demisexual',\n",
        "                  'sexuly': 'sexily', 'Pornosexuality': 'Porno sexuality', 'sexond': 'second', 'sexxual': 'sexual',\n",
        "                  'asexaul': 'asexual', 'sextactic': 'sex tactic', 'sexualityism': 'sexuality ism',\n",
        "                  'monosexuality': 'mono sexuality', 'intwrsex': 'intersex', 'hypersexualize': 'hyper sexualize',\n",
        "                  'homosexualtiy': 'homosexuality', 'examsexams': 'exams exams', 'sexmates': 'sex mates',\n",
        "                  'sexyjobs': 'sexy jobs', 'sexitest': 'sexiest', 'fraysexual': 'fray sexual',\n",
        "                  'sexsurrogates': 'sex surrogates', 'sexuallly': 'sexually', 'gamersexual': 'gamer sexual',\n",
        "                  'greysexual': 'grey sexual', 'omnisexuality': 'omni sexuality', 'hetereosexual': 'heterosexual',\n",
        "                  'productsexamples': 'products examples', 'sexgods': 'sex gods', 'semisexual': 'semi sexual',\n",
        "                  'homosexulity': 'homosexuality', 'sexeverytime': 'sex everytime', 'neurosexist': 'neuro sexist',\n",
        "                  'worldquant': 'world quant', 'Freshersworld': 'Freshers world', 'smartworld': 'sm artworld',\n",
        "                  'Mistworlds': 'Mist worlds', 'boothworld': 'booth world', 'ecoworld': 'eco world',\n",
        "                  'Ecoworld': 'Eco world', 'underworldly': 'under worldly', 'worldrank': 'world rank',\n",
        "                  'Clearworld': 'Clear world', 'Boothworld': 'Booth world', 'Rimworld': 'Rim world',\n",
        "                  'cryptoworld': 'crypto world', 'machineworld': 'machine world', 'worldwideley': 'worldwide ley',\n",
        "                  'capuletwant': 'capulet want', 'Bhagwanti': 'Bhagwant i', 'Unwanted72': 'Unwanted 72',\n",
        "                  'wantrank': 'want rank',\n",
        "                  'willhappen': 'will happen', 'thateasily': 'that easily',\n",
        "                  'Whatevidence': 'What evidence', 'metaphosphates': 'meta phosphates',\n",
        "                  'exilarchate': 'exilarch ate', 'aulphate': 'sulphate', 'Whateducation': 'What education',\n",
        "                  'persulphates': 'per sulphates', 'disulphate': 'di sulphate', 'picosulphate': 'pico sulphate',\n",
        "                  'tetraosulphate': 'tetrao sulphate', 'prechinese': 'pre chinese',\n",
        "                  'Hellochinese': 'Hello chinese', 'muchdeveloped': 'much developed', 'stomuch': 'stomach',\n",
        "                  'Whatmakes': 'What makes', 'Lensmaker': 'Lens maker', 'eyemake': 'eye make',\n",
        "                  'Techmakers': 'Tech makers', 'cakemaker': 'cake maker', 'makeup411': 'makeup 411',\n",
        "                  'objectmake': 'object make', 'crazymaker': 'crazy maker', 'techmakers': 'tech makers',\n",
        "                  'makedonian': 'macedonian', 'makeschool': 'make school', 'anxietymake': 'anxiety make',\n",
        "                  'makeshifter': 'make shifter', 'countryball': 'country ball', 'Whichcountry': 'Which country',\n",
        "                  'countryHow': 'country How', 'Zenfone': 'Zen fone', 'Electroneum': 'Electro neum',\n",
        "                  'electroneum': 'electro neum', 'Demonetisation': 'demonetization', 'zenfone': 'zen fone',\n",
        "                  'ZenFone': 'Zen Fone', 'onecoin': 'one coin', 'demonetizing': 'demonetized',\n",
        "                  'iphone7': 'iPhone', 'iPhone6': 'iPhone', 'microneedling': 'micro needling', 'iphone6': 'iPhone',\n",
        "                  'Monegasques': 'Monegasque s', 'demonetised': 'demonetized',\n",
        "                  'EveryoneDiesTM': 'EveryoneDies TM', 'teststerone': 'testosterone', 'DoneDone': 'Done Done',\n",
        "                  'papermoney': 'paper money', 'Sasabone': 'Sasa bone', 'Blackphone': 'Black phone',\n",
        "                  'Bonechiller': 'Bone chiller', 'Moneyfront': 'Money front', 'workdone': 'work done',\n",
        "                  'iphoneX': 'iPhone', 'roxycodone': 'r oxycodone',\n",
        "                  'moneycard': 'money card', 'Fantocone': 'Fantocine', 'eletronegativity': 'electronegativity',\n",
        "                  'mellophones': 'mellophone s', 'isotones': 'iso tones', 'donesnt': 'doesnt',\n",
        "                  'thereanyone': 'there anyone', 'electronegativty': 'electronegativity',\n",
        "                  'commissiioned': 'commissioned', 'earvphone': 'earphone', 'condtioners': 'conditioners',\n",
        "                  'demonetistaion': 'demonetization', 'ballonets': 'ballo nets', 'DoneClaim': 'Done Claim',\n",
        "                  'alimoney': 'alimony', 'iodopovidone': 'iodo povidone', 'bonesetters': 'bone setters',\n",
        "                  'componendo': 'compon endo', 'probationees': 'probationers', 'one300': 'one 300',\n",
        "                  'nonelectrolyte': 'non electrolyte', 'ozonedepletion': 'ozone depletion',\n",
        "                  'Stonehart': 'Stone hart', 'Vodafone2': 'Vodafones', 'chaparone': 'chaperone',\n",
        "                  'Noonein': 'Noo nein', 'Frosione': 'Erosion', 'IPhone7': 'Iphone', 'pentanone': 'penta none',\n",
        "                  'poneglyphs': 'pone glyphs', 'cyclohexenone': 'cyclohexanone', 'marlstone': 'marls tone',\n",
        "                  'androneda': 'andromeda', 'iphone8': 'iPhone', 'acidtone': 'acid tone',\n",
        "                  'noneconomically': 'non economically', 'Honeyfund': 'Honey fund', 'germanophone': 'Germanophobe',\n",
        "                  'Democratizationed': 'Democratization ed', 'haoneymoon': 'honeymoon', 'iPhone7': 'iPhone 7',\n",
        "                  'someonewith': 'some onewith', 'Hexanone': 'Hexa none', 'bonespur': 'bones pur',\n",
        "                  'sisterzoned': 'sister zoned', 'HasAnyone': 'Has Anyone',\n",
        "                  'stonepelters': 'stone pelters', 'Chronexia': 'Chronaxia', 'brotherzone': 'brother zone',\n",
        "                  'brotherzoned': 'brother zoned', 'fonecare': 'f onecare', 'nonexsistence': 'nonexistence',\n",
        "                  'conents': 'contents', 'phonecases': 'phone cases', 'Commissionerates': 'Commissioner ates',\n",
        "                  'activemoney': 'active money', 'dingtone': 'ding tone', 'wheatestone': 'wheatstone',\n",
        "                  'chiropractorone': 'chiropractor one', 'heeadphones': 'headphones', 'Maimonedes': 'Maimonides',\n",
        "                  'onepiecedeals': 'onepiece deals', 'oneblade': 'one blade', 'venetioned': 'Venetianed',\n",
        "                  'sunnyleone': 'sunny leone', 'prendisone': 'prednisone', 'Anglosaxophone': 'Anglo saxophone',\n",
        "                  'Blackphones': 'Black phones', 'jionee': 'jinnee', 'chromonema': 'chromo nema',\n",
        "                  'iodoketones': 'iodo ketones', 'demonetizations': 'demonetization', 'aomeone': 'someone',\n",
        "                  'trillonere': 'trillones', 'abandonee': 'abandon',\n",
        "                  'MasterColonel': 'Master Colonel', 'fronend': 'friend', 'Wildstone': 'Wilds tone',\n",
        "                  'patitioned': 'petitioned', 'lonewolfs': 'lone wolfs', 'Spectrastone': 'Spectra stone',\n",
        "                  'dishonerable': 'dishonorable', 'poisiones': 'poisons',\n",
        "                  'condioner': 'conditioner', 'unpermissioned': 'unper missioned', 'friedzone': 'fried zone',\n",
        "                  'umumoney': 'umu money', 'anyonestudied': 'anyone studied', 'dictioneries': 'dictionaries',\n",
        "                  'nosebone': 'nose bone', 'ofVodafone': 'of Vodafone',\n",
        "                  'Yumstone': 'Yum stone', 'oxandrolonesteroid': 'oxandrolone steroid',\n",
        "                  'Mifeprostone': 'Mifepristone', 'pheramones': 'pheromones',\n",
        "                  'sinophone': 'Sinophobe', 'peloponesian': 'peloponnesian', 'michrophone': 'microphone',\n",
        "                  'commissionets': 'commissioners', 'methedone': 'methadone', 'cobditioners': 'conditioners',\n",
        "                  'urotone': 'protone', 'smarthpone': 'smartphone', 'conecTU': 'connect you', 'beloney': 'boloney',\n",
        "                  'comfortzone': 'comfort zone', 'testostersone': 'testosterone', 'camponente': 'component',\n",
        "                  'Idonesia': 'Indonesia', 'dolostones': 'dolostone', 'psiphone': 'psi phone',\n",
        "                  'ceftriazone': 'ceftriaxone', 'feelonely': 'feel onely', 'monetation': 'moderation',\n",
        "                  'activationenergy': 'activation energy', 'moneydriven': 'money driven',\n",
        "                  'staionery': 'stationery', 'zoneflex': 'zone flex', 'moneycash': 'money cash',\n",
        "                  'conectiin': 'connection', 'Wannaone': 'Wanna one',\n",
        "                  'Pictones': 'Pict ones', 'demonentization': 'demonetization',\n",
        "                  'phenonenon': 'phenomenon', 'evenafter': 'even after', 'Sevenfriday': 'Seven friday',\n",
        "                  'Devendale': 'Evendale', 'theeventchronicle': 'the event chronicle',\n",
        "                  'seventysomething': 'seventy something', 'sevenpointed': 'seven pointed',\n",
        "                  'richfeel': 'rich feel', 'overfeel': 'over feel', 'feelingstupid': 'feeling stupid',\n",
        "                  'Photofeeler': 'Photo feeler', 'feelomgs': 'feelings', 'feelinfs': 'feelings',\n",
        "                  'PlayerUnknown': 'Player Unknown', 'Playerunknown': 'Player unknown', 'knowlefge': 'knowledge',\n",
        "                  'knowledgd': 'knowledge', 'knowledeg': 'knowledge', 'knowble': 'Knowle', 'Howknow': 'Howk now',\n",
        "                  'knowledgeWoods': 'knowledge Woods', 'knownprogramming': 'known programming',\n",
        "                  'selfknowledge': 'self knowledge', 'knowldage': 'knowledge', 'knowyouve': 'know youve',\n",
        "                  'aknowlege': 'knowledge', 'Audetteknown': 'Audette known', 'knowlegdeable': 'knowledgeable',\n",
        "                  'trueoutside': 'true outside', 'saynthesize': 'synthesize', 'EssayTyper': 'Essay Typer',\n",
        "                  'meesaya': 'mee saya', 'Rasayanam': 'Rasayan am', 'fanessay': 'fan essay', 'momsays': 'moms ays',\n",
        "                  'sayying': 'saying', 'saydaw': 'say daw', 'Fanessay': 'Fan essay', 'theyreally': 'they really',\n",
        "                  'gayifying': 'gayed up with homosexual love', 'gayke': 'gay Online retailers',\n",
        "                  'Lingayatism': 'Lingayat',\n",
        "                  'macapugay': 'Macaulay', 'jewsplain': 'jews plain',\n",
        "                  'banggood': 'bang good', 'goodfriends': 'good friends',\n",
        "                  'goodfirms': 'good firms', 'Banggood': 'Bang good', 'dogooder': 'do gooder',\n",
        "                  'stillshots': 'stills hots', 'stillsuits': 'still suits', 'panromantic': 'pan romantic',\n",
        "                  'paracommando': 'para commando', 'romantize': 'romanize', 'manupulative': 'manipulative',\n",
        "                  'manjha': 'mania', 'mankrit': 'mank rit',\n",
        "                  'heteroromantic': 'hetero romantic', 'pulmanery': 'pulmonary', 'manpads': 'man pads',\n",
        "                  'supermaneuverable': 'super maneuverable', 'mandatkry': 'mandatory', 'armanents': 'armaments',\n",
        "                  'manipative': 'mancipative', 'himanity': 'humanity', 'maneuever': 'maneuver',\n",
        "                  'Kumarmangalam': 'Kumar mangalam', 'Brahmanwadi': 'Brahman wadi',\n",
        "                  'exserviceman': 'ex serviceman',\n",
        "                  'managewp': 'managed', 'manies': 'many', 'recordermans': 'recorder mans',\n",
        "                  'Feymann': 'Heymann', 'salemmango': 'salem mango', 'manufraturing': 'manufacturing',\n",
        "                  'sreeman': 'freeman', 'tamanaa': 'Tamanac', 'chlamydomanas': 'chlamydomonas',\n",
        "                  'comandant': 'commandant', 'huemanity': 'humanity', 'manaagerial': 'managerial',\n",
        "                  'lithromantics': 'lith romantics',\n",
        "                  'geramans': 'germans', 'Nagamandala': 'Naga mandala', 'humanitariarism': 'humanitarianism',\n",
        "                  'wattman': 'watt man', 'salesmanago': 'salesman ago', 'Washwoman': 'Wash woman',\n",
        "                  'rammandir': 'ram mandir', 'nomanclature': 'nomenclature', 'Haufman': 'Kaufman',\n",
        "                  'prefomance': 'performance', 'ramanunjan': 'Ramanujan', 'Freemansonry': 'Freemasonry',\n",
        "                  'supermaneuverability': 'super maneuverability', 'manstruate': 'menstruate',\n",
        "                  'Tarumanagara': 'Taruma nagara', 'RomanceTale': 'Romance Tale', 'heteromantic': 'hete romantic',\n",
        "                  'terimanals': 'terminals', 'womansplaining': 'wo mansplaining',\n",
        "                  'performancelearning': 'performance learning', 'sociomantic': 'sciomantic',\n",
        "                  'batmanvoice': 'batman voice', 'PerformanceTesting': 'Performance Testing',\n",
        "                  'manorialism': 'manorial ism', 'newscommando': 'news commando',\n",
        "                  'Entwicklungsroman': 'Entwicklungs roman',\n",
        "                  'Kunstlerroman': 'Kunstler roman', 'bodhidharman': 'Bodhidharma', 'Howmaney': 'How maney',\n",
        "                  'manufucturing': 'manufacturing', 'remmaning': 'remaining', 'rangeman': 'range man',\n",
        "                  'mythomaniac': 'mythomania', 'katgmandu': 'katmandu',\n",
        "                  'Superowoman': 'Superwoman', 'Rahmanland': 'Rahman land', 'Dormmanu': 'Dormant',\n",
        "                  'Geftman': 'Gentman', 'manufacturig': 'manufacturing', 'bramanistic': 'Brahmanistic',\n",
        "                  'padmanabhanagar': 'padmanabhan agar', 'homoromantic': 'homo romantic', 'femanists': 'feminists',\n",
        "                  'demihuman': 'demi human', 'manrega': 'Manresa', 'Pasmanda': 'Pas manda',\n",
        "                  'manufacctured': 'manufactured', 'remaninder': 'remainder', 'Marimanga': 'Mari manga',\n",
        "                  'Sloatman': 'Sloat man', 'manlet': 'man let', 'perfoemance': 'performance',\n",
        "                  'mangolian': 'mongolian', 'mangekyu': 'mange kyu', 'mansatory': 'mandatory',\n",
        "                  'managemebt': 'management', 'manufctures': 'manufactures', 'Bramanical': 'Brahmanical',\n",
        "                  'manaufacturing': 'manufacturing', 'Lakhsman': 'Lakhs man', 'Sarumans': 'Sarum ans',\n",
        "                  'mangalasutra': 'mangalsutra', 'Germanised': 'German ised',\n",
        "                  'managersworking': 'managers working', 'cammando': 'commando', 'mandrillaris': 'mandrill aris',\n",
        "                  'Emmanvel': 'Emmarvel', 'manupalation': 'manipulation', 'welcomeromanian': 'welcome romanian',\n",
        "                  'humanfemale': 'human female', 'mankirt': 'mankind', 'Haffmann': 'Hoffmann',\n",
        "                  'Panromantic': 'Pan romantic', 'demantion': 'detention', 'Suparwoman': 'Superwoman',\n",
        "                  'parasuramans': 'parasuram ans', 'sulmann': 'Suilmann', 'Shubman': 'Subman',\n",
        "                  'manspread': 'man spread', 'mandingan': 'Mandingan', 'mandalikalu': 'mandalika lu',\n",
        "                  'manufraturer': 'manufacturer', 'Wedgieman': 'Wedgie man', 'manwues': 'manages',\n",
        "                  'humanzees': 'human zees', 'Steymann': 'Stedmann', 'Jobberman': 'Jobber man',\n",
        "                  'maniquins': 'mani quins', 'biromantical': 'bi romantical', 'Rovman': 'Roman',\n",
        "                  'pyromantic': 'pyro mantic', 'Tastaman': 'Rastaman', 'Spoolman': 'Spool man',\n",
        "                  'Subramaniyan': 'Subramani yan', 'abhimana': 'abhiman a', 'manholding': 'man holding',\n",
        "                  'seviceman': 'serviceman', 'womansplained': 'womans plained', 'manniya': 'mania',\n",
        "                  'Bhraman': 'Braman', 'Laakman': 'Layman', 'mansturbate': 'masturbate',\n",
        "                  'Sulamaniya': 'Sulamani ya', 'demanters': 'decanters', 'postmanare': 'postman are',\n",
        "                  'mannualy': 'annual', 'rstman': 'Rotman', 'permanentjobs': 'permanent jobs',\n",
        "                  'Allmang': 'All mang', 'TradeCommander': 'Trade Commander', 'BasedStickman': 'Based Stickman',\n",
        "                  'Deshabhimani': 'Desha bhimani', 'manslamming': 'mans lamming', 'Brahmanwad': 'Brahman wad',\n",
        "                  'fundemantally': 'fundamentally', 'supplemantary': 'supplementary', 'egomanias': 'ego manias',\n",
        "                  'manvantar': 'Manvantara', 'spymania': 'spy mania', 'mangonada': 'mango nada',\n",
        "                  'manthras': 'mantras', 'Humanpark': 'Human park', 'manhuas': 'mahuas',\n",
        "                  'manterrupting': 'interrupting', 'dermatillomaniac': 'dermatillomania',\n",
        "                  'performancies': 'performances', 'manipulant': 'manipulate',\n",
        "                  'painterman': 'painter man', 'mangalik': 'manglik',\n",
        "                  'Neurosemantics': 'Neuro semantics', 'discrimantion': 'discrimination',\n",
        "                  'Womansplaining': 'feminist', 'mongodump': 'mongo dump', 'roadgods': 'road gods',\n",
        "                  'Oligodendraglioma': 'Oligodendroglioma', 'unrightly': 'un rightly', 'Janewright': 'Jane wright',\n",
        "                  ' righten ': ' tighten ', 'brightiest': 'brightest',\n",
        "                  'frighter': 'fighter', 'righteouness': 'righteousness', 'triangleright': 'triangle right',\n",
        "                  'Brightspace': 'Brights pace', 'techinacal': 'technical', 'chinawares': 'china wares',\n",
        "                  'Vancouever': 'Vancouver', 'cheverlet': 'cheveret', 'deverstion': 'diversion',\n",
        "                  'everbodys': 'everybody', 'Dramafever': 'Drama fever', 'reverificaton': 'reverification',\n",
        "                  'canterlever': 'canter lever', 'keywordseverywhere': 'keywords everywhere',\n",
        "                  'neverunlearned': 'never unlearned', 'everyfirst': 'every first',\n",
        "                  'neverhteless': 'nevertheless', 'clevercoyote': 'clever coyote', 'irrevershible': 'irreversible',\n",
        "                  'achievership': 'achievers hip', 'easedeverything': 'eased everything', 'youbever': 'you bever',\n",
        "                  'everperson': 'ever person', 'everydsy': 'everyday', 'whemever': 'whenever',\n",
        "                  'everyonr': 'everyone', 'severiity': 'severity', 'narracist': 'nar racist',\n",
        "                  'racistly': 'racist', 'takesuch': 'take such', 'mystakenly': 'mistakenly',\n",
        "                  'shouldntake': 'shouldnt take', 'Kalitake': 'Kali take', 'msitake': 'mistake',\n",
        "                  'straitstimes': 'straits times', 'timefram': 'timeframe', 'watchtime': 'watch time',\n",
        "                  'timetraveling': 'timet traveling', 'peactime': 'peacetime', 'timetabe': 'timetable',\n",
        "                  'cooktime': 'cook time', 'blocktime': 'block time', 'timesjobs': 'times jobs',\n",
        "                  'timesence': 'times ence', 'Touchtime': 'Touch time', 'timeloop': 'time loop',\n",
        "                  'subcentimeter': 'sub centimeter', 'timejobs': 'time jobs', 'Guardtime': 'Guard time',\n",
        "                  'realtimepolitics': 'realtime politics', 'loadingtimes': 'loading times',\n",
        "                  'timesnow': '24-hour English news channel in India', 'timesspark': 'times spark',\n",
        "                  'timetravelling': 'timet ravelling',\n",
        "                  'antimeter': 'anti meter', 'timewaste': 'time waste', 'cryptochristians': 'crypto christians',\n",
        "                  'Whatcould': 'What could', 'becomesdouble': 'becomes double', 'deathbecomes': 'death becomes',\n",
        "                  'youbecome': 'you become', 'greenseer': 'people who possess the magical ability',\n",
        "                  'rseearch': 'research', 'homeseek': 'home seek',\n",
        "                  'Greenseer': 'people who possess the magical ability', 'starseeders': 'star seeders',\n",
        "                  'seekingmillionaire': 'seeking millionaire', 'see\\u202c': 'see',\n",
        "                  'seeies': 'series', 'CodeAgon': 'Code Agon',\n",
        "                  'royago': 'royal', 'Dragonkeeper': 'Dragon keeper', 'mcgreggor': 'McGregor',\n",
        "                  'catrgory': 'category', 'Dragonknight': 'Dragon knight', 'Antergos': 'Anteros',\n",
        "                  'togofogo': 'togo fogo', 'mongorestore': 'mongo restore', 'gorgops': 'gorgons',\n",
        "                  'withgoogle': 'with google', 'goundar': 'Gondar', 'algorthmic': 'algorithmic',\n",
        "                  'goatnuts': 'goat nuts', 'vitilgo': 'vitiligo', 'polygony': 'poly gony',\n",
        "                  'digonals': 'diagonals', 'Luxemgourg': 'Luxembourg', 'UCSanDiego': 'UC SanDiego',\n",
        "                  'Ringostat': 'Ringo stat', 'takingoff': 'taking off', 'MongoImport': 'Mongo Import',\n",
        "                  'alggorithms': 'algorithms', 'dragonknight': 'dragon knight', 'negotiatior': 'negotiation',\n",
        "                  'gomovies': 'go movies', 'Withgott': 'Without',\n",
        "                  'categoried': 'categories', 'Stocklogos': 'Stock logos', 'Pedogogical': 'Pedological',\n",
        "                  'Wedugo': 'Wedge', 'golddig': 'gold dig', 'goldengroup': 'golden group',\n",
        "                  'merrigo': 'merligo', 'googlemapsAPI': 'googlemaps API', 'goldmedal': 'gold medal',\n",
        "                  'golemized': 'polemized', 'Caligornia': 'California', 'unergonomic': 'un ergonomic',\n",
        "                  'fAegon': 'wagon', 'vertigos': 'vertigo s', 'trigonomatry': 'trigonometry',\n",
        "                  'hypogonadic': 'hypogonadia', 'Mogolia': 'Mongolia', 'governmaent': 'government',\n",
        "                  'ergotherapy': 'ergo therapy', 'Bogosort': 'Bogo sort', 'goalwise': 'goal wise',\n",
        "                  'alogorithms': 'algorithms', 'MercadoPago': 'Mercado Pago', 'rivigo': 'rivi go',\n",
        "                  'govshutdown': 'gov shutdown', 'gorlfriend': 'girlfriend',\n",
        "                  'stategovt': 'state govt', 'Chickengonia': 'Chicken gonia', 'Yegorovich': 'Yegorov ich',\n",
        "                  'regognitions': 'recognitions', 'gorichen': 'Gori Chen Mountain',\n",
        "                  'goegraphies': 'geographies', 'gothras': 'goth ras', 'belagola': 'bela gola',\n",
        "                  'snapragon': 'snapdragon', 'oogonial': 'oogonia l', 'Amigofoods': 'Amigo foods',\n",
        "                  'Sigorn': 'son of Styr', 'algorithimic': 'algorithmic',\n",
        "                  'innermongolians': 'inner mongolians', 'ArangoDB': 'Arango DB', 'zigolo': 'gigolo',\n",
        "                  'regognized': 'recognized', 'Moongot': 'Moong ot', 'goldquest': 'gold quest',\n",
        "                  'catagorey': 'category', 'got7': 'got', 'jetbingo': 'jet bingo', 'Dragonchain': 'Dragon chain',\n",
        "                  'catwgorized': 'categorized', 'gogoro': 'gogo ro', 'Tobagoans': 'Tobago ans',\n",
        "                  'digonal': 'di gonal', 'algoritmic': 'algorismic', 'dragonflag': 'dragon flag',\n",
        "                  'Indigoflight': 'Indigo flight',\n",
        "                  'governening': 'governing', 'ergosphere': 'ergo sphere',\n",
        "                  'pingo5': 'pingo', 'Montogo': 'montego', 'Rivigo': 'technology-enabled logistics company',\n",
        "                  'Jigolo': 'Gigolo', 'phythagoras': 'pythagoras', 'Mangolian': 'Mongolian',\n",
        "                  'forgottenfaster': 'forgotten faster', 'stargold': 'a Hindi movie channel',\n",
        "                  'googolplexain': 'googolplexian', 'corpgov': 'corp gov',\n",
        "                  'govtribe': 'provides real-time federal contracting market intel',\n",
        "                  'dragonglass': 'dragon glass', 'gorakpur': 'Gorakhpur', 'MangoPay': 'Mango Pay',\n",
        "                  'chigoe': 'sub-tropical climates', 'BingoBox': 'an investment company', '走go': 'go',\n",
        "                  'followingorder': 'following order', 'pangolinminer': 'pangolin miner',\n",
        "                  'negosiation': 'negotiation', 'lexigographers': 'lexicographers', 'algorithom': 'algorithm',\n",
        "                  'unforgottable': 'unforgettable', 'wellsfargoemail': 'wellsfargo email',\n",
        "                  'daigonal': 'diagonal', 'Pangoro': 'cantankerous Pokemon', 'negotiotions': 'negotiations',\n",
        "                  'Swissgolden': 'Swiss golden', 'google4': 'google', 'Agoraki': 'Ago raki',\n",
        "                  'Garthago': 'Carthago', 'Stegosauri': 'stegosaurus', 'ergophobia': 'ergo phobia',\n",
        "                  'bigolive': 'big olive', 'bittergoat': 'bitter goat', 'naggots': 'faggots',\n",
        "                  'googology': 'online encyclopedia', 'algortihms': 'algorithms', 'bengolis': 'Bengalis',\n",
        "                  'fingols': 'Finnish people are supposedly descended from Mongols',\n",
        "                  'savethechildren': 'save thechildren',\n",
        "                  'stopings': 'stoping', 'stopsits': 'stop sits', 'stopsigns': 'stop signs',\n",
        "                  'Galastop': 'Galas top', 'pokestops': 'pokes tops', 'forcestop': 'forces top',\n",
        "                  'Hopstop': 'Hops top', 'stoppingexercises': 'stopping exercises', 'coinstop': 'coins top',\n",
        "                  'stoppef': 'stopped', 'workaway': 'work away', 'snazzyway': 'snazzy way',\n",
        "                  'Rewardingways': 'Rewarding ways', 'cloudways': 'cloud ways', 'Cloudways': 'Cloud ways',\n",
        "                  'Brainsway': 'Brains way', 'nesraway': 'nearaway',\n",
        "                  'AlwaysHired': 'Always Hired', 'expessway': 'expressway', 'Syncway': 'Sync way',\n",
        "                  'LeewayHertz': 'Blockchain Company', 'towayrds': 'towards', 'swayable': 'sway able',\n",
        "                  'Telloway': 'Tello way', 'palsmodium': 'plasmodium', 'Gobackmodi': 'Goback modi',\n",
        "                  'comodies': 'corodies', 'islamphobic': 'islam phobic', 'islamphobia': 'islam phobia',\n",
        "                  'citiesbetter': 'cities better', 'betterv3': 'better', 'betterDtu': 'better Dtu',\n",
        "                  'Babadook': 'a horror drama film', 'Ahemadabad': 'Ahmadabad', 'faidabad': 'Faizabad',\n",
        "                  'Amedabad': 'Ahmedabad', 'kabadii': 'kabaddi', 'badmothing': 'badmouthing',\n",
        "                  'badminaton': 'badminton', 'badtameezdil': 'badtameez dil', 'badeffects': 'bad effects',\n",
        "                  '∠bad': 'bad', 'ahemadabad': 'Ahmadabad', 'embaded': 'embased', 'Isdhanbad': 'Is dhanbad',\n",
        "                  'badgermoles': 'enormous, blind mammal', 'allhabad': 'Allahabad', 'ghazibad': 'ghazi bad',\n",
        "                  'htderabad': 'Hyderabad', 'Auragabad': 'Aurangabad', 'ahmedbad': 'Ahmedabad',\n",
        "                  'ahmdabad': 'Ahmadabad', 'alahabad': 'Allahabad',\n",
        "                  'Hydeabad': 'Hyderabad', 'Gyroglove': 'wearable technology', 'foodlovee': 'food lovee',\n",
        "                  'slovenised': 'slovenia', 'handgloves': 'hand gloves', 'lovestep': 'love step',\n",
        "                  'lovejihad': 'love jihad', 'RolloverBox': 'Rollover Box', 'stupidedt': 'stupidest',\n",
        "                  'toostupid': 'too stupid',\n",
        "                  'pakistanisbeautiful': 'pakistanis beautiful', 'ispakistan': 'is pakistan',\n",
        "                  'inpersonations': 'impersonations', 'medicalperson': 'medical person',\n",
        "                  'interpersonation': 'inter personation', 'workperson': 'work person',\n",
        "                  'personlich': 'person lich', 'persoenlich': 'person lich',\n",
        "                  'middleperson': 'middle person', 'personslized': 'personalized',\n",
        "                  'personifaction': 'personification', 'welcomemarriage': 'welcome marriage',\n",
        "                  'come2': 'come to', 'upcomedians': 'up comedians', 'overvcome': 'overcome',\n",
        "                  'talecome': 'tale come', 'cometitive': 'competitive', 'arencome': 'aren come',\n",
        "                  'achecomes': 'ache comes', '」come': 'come',\n",
        "                  'comepleted': 'completed', 'overcomeanxieties': 'overcome anxieties',\n",
        "                  'demigirl': 'demi girl', 'gridgirl': 'female models of the race', 'halfgirlfriend': 'half girlfriend',\n",
        "                  'girlriend': 'girlfriend', 'fitgirl': 'fit girl', 'girlfrnd': 'girlfriend', 'awrong': 'aw rong',\n",
        "                  'northcap': 'north cap', 'productionsupport': 'production support',\n",
        "                  'Designbold': 'Online Photo Editor Design Studio',\n",
        "                  'skyhold': 'sky hold', 'shuoldnt': 'shouldnt', 'anarold': 'Android', 'yaerold': 'year old',\n",
        "                  'soldiders': 'soldiers', 'indrold': 'Android', 'blindfoldedly': 'blindfolded',\n",
        "                  'overcold': 'over cold', 'Goldmont': 'microarchitecture in Intel', 'boldspot': 'bolds pot',\n",
        "                  'Rankholders': 'Rank holders', 'cooldrink': 'cool drink', 'beltholders': 'belt holders',\n",
        "                  'GoldenDict': 'open-source dictionary program', 'softskill': 'softs kill',\n",
        "                  'Cooldige': 'the 30th president of the United States',\n",
        "                  'newkiller': 'new killer', 'skillselect': 'skills elect', 'nonskilled': 'non skilled',\n",
        "                  'killyou': 'kill you', 'Skillport': 'Army e-Learning Program', 'unkilled': 'un killed',\n",
        "                  'killikng': 'killing', 'killograms': 'kilograms',\n",
        "                  'Worldkillers': 'World killers', 'reskilled': 'skilled',\n",
        "                  'killedshivaji': 'killed shivaji', 'honorkillings': 'honor killings',\n",
        "                  'skillclasses': 'skill classes', 'microskills': 'micros kills',\n",
        "                  'Skillselect': 'Skills elect', 'ratkill': 'rat kill',\n",
        "                  'pleasegive': 'please give', 'flashgive': 'flash give',\n",
        "                  'southerntelescope': 'southern telescope', 'westsouth': 'west south',\n",
        "                  'southAfricans': 'south Africans', 'Joboutlooks': 'Job outlooks', 'joboutlook': 'job outlook',\n",
        "                  'Outlook365': 'Outlook 365', 'Neulife': 'Neu life', 'qualifeid': 'qualified',\n",
        "                  'nullifed': 'nullified', 'lifeaffect': 'life affect', 'lifestly': 'lifestyle',\n",
        "                  'aristocracylifestyle': 'aristocracy lifestyle', 'antilife': 'anti life',\n",
        "                  'afterafterlife': 'after afterlife', 'lifestylye': 'lifestyle', 'prelife': 'pre life',\n",
        "                  'lifeute': 'life ute', 'liferature': 'literature',\n",
        "                  'securedlife': 'secured life', 'doublelife': 'double life', 'antireligion': 'anti religion',\n",
        "                  'coreligionist': 'co religionist', 'petrostates': 'petro states', 'otherstates': 'others tates',\n",
        "                  'spacewithout': 'space without', 'withoutyou': 'without you',\n",
        "                  'withoutregistered': 'without registered', 'weightwithout': 'weight without',\n",
        "                  'withoutcheck': 'without check', 'milkwithout': 'milk without',\n",
        "                  'Highschoold': 'High school', 'memoney': 'money', 'moneyof': 'mony of', 'Oneplus': 'OnePlus',\n",
        "                  'OnePlus': 'Chinese smartphone manufacturer', 'Beerus': 'the God of Destruction',\n",
        "                  'takeoverr': 'takeover', 'demonetizedd': 'demonetized', 'polyhouse': 'Polytunnel',\n",
        "                  'Elitmus': 'eLitmus', 'eLitmus': 'Indian company that helps companies in hiring employees',\n",
        "                  'becone': 'become', 'nestaway': 'nest away', 'takeoverrs': 'takeovers', 'Istop': 'I stop',\n",
        "                  'Austira': 'Australia', 'germeny': 'Germany', 'mansoon': 'man soon',\n",
        "                  'worldmax': 'wholesaler of drum parts',\n",
        "                  'ammusement': 'amusement', 'manyare': 'many are', 'supplymentary': 'supply mentary',\n",
        "                  'timesup': 'times up', 'homologus': 'homologous', 'uimovement': 'ui movement', 'spause': 'spouse',\n",
        "                  'aesexual': 'asexual', 'Iovercome': 'I overcome', 'developmeny': 'development',\n",
        "                  'hindusm': 'hinduism', 'sexpat': 'sex tourism', 'sunstop': 'sun stop', 'polyhouses': 'Polytunnel',\n",
        "                  'usefl': 'useful', 'Fundamantal': 'fundamental', 'environmentai': 'environmental',\n",
        "                  'Redmi': 'Xiaomi Mobile', 'Loy Machedo': ' Motivational Speaker ', 'unacademy': 'Unacademy',\n",
        "                  'Boruto': 'Naruto Next Generations', 'Upwork': 'Up work',\n",
        "                  'Unacademy': 'educational technology company',\n",
        "                  'HackerRank': 'Hacker Rank', 'upwork': 'up work', 'Chromecast': 'Chrome cast',\n",
        "                  'microservices': 'micro services', 'Undertale': 'video game', 'undergraduation': 'under graduation',\n",
        "                  'chapterwise': 'chapter wise', 'twinflame': 'twin flame', 'Hotstar': 'Hot star',\n",
        "                  'blockchains': 'blockchain',\n",
        "                  'darkweb': 'dark web', 'Microservices': 'Micro services', 'Nearbuy': 'Nearby',\n",
        "                  ' Padmaavat ': ' Padmavati ', ' padmavat ': ' Padmavati ', ' Padmaavati ': ' Padmavati ',\n",
        "                  ' Padmavat ': ' Padmavati ', ' internshala ': ' internship and online training platform in India ',\n",
        "                  'dream11': ' fantasy sports platform in India ', 'conciousnesss': 'consciousnesses',\n",
        "                  'Dream11': ' fantasy sports platform in India ', 'cointry': 'country', ' coinvest ': ' invest ',\n",
        "                  '23 andme': 'privately held personal genomics and biotechnology company in California',\n",
        "                  'Trumpism': 'philosophy and politics espoused by Donald Trump',\n",
        "                  'Trumpian': 'viewpoints of President Donald Trump', 'Trumpists': 'admirer of Donald Trump',\n",
        "                  'coincidents': 'coincidence', 'coinsized': 'coin sized', 'coincedences': 'coincidences',\n",
        "                  'cointries': 'countries', 'coinsidered': 'considered', 'coinfirm': 'confirm',\n",
        "                  'humilates':'humiliates', 'vicevice':'vice vice', 'politicak':'political', 'Sumaterans':'Sumatrans',\n",
        "                  'Kamikazis':'Kamikazes', 'unmoraled':'unmoral', 'eduacated':'educated', 'moraled':'morale',\n",
        "                  'Amharc':'Amarc', 'where Burkhas':'wear Burqas', 'Baloochistan':'Balochistan', 'durgahs':'durgans',\n",
        "                  'illigitmate':'illegitimate', 'hillum':'helium','treatens':'threatens','mutiliating':'mutilating',\n",
        "                  'speakingly':'speaking', 'pretex':'pretext', 'menstruateion':'menstruation', \n",
        "                  'genocidizing':'genociding', 'maratis':'Maratism','Parkistinian':'Pakistani', 'SPEICIAL':'SPECIAL',\n",
        "                  'REFERNECE':'REFERENCE', 'provocates':'provokes', 'FAMINAZIS':'FEMINAZIS', 'repugicans':'republicans',\n",
        "                  'tonogenesis':'tone', 'winor':'win', 'redicules':'ridiculous', 'Beluchistan':'Balochistan', \n",
        "                  'volime':'volume', 'namaj':'namaz', 'CONgressi':'Congress', 'Ashifa':'Asifa', 'queffing':'queefing',\n",
        "                  'montheistic':'nontheistic', 'Rajsthan':'Rajasthan', 'Rajsthanis':'Rajasthanis', 'specrum':'spectrum',\n",
        "                  'brophytes':'bryophytes', 'adhaar':'Adhara', 'slogun':'slogan', 'harassd':'harassed',\n",
        "                  'transness':'trans gender', 'Insdians':'Indians', 'Trampaphobia':'Trump aphobia', 'attrected':'attracted',\n",
        "                  'Yahtzees':'Yahtzee', 'thiests':'atheists', 'thrir':'their', 'extraterestrial':'extraterrestrial',\n",
        "                  'silghtest':'slightest', 'primarty':'primary','brlieve':'believe', 'fondels':'fondles',\n",
        "                  'loundly':'loudly', 'bootythongs':'booty thongs', 'understamding':'understanding', 'degenarate':'degenerate',\n",
        "                  'narsistic':'narcistic', 'innerskin':'inner skin','spectulated':'speculated', 'hippocratical':'Hippocratical',\n",
        "                  'itstead':'instead', 'parralels':'parallels', 'sloppers':'slippers'\n",
        "                  }\n",
        "\n",
        "def clean_bad_case_words(text):\n",
        "    for bad_word in bad_case_words:\n",
        "        if bad_word in text:\n",
        "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
        "        elif bad_word.capitalize() in text:\n",
        "            text = text.replace(bad_word.capitalize(), bad_case_words[bad_word])\n",
        "        elif bad_word.lower() in text:\n",
        "            text = text.replace(bad_word.lower(), bad_case_words[bad_word])\n",
        "        elif bad_word.upper() in text:\n",
        "            text = text.replace(bad_word.upper(), bad_case_words[bad_word])\n",
        "            \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAfKBwpkFwzK",
        "colab_type": "text"
      },
      "source": [
        "### preprocess pipeline (60min+)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slf1kQLv-o84",
        "colab_type": "code",
        "outputId": "d71fa789-2ac5-4b9a-be32-48d2b76409a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "#clean text main method\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = my_demojize(text)\n",
        "    text = RE_SPACE.sub(\" \", text)\n",
        "    #normalize special unicode characters\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = text.translate(TABLE)\n",
        "    #remove multispaces last\n",
        "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
        "    \n",
        "    text = pre_clean_rare_words(text)\n",
        "    text = clean_misspell(text)\n",
        "    text = clean_bad_case_words(text)\n",
        "    text = spacing_hash(text)\n",
        "    text = cleanUpT(text) #twitter cleaner\n",
        "    #text = spell_correct(text) #twitter spell corrector (doesn't work well)\n",
        "    text = spacing_some_connect_words(text) #how what when where while whatsapp\n",
        "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
        "\n",
        "    #custom word replacer\n",
        "    for pattern, repl in REGEX_REPLACER:\n",
        "        text = pattern.sub(repl, text)\n",
        "\n",
        "    text = spacing_punctuation(text)\n",
        "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "#with multiprocessing.Pool(processes=CORE_COUNT) as pool:\n",
        "#     text_list = pool.map(normalize_text, test_private_comments.comment_text.tolist())\n",
        "\n",
        "def text_clean_wrapper(df):\n",
        "    #df[\"comment_text_normalized\"] = df[\"comment_text_normalized\"].apply(normalize_text)\n",
        "    df[\"comment_text_normalized\"] = df[\"comment_text_normalized\"].progress_apply(normalize_text)\n",
        "    return df"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 14.5 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvHrlsGANik0",
        "colab_type": "text"
      },
      "source": [
        "example normalized text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVeP_bDd_1g5",
        "colab_type": "code",
        "outputId": "97177f68-ff15-47ea-eac2-b158b0126e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sents = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yay yaaaay!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuuuuucks!!! WAISTED $10... #badmovies >3:/\",\n",
        "    \"@SentimentSymp:    can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/...\",\n",
        "    \":-) <> () {} [] $@#$%:-p$#%@\",\n",
        "    '＼(^o^)／$100#F**kingloveyou#lovewordpp korrect SB91 jizhou.wang@mail.mcgill.ca(...)',\n",
        "]\n",
        "for s in sents:\n",
        "  print(normalize_text(correct_contraction(s, contraction_mapping)))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CANT WAIT for the new season of Twin Peaks \\ ( ^ o ^ ) / yay yaaaay ! ! ! david lynch tv series < happy >\n",
            "I saw the new john doe movie and it suuuuuuuucks ! ! ! WAISTED < money > . . . bad movies > < number > < annoyed >\n",
            "< user > : cannot wait for the < date > Sentiment talks ! YAAAAAAY ! ! ! < devil > < url >\n",
            "< happy > < > ( ) { } [ ] $ @ # $ % < tong > $ # % @\n",
            "\\ ( ^ o ^ ) / < money > fucking love you love word pp korrect senate bill < email > ( ... )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekWZIxGNj6M",
        "colab_type": "text"
      },
      "source": [
        "running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju4efazIADKa",
        "colab_type": "code",
        "outputId": "bf84a9cd-203d-4341-b972-9794a2427bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#correct contractions and create normalized column\n",
        "test_private_comments['comment_text_normalized']  = test_private_comments['comment_text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
        "train_comments['comment_text_normalized']  = train_comments['comment_text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 97320/97320 [00:37<00:00, 2589.07it/s]\n",
            "100%|██████████| 1804874/1804874 [11:34<00:00, 2599.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf1yYaQWAgdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split function for big dataframes (not needed)\n",
        "def df_split_run(df):\n",
        "    df_split = np.array_split(df, 4)\n",
        "    for i in tqdm(range(4)):\n",
        "        #print(i)\n",
        "        #print(df_split[i]['comment_text_normalized'])\n",
        "        df_split[i] = df_parallelize_run(df_split[i], text_clean_wrapper)\n",
        "        #df_split[i]['comment_text_normalized']  = df_split[i]['comment_text_normalized'].progress_apply(normalize_text)\n",
        "\n",
        "    df = pd.concat(df_split)\n",
        "    #print(df.comment_text_normalized.values)\n",
        "    return df\n",
        "\n",
        "test_private_comments = df_parallelize_run(test_private_comments, text_clean_wrapper)\n",
        "\n",
        "#comment id 5706703 URL causing recursion errors in twitter preprocessor, manually change\n",
        "#train_comments[train_comments['id'] = 5706703]['comment_text_normalized']= 'Strange that Carrigan makes no mention of the Bataan Death March . < url >'\n",
        "#print(train_comments[train_comments['id'] == 5706703]['comment_text_normalized'].values)\n",
        "train_comments.at[1302664, 'comment_text_normalized'] = 'Strange that Carrigan makes no mention of the Bataan Death March . < url >'\n",
        "\n",
        "train_comments = df_parallelize_run(train_comments, text_clean_wrapper)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKEnFXfpMG2o",
        "colab_type": "code",
        "outputId": "62adea5b-9984-41eb-ff82-f9081bbcfd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "train_comments['comment_text_normalized'].head(10).values"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"This is so cool . it is like , ' would you want your mother to read this ? ? ' Really great idea , well done !\",\n",
              "       'Thank you ! ! This would make my life a lot less anxiety - inducing . Keep it up , and do not let anyone get in your way !',\n",
              "       'This is such an urgent design problem ; kudos to you for taking it on . Very impressive !',\n",
              "       'Is this something i will be able to install on my site ? When will you be releasing it ?',\n",
              "       'haha you guys are a bunch of losers .', 'ur a shitty comment .',\n",
              "       'hahahahahahahahhha suck it .', 'FFFFUUUUUUUUUUUUUUU',\n",
              "       'The ranchers seem motivated by mostly by greed ; no one should have the right to allow their animals destroy public land .',\n",
              "       'It was a great s How . Not a combo i would of expected to be good together but it was .'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI225qtvMLlF",
        "colab_type": "code",
        "outputId": "c307f09d-1056-47e5-9a3a-20f552c3ef74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "train_comments['comment_text'].head(10).values"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n",
              "       \"Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\",\n",
              "       'This is such an urgent design problem; kudos to you for taking it on. Very impressive!',\n",
              "       \"Is this something I'll be able to install on my site? When will you be releasing it?\",\n",
              "       'haha you guys are a bunch of losers.', 'ur a sh*tty comment.',\n",
              "       'hahahahahahahahhha suck it.', 'FFFFUUUUUUUUUUUUUUU',\n",
              "       'The ranchers seem motivated by mostly by greed; no one should have the right to allow their animals destroy public land.',\n",
              "       \"It was a great show. Not a combo I'd of expected to be good together but it was.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2gKTclD4aAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save to csv\n",
        "!mkdir /content/normalized_dataset\n",
        "train_comments_new.to_csv('/content/normalized_dataset/train_normalized.csv', index=None)\n",
        "test_private_comments.to_csv('/content/normalized_dataset/test_normalized.csv', index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_j3VTyZVLtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#zip and download normalized text\n",
        "!zip -r /content/normalized_dataset.zip /content/normalized_dataset\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/normalized_dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsS5O6SCAXqm",
        "colab_type": "text"
      },
      "source": [
        "###load normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBHW0f6MAbis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_comments = pd.read_csv('/content/normalized_dataset/train_normalized.csv')\n",
        "test_private_comments = pd.read_csv('/content/normalized_dataset/test_normalized.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT_ATBLjpFTg",
        "colab_type": "text"
      },
      "source": [
        "###word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEId5yjg-wPF",
        "colab_type": "text"
      },
      "source": [
        "#### compare tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oFM9QCTsm9q",
        "colab_type": "code",
        "outputId": "e2f0a556-9289-454b-bc46-0592ae5cdc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "def wsp_tokenizer(text):\n",
        "    return text.split(\" \")\n",
        "\n",
        "puncttok = WordPunctTokenizer().tokenize\n",
        "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize\n",
        "social_tokenizer = SocialTokenizer(lowercase=False).tokenize\n",
        "basic_tokenizer=tokenizeBasic().tokenize\n",
        "no_tokenizer=no_tokenize().tokenize\n",
        "    \n",
        "sents = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yaaaaaaaaay!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuuuuucks!!! WAISTED $10... #badmovies >3:/\",\n",
        "    \"@SentimentSymp:    can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/...\",\n",
        "    \":-) <> () {} [] $@#$%:-p$#%@\",\n",
        "]\n",
        "\n",
        "for s in sents:\n",
        "    print()\n",
        "    print(\"ORG: \", s)  # original sentence\n",
        "    print(\"WSP : \", wsp_tokenizer(s))  # whitespace tokenizer\n",
        "    print(\"WPU : \", puncttok(s))  # WordPunct tokenizer\n",
        "    print(\"SC : \", social_tokenizer(s))  # social tokenizer\n",
        "    print(\"TW : \", tknzr(s))  # social tokenizer\n",
        "    print(\"BC : \", basic_tokenizer(s))  # basic tokenizer\n",
        "    print(\"NO : \", no_tokenizer(s))  # basic tokenizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ORG:  CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yaaaaaaaaay!!! #davidlynch #tvseries :)))\n",
            "WSP :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼(^o^)／', 'yaaaaaaaaay!!!', '#davidlynch', '#tvseries', ':)))']\n",
            "WPU :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#', 'TwinPeaks', '＼(^', 'o', '^)／', 'yaaaaaaaaay', '!!!', '#', 'davidlynch', '#', 'tvseries', ':)))']\n",
            "SC :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼(^o^)／', 'yaaaaaaaaay', '!', '!', '!', '#davidlynch', '#tvseries', ':)))']\n",
            "TW :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼', '(', '^', 'o', '^', ')', '／', 'yaaay', '!', '!', '!', '#davidlynch', '#tvseries', ':)', ')', ')']\n",
            "BC :  ['CANT', 'WAIT', 'for', 'the', 'new', 'season', 'of', '#TwinPeaks', '＼(^o^)／', 'yaaaaaaaaay!!!', '#davidlynch', '#tvseries', ':)))']\n",
            "NO :  CANT WAIT for the new season of #TwinPeaks ＼(^o^)／ yaaaaaaaaay!!! #davidlynch #tvseries :)))\n",
            "\n",
            "ORG:  I saw the new #johndoe movie and it suuuuuuuucks!!! WAISTED $10... #badmovies >3:/\n",
            "WSP :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuuuuuuucks!!!', 'WAISTED', '$10...', '#badmovies', '>3:/']\n",
            "WPU :  ['I', 'saw', 'the', 'new', '#', 'johndoe', 'movie', 'and', 'it', 'suuuuuuuucks', '!!!', 'WAISTED', '$', '10', '...', '#', 'badmovies', '>', '3', ':/']\n",
            "SC :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuuuuuuucks', '!', '!', '!', 'WAISTED', '$10', '.', '.', '.', '#badmovies', '>', '3:/']\n",
            "TW :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuucks', '!', '!', '!', 'WAISTED', '$', '10', '...', '#badmovies', '>', '3', ':/']\n",
            "BC :  ['I', 'saw', 'the', 'new', '#johndoe', 'movie', 'and', 'it', 'suuuuuuuucks!!!', 'WAISTED', '$10...', '#badmovies', '>3:/']\n",
            "NO :  I saw the new #johndoe movie and it suuuuuuuucks!!! WAISTED $10... #badmovies >3:/\n",
            "\n",
            "ORG:  @SentimentSymp,:    can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/...\n",
            "WSP :  ['@SentimentSymp,:', '', '', '', \"can't\", 'wait', 'for', 'the', 'Nov', '9', '#Sentiment', 'talks!', '', 'YAAAAAAY', '!!!', '>:-D', 'http://sentimentsymposium.com/...']\n",
            "WPU :  ['@', 'SentimentSymp', ',:', 'can', \"'\", 't', 'wait', 'for', 'the', 'Nov', '9', '#', 'Sentiment', 'talks', '!', 'YAAAAAAY', '!!!', '>:-', 'D', 'http', '://', 'sentimentsymposium', '.', 'com', '/...']\n",
            "SC :  ['@SentimentSymp', ',', ':', 'can', \"'\", 't', 'wait', 'for', 'the', 'Nov 9', '#Sentiment', 'talks', '!', 'YAAAAAAY', '!', '!', '!', '>:-D', 'http://sentimentsymposium.com/...']\n",
            "TW :  [',', ':', \"can't\", 'wait', 'for', 'the', 'Nov', '9', '#Sentiment', 'talks', '!', 'YAAAY', '!', '!', '!', '>:-D', 'http://sentimentsymposium.com/', '...']\n",
            "BC :  ['@SentimentSymp,:', \"can't\", 'wait', 'for', 'the', 'Nov', '9', '#Sentiment', 'talks!', 'YAAAAAAY', '!!!', '>:-D', 'http://sentimentsymposium.com/...']\n",
            "NO :  @SentimentSymp,:    can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/...\n",
            "\n",
            "ORG:  :-) <> () {} [] $@#$%:-p$#%@\n",
            "WSP :  [':-)', '<>', '()', '{}', '[]', '$@#$%:-p$#%@']\n",
            "WPU :  [':-)', '<>', '()', '{}', '[]', '$@#$%:-', 'p', '$#%@']\n",
            "SC :  [':-)', '<', '>', '(', ')', '{', '}', '[', ']', '$@', '#$', '%', ':-p', '$#', '%', '@']\n",
            "TW :  [':-)', '<', '>', '(', ')', '{', '}', '[', ']', '$', '@', '#', '$', '%', ':-p', '$', '#', '%', '@']\n",
            "BC :  [':-)', '<>', '()', '{}', '[]', '$@#$%:-p$#%@']\n",
            "NO :  :-) <> () {} [] $@#$%:-p$#%@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6glvOs5KDpSL",
        "colab_type": "text"
      },
      "source": [
        "#### build a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLrPDKnY_G4k",
        "colab_type": "code",
        "outputId": "4c4793ce-bacd-4100-b76c-05c462ebfc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "#simple vocab builder\n",
        "# def build_vocab(texts):\n",
        "#     sentences = texts.progress_apply(lambda x: x.split()).values\n",
        "#     vocab = {}\n",
        "#     for sentence in sentences:\n",
        "#         for word in sentence:\n",
        "#             try:\n",
        "#                 vocab[word] += 1\n",
        "#             except KeyError:\n",
        "#                 vocab[word] = 1\n",
        "#     return vocab\n",
        "def build_vocab(texts):\n",
        "    vocab = {}\n",
        "    for sentence in texts.values:\n",
        "        for word in sentence.split():\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "#complex vocab builder\n",
        "def build_complex_vocab(text_list,word_dict={}):\n",
        "    tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
        "    #social_tokenizer = SocialTokenizer(lowercase=False)\n",
        "    test_word_sequences = []\n",
        "    word_index = 1\n",
        "\n",
        "    for doc in text_list.values:\n",
        "        word_seq = []\n",
        "                    #word_tokenize(doc)\n",
        "        for token in tknzr.tokenize(doc):\n",
        "            if token not in word_dict:\n",
        "                word_dict[token] = word_index\n",
        "                word_index += 1\n",
        "            word_seq.append(word_dict[token])\n",
        "        test_word_sequences.append(word_seq)\n",
        "\n",
        "    return word_dict, test_word_sequences\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 6.44 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yBXfqVWzNvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert taget and identity columns to booleans\n",
        "def convert_to_bool(df, col_name):\n",
        "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
        "    # df.loc[df.col_name >= 0.5, col_name] = True\n",
        "    # df.loc[df.col_name < 0.5, col_name] = False\n",
        "\n",
        "\n",
        "def convert_dataframe_to_bool(df):\n",
        "    bool_df = df.copy()\n",
        "    for col in [\"target\"] + identity_columns:\n",
        "        convert_to_bool(bool_df, col)\n",
        "    return bool_df\n",
        "\n",
        "#%%\n",
        "train_comments_bool = convert_dataframe_to_bool(train_comments)\n",
        "test_private_comments_bool = convert_dataframe_to_bool(test_private_comments)\n",
        "\n",
        "#for KF CV if there is enough time\n",
        "# train_comments_bool, validate_comments_bool = train_test_split(\n",
        "#     train_comments_bool, test_size=0.2, random_state=42, shuffle=False\n",
        "# )\n",
        "# print(\"%d train comments, %d validate comments\" % (len(train_comments_bool), len(validate_comments_bool)))\n",
        "\n",
        "# x_train = preprocess(train['comment_text'])\n",
        "# y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
        "# y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "# x_test = preprocess(test['comment_text'])\n",
        "\n",
        "x_train = train_comments_bool['comment_text']\n",
        "y_train = train_comments_bool['target']\n",
        "y_aux_train = train_comments_bool[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "\n",
        "x_test = test_private_comments_bool['comment_text']\n",
        "y_test = test_private_comments_bool['target']\n",
        "y_aux_test = test_private_comments_bool[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMzHpmTgJWSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test?\n",
        "pd.concat([x_train, x_test]).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoZlVx-0CT44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "#build dictionary for dataset\n",
        "word_dict, x_train = build_complex_vocab(x_train)\n",
        "word_dict, x_test = build_complex_vocab(x_test, word_dict)\n",
        "\n",
        "#pad as input to model\n",
        "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
        "maxlentest = test_lengths.max() \n",
        "print(f\"Max test len = {maxlentest}\")\n",
        "train_lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
        "maxlentrain = train_lengths.max() \n",
        "print(f\"Max train len = {maxlentrain}\")\n",
        "MAX_LEN = min(maxlentest,maxlentrain, 400)\n",
        "\n",
        "x_test = torch.tensor(pad_sequences(x_test, maxlen=maxlen)).long()\n",
        "x_train = torch.tensor(pad_sequences(x_train, maxlen=maxlen)).long()\n",
        "\n",
        "#test_collator = SequenceBucketCollator(torch.max, sequence_index=0, length_index=1)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv_5no_T_GL7",
        "colab_type": "text"
      },
      "source": [
        "#### embedding tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si3G79JuptEN",
        "colab_type": "code",
        "outputId": "c06674fe-1014-428a-ffde-fcbfda165477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "print(word2vec[\"cat\"].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-24 19:56:44--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.38.222\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.38.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJLHhO6MwiDX",
        "colab_type": "code",
        "outputId": "c1c09a3e-68fc-4e78-9b44-2dafe04c89c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "word2vec.most_similar(positive=['cant'], topn=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dont', 0.8604573011398315),\n",
              " ('wont', 0.8167182803153992),\n",
              " ('i_cant', 0.7770447731018066),\n",
              " ('i_dont', 0.7551953196525574),\n",
              " ('wouldnt', 0.7540680170059204),\n",
              " ('couldnt', 0.7380253672599792),\n",
              " ('thats', 0.7338809967041016),\n",
              " ('u_cant', 0.7163205146789551),\n",
              " ('dosnt', 0.6956251859664917),\n",
              " ('whats', 0.6946329474449158),\n",
              " (\"did'nt\", 0.6928683519363403),\n",
              " ('i_couldnt', 0.6891015768051147),\n",
              " ('dont_wanna', 0.6860548257827759),\n",
              " (\"does'nt\", 0.6856260299682617),\n",
              " ('gona', 0.6839221715927124),\n",
              " (\"would'nt\", 0.683076024055481),\n",
              " ('dosent', 0.6830652952194214),\n",
              " ('Dont', 0.6825885772705078),\n",
              " ('u_dont', 0.679046094417572),\n",
              " ('i_wouldnt', 0.6786744594573975)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO6ZfmqLqkZi",
        "colab_type": "code",
        "outputId": "440a1c69-9bf2-46d3-95bf-7dd5884b53fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#print known and unknown punctuations in embedding\n",
        "def unknown_punct(embed, punct):\n",
        "    unknown = ''\n",
        "    for p in punct:\n",
        "        if p not in embed:\n",
        "            unknown += p\n",
        "            unknown += ' '\n",
        "    return unknown\n",
        "\n",
        "def known_punct(embed, punct):\n",
        "    known = ''\n",
        "    for p in punct:\n",
        "        if p in embed:\n",
        "            known += p\n",
        "            known += ' '\n",
        "    return known\n",
        "\n",
        "print(unknown_punct(word2vec,all_punct_string))\n",
        "print(known_punct(word2vec,all_punct_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "″ : ! ╣ / ‹ ・ ₹ ╩ – [ − 、 . ═ \\ ╚ ( ║ ´ } ▓ , ░ ▒ … | ╦ ▄ （ ╔ ' ? ▾ ′ < ∅ \" { ） ‘ ¦ “ ; — ： ’ - › ] ” ▬ ╗ ， ) ― ❤ \n",
            "â ∞ √ ¬ │ % • α » ® ↓ ¸ ~ £ ¾ é Ã ► ` ¥ █ ‡ ⋅ ½ Ø ■ ☆ ♪ = ♫ § ™ ▀ à ● × ¡ ▪ → « · ÷ ¶ + θ € > ★ π ▲ ▼ ³ ° ± ♥ ¨ ¼ † @ ♦ * ï ¿ ¤ Â è ⊕ ∙ ¯ ₤ _ ↑ ^ ¢ © ← $ ─ ≤ β ¹ # ² & º \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WGag1N6-9XQ",
        "colab_type": "text"
      },
      "source": [
        "#### embedding input for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAl-OCCxpENf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#embedding functions\n",
        "from gensim.models import KeyedVectors\n",
        "#extract word embeddings\n",
        "\n",
        "PORTER_STEMMER = PorterStemmer()\n",
        "LANCASTER_STEMMER = LancasterStemmer()\n",
        "SNOWBALL_STEMMER = SnowballStemmer(\"english\")\n",
        "LEMM = WordNetLemmatizer()\n",
        "#functions below for word form check and output word embeddings\n",
        "def word_forms(word):\n",
        "    yield word\n",
        "    yield word.lower()\n",
        "    yield word.upper()\n",
        "    yield word.capitalize()\n",
        "    yield PORTER_STEMMER.stem(word)\n",
        "    yield LANCASTER_STEMMER.stem(word)\n",
        "    yield SNOWBALL_STEMMER.stem(word)\n",
        "    yield LEMM.lemmatize(word.lower())\n",
        "\n",
        "def maybe_get_embedding(word, model):\n",
        "    for form in word_forms(word):\n",
        "        if form in model:\n",
        "            return model[form]\n",
        "\n",
        "    word = word.strip(\"-'\")\n",
        "    for form in word_forms(word):\n",
        "        if form in model:\n",
        "            return model[form]\n",
        "\n",
        "    return None\n",
        "\n",
        "#check embedding coverage and return out of vocab (oov) words\n",
        "def check_coverage(vocab, embeddings_index):\n",
        "    known_words = {}\n",
        "    unknown_words = {}\n",
        "    nb_known_words = 0\n",
        "    nb_unknown_words = 0\n",
        "    embedding_form = ''\n",
        "    isEmbedding = False\n",
        "\n",
        "    for word in vocab.keys():\n",
        "      #check for word forms in embeddings\n",
        "      for form in word_forms(word):\n",
        "        isEmbedding = False\n",
        "        if form in embeddings_index:\n",
        "            isEmbedding = True\n",
        "            embedding_form = form\n",
        "            break\n",
        "      if isEmbedding:\n",
        "          known_words[word] = embeddings_index[form]\n",
        "          nb_known_words += vocab[word]\n",
        "      else:\n",
        "          unknown_words[word] = vocab[word]\n",
        "          nb_unknown_words += vocab[word]\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
        "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return unknown_words\n",
        "\n",
        "#check vocab coverage in embedding from both train and test\n",
        "def vocab_check_coverage(train, test):\n",
        "    df = pd.concat([train, test]).reset_index(drop=True)\n",
        "    \n",
        "    vocab = build_vocab(df['comment_text'])\n",
        "    vocab, _ = build_complex_vocab(df['comment_text'])\n",
        "    print(\"Glove : \")\n",
        "    oov_glove = check_coverage(vocab, embed_glove)\n",
        "    oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
        "    print(\"Paragram : \")\n",
        "    oov_paragram = check_coverage(vocab, embed_paragram)\n",
        "    oov_paragram = {\"oov_rate\": len(oov_paragram) / len(vocab), 'oov_words': oov_paragram}\n",
        "    print(\"FastText : \")\n",
        "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
        "    oov_fasttext = {\"oov_rate\": len(oov_fasttext) / len(vocab), 'oov_words': oov_fasttext}\n",
        "    print(\"Google : \")\n",
        "    oov_google = check_coverage(vocab, embed_google)\n",
        "    oov_google = {\"oov_rate\": len(oov_google) / len(vocab), 'oov_words': oov_google}\n",
        "    \n",
        "    return oov_glove, oov_paragram, oov_fasttext\n",
        "\n",
        "\n",
        "#create embedding matrix\n",
        "def gensim_to_embedding_matrix(word2index, path):\n",
        "    model = KeyedVectors.load(path, mmap=\"r\")\n",
        "    embedding_matrix = np.zeros((max(word2index.values()) + 1, model.vector_size), dtype=np.float32)\n",
        "    #embeddings_index = {}\n",
        "    unknown_words = []\n",
        "\n",
        "    for word, i in word2index.items():\n",
        "        maybe_embedding = maybe_get_embedding(word, model)\n",
        "        if maybe_embedding is not None:\n",
        "            #embeddings_index[word] = maybe_embedding\n",
        "            embedding_matrix[i] = maybe_embedding\n",
        "        else:\n",
        "            unknown_words.append(word)\n",
        "\n",
        "    return embedding_matrix, unknown_words, model\n",
        "\n",
        "    \n",
        "# EMBEDDING_FILE = '/content/gensim-embeddings-dataset/file.genism'\n",
        "\n",
        "# from gensim.models import KeyedVectors\n",
        "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "# x = word2vec.word_vec(\"test\")\n",
        "\n",
        "#one_hot_embeddings to distinguish between rare words\n",
        "def one_hot_embeddings(word2index, vectorizer):\n",
        "    words = [\"\"] * (max(word2index.values()) + 1)\n",
        "    for word, i in word2index.items():\n",
        "        words[i] = word\n",
        "\n",
        "    return vectorizer.fit_transform(words).toarray().astype(np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yiY_ocOpI5L",
        "colab_type": "code",
        "outputId": "193d182b-6582-433e-a3ec-dad2a50e6c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "glove_matrix, _, embed_glove = gensim_to_embedding_matrix(\n",
        "    word_dict,\n",
        "    \"/content/gensim-embeddings-dataset/glove.840B.300d.gensim\",\n",
        ")\n",
        "\n",
        "crawl_matrix, _, embed_fasttext = gensim_to_embedding_matrix(\n",
        "    word_dict, \n",
        "    \"/content/gensim-embeddings-dataset/crawl-300d-2M.gensim\",\n",
        ")\n",
        "\n",
        "para_matrix, _, embed_paragram= gensim_to_embedding_matrix(\n",
        "    word_dict, \n",
        "    \"/content/gensim-embeddings-dataset/paragram_300_sl999.gensim\",\n",
        ")\n",
        "\n",
        "w2v_matrix, _,  embed_google= gensim_to_embedding_matrix(\n",
        "    word_dict, \n",
        "    \"/content/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim\",\n",
        ")\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "one_hot_matrix = one_hot_embeddings(\n",
        "    word_dict,\n",
        "    count_vectorizer,\n",
        ")\n",
        "\n",
        "\n",
        "embedding_matrix = np.concatenate([glove_matrix, crawl_matrix, w2v_matrix, one_hot_matrix], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37 s, sys: 1.9 s, total: 38.9 s\n",
            "Wall time: 1min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQxN98jVWqN7",
        "colab_type": "text"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrkQcNH67m2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "from keras.preprocessing import text, sequence\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#set the number of model seeds to train\n",
        "NUM_MODELS = 2\n",
        "\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "#MAX_LEN = 220\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBE2aItX-9zE",
        "colab_type": "text"
      },
      "source": [
        "## model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOj4ugyeWmUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#set the epoc/lr/batch size here\n",
        "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
        "                batch_size=512, n_epochs=4,\n",
        "                enable_checkpoint_ensemble=True):\n",
        "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
        "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
        "    #adaptive scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "    all_test_preds = []\n",
        "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        model.train()\n",
        "        avg_loss = 0.\n",
        "        \n",
        "        for data in tqdm(train_loader, disable=False):\n",
        "            x_batch = data[:-1]\n",
        "            y_batch = data[-1]\n",
        "\n",
        "            y_pred = model(*x_batch)            \n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item() / len(train_loader)\n",
        "            \n",
        "        model.eval()\n",
        "        test_preds = np.zeros((len(test), output_dim))\n",
        "    \n",
        "        for i, x_batch in enumerate(test_loader):\n",
        "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
        "\n",
        "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
        "\n",
        "        all_test_preds.append(test_preds)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
        "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
        "\n",
        "    if enable_checkpoint_ensemble:\n",
        "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
        "    else:\n",
        "        test_preds = all_test_preds[-1]\n",
        "        \n",
        "    return test_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TzheGJL_Njc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n",
        "    \n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_aux_targets):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        embed_size = embedding_matrix.shape[1]\n",
        "        \n",
        "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.embedding_dropout = SpatialDropout(0.3)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "    \n",
        "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "        \n",
        "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
        "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = self.embedding_dropout(h_embedding)\n",
        "        \n",
        "        h_lstm1, _ = self.lstm1(h_embedding)\n",
        "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
        "        \n",
        "        # global average pooling\n",
        "        avg_pool = torch.mean(h_lstm2, 1)\n",
        "        # global max pooling\n",
        "        max_pool, _ = torch.max(h_lstm2, 1)\n",
        "        \n",
        "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
        "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
        "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
        "        \n",
        "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
        "        \n",
        "        result = self.linear_out(hidden)\n",
        "        aux_result = self.linear_aux_out(hidden)\n",
        "        out = torch.cat([result, aux_result], 1)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDR1pPWoPobO",
        "colab_type": "text"
      },
      "source": [
        "## model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2KpEDdPtAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
        "x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
        "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()\n",
        "y_test_torch = torch.tensor(np.hstack([y_test[:, np.newaxis], y_aux_test]), dtype=torch.float32).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLUhx-8JP-IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
        "test_dataset = data.TensorDataset(x_test_torch, y_test_torch)\n",
        "\n",
        "all_test_preds = []\n",
        "\n",
        "#train two different model seeds\n",
        "for model_idx in range(NUM_MODELS):\n",
        "    print('Model ', model_idx)\n",
        "    seed_everything(1234 + model_idx)\n",
        "    \n",
        "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
        "    model.cuda()\n",
        "    \n",
        "    test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n",
        "                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
        "    all_test_preds.append(test_preds)\n",
        "    print()\n",
        "    \n",
        "#prediction column\n",
        "MODEL_NAME = 'prediction'\n",
        "test_private_comments_bool[MODEL_NAME] = np.mean(all_test_preds, axis=0)[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmK2NM-DWa2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save and load models\n",
        "# import pickle\n",
        "# with open(\"/content/model.pickle\", \"wb\") as f:\n",
        "#     pickle.dump(model, f)\n",
        "\n",
        "# with open(\"/content/model.pickle\", \"rb\") as f:\n",
        "#     model = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3RNKhERRkY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission = pd.DataFrame.from_dict({\n",
        "#     'id': test['id'],\n",
        "#     'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
        "# })\n",
        "\n",
        "# submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0sRf-jWvtK",
        "colab_type": "text"
      },
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ydpojr1Y_zO",
        "colab_type": "text"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Neyk0DaZMr_",
        "colab_type": "text"
      },
      "source": [
        "Evaluate biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8U4QVHmY-UK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "from sklearn import metrics\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "#Define bias metrics, then evaluate our new model for bias using the validation set predictions\n",
        "\n",
        "SUBGROUP_AUC = 'subgroup_auc'\n",
        "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
        "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    try:\n",
        "        return metrics.roc_auc_score(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
        "    subgroup_examples = df[df[subgroup]]\n",
        "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
        "\n",
        "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
        "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
        "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
        "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
        "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
        "    return compute_auc(examples[label], examples[model_name])\n",
        "\n",
        "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
        "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
        "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
        "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
        "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
        "    return compute_auc(examples[label], examples[model_name])\n",
        "\n",
        "def compute_bias_metrics_for_model(dataset,\n",
        "                                   subgroups,\n",
        "                                   model,\n",
        "                                   label_col,\n",
        "                                   include_asegs=False):\n",
        "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
        "    records = []\n",
        "    for subgroup in subgroups:\n",
        "        record = {\n",
        "            'subgroup': subgroup,\n",
        "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
        "        }\n",
        "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
        "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
        "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
        "        records.append(record)\n",
        "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
        "\n",
        "#uncomment to show only bias metric\n",
        "#bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
        "#bias_metrics_df\n",
        "\n",
        "#NOTE use bettermetric.py for below method\n",
        "#calculate final score\n",
        "TOXICITY_COLUMN = 'target'\n",
        "def calculate_overall_auc(df, model_name):\n",
        "    true_labels = df[TOXICITY_COLUMN]\n",
        "    predicted_labels = df[model_name]\n",
        "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
        "\n",
        "def power_mean(series, p):\n",
        "    total = sum(np.power(series, p))\n",
        "    return np.power(total / len(series), 1 / p)\n",
        "\n",
        "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
        "    bias_score = np.average([\n",
        "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
        "        power_mean(bias_df[BPSN_AUC], POWER),\n",
        "        power_mean(bias_df[BNSP_AUC], POWER)\n",
        "    ])\n",
        "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
        "#uncomment to show overall metric    \n",
        "#get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltRDaIthZPC5",
        "colab_type": "text"
      },
      "source": [
        "Evaluate overall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfHPbasOZTfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "class JigsawEvaluator:\n",
        "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
        "        self.y = (y_true >= 0.5).astype(int)\n",
        "        self.y_i = (y_identity >= 0.5).astype(int)\n",
        "        self.n_subgroups = self.y_i.shape[1]\n",
        "        self.power = power\n",
        "        self.overall_model_weight = overall_model_weight\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_auc(y_true, y_pred):\n",
        "        try:\n",
        "            return roc_auc_score(y_true, y_pred)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "    def _compute_subgroup_auc(self, i, y_pred):\n",
        "        mask = self.y_i[:, i] == 1\n",
        "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
        "\n",
        "    def _compute_bpsn_auc(self, i, y_pred):\n",
        "        mask = self.y_i[:, i] + self.y == 1\n",
        "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
        "\n",
        "    def _compute_bnsp_auc(self, i, y_pred):\n",
        "        mask = self.y_i[:, i] + self.y != 1\n",
        "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
        "\n",
        "    def compute_bias_metrics_for_model(self, y_pred):\n",
        "        records = np.zeros((3, self.n_subgroups))\n",
        "        for i in range(self.n_subgroups):\n",
        "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
        "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
        "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
        "        return records\n",
        "\n",
        "    def _calculate_overall_auc(self, y_pred):\n",
        "        return roc_auc_score(self.y, y_pred)\n",
        "\n",
        "    def _power_mean(self, array):\n",
        "        total = sum(np.power(array, self.power))\n",
        "        return np.power(total / len(array), 1 / self.power)\n",
        "\n",
        "    def get_final_metric(self, y_pred):\n",
        "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
        "        bias_score = np.average(\n",
        "            [\n",
        "                self._power_mean(bias_metrics[0]),\n",
        "                self._power_mean(bias_metrics[1]),\n",
        "                self._power_mean(bias_metrics[2]),\n",
        "            ]\n",
        "        )\n",
        "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
        "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
        "        return overall_score + bias_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh62VRjGY-yy",
        "colab_type": "text"
      },
      "source": [
        "## evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaSHJttkWl7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = test_private_comments_bool[TOXICITY_COLUMN].values\n",
        "y_identity = test_private_comments_bool[identity_columns].values\n",
        "y_pred = test_private_comments_bool[MODEL_NAME].values\n",
        "\n",
        "# evaluate\n",
        "evaluator = JigsawEvaluator(y_true, y_identity)\n",
        "auc_score = evaluator.get_final_metric(y_pred)\n",
        "print(auc_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmrVpZrVYTPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#only bias metric\n",
        "bias_metrics_df = compute_bias_metrics_for_model(test_private_comments_bool, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
        "bias_metrics_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}